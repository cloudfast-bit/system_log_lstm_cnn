{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics, cross_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 0.4)\n",
    "sess_config = tf.ConfigProto(gpu_options=gpu_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = {'0':'file', '1':'network', '2':'service', '3':'database', '4':'communication', '5':'memory', '6':'driver', \n",
    "    '7':'system', '8':'application', '9':'io', '10':'others', '11':'security', '12':'disk', '13':'processor'}\n",
    "\n",
    "fault_label = {'0':'file', '1':'network', '2':'service', '3':'database', '4':'communication', '5':'memory', '6':'driver', \n",
    "    '7':'system', '8':'application', '9':'io', '10':'others', '11':'security', '12':'disk', '13':'processor'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separating data into 80% training set & 20% test set...\n",
      "Dataset separated.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load \"X\" (the neural network's training and testing inputs)\n",
    "def load_X(X_path):\n",
    "    X_list = []\n",
    "    file = open(X_path, 'r')\n",
    "    # Read dataset from disk, dealing with text files' syntax\n",
    "    X_signal = [np.array(item, dtype=np.float32) for item in [\n",
    "               line.strip().split('\\t') for line in file]]\n",
    "    X_list.append(X_signal)\n",
    "    file.close()\n",
    "    return np.transpose(np.array(X_list), (1, 2, 0))\n",
    "\n",
    "\n",
    "# Load \"y\" (the neural network's training and testing outputs)\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    # Read dataset from disk, dealing with text file's syntax\n",
    "    y_ = np.array([elem for elem in [line.strip().split('\\t') for line in file]], \n",
    "                  dtype=np.int32)\n",
    "    file.close()\n",
    "    # Substract 1 to each output class for friendly 0-based indexing\n",
    "    return y_-1\n",
    "\n",
    "dataset_path = \"data_msg_type/\"\n",
    "X_path = dataset_path + \"semantic_sim.txt\"\n",
    "y_path = dataset_path + \"semantic_label_index.txt\"\n",
    "\n",
    "X = load_X(X_path)\n",
    "y = load_y(y_path)\n",
    "\n",
    "# Separate our training data into test and training.\n",
    "print(\"Separating data into 80% training set & 20% test set...\")\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\n",
    "    X, y, test_size=0.2, random_state=33)#add random state here...\n",
    "print(\"Dataset separated.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# param config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test features shape, labels shape, each features mean, each features standard deviation\n",
      "((20000, 140, 1), (20000, 1), 0.22152378, 0.18706892)\n",
      "the dataset is properly normalised, as expected.\n"
     ]
    }
   ],
   "source": [
    "class Config(object):\n",
    "    \"\"\"\n",
    "    A class to store parameters, the input should be feature mat of training and testing\n",
    "    Note: it would be more interesting to use a HyperOpt search space:\n",
    "    https://github.com/hyperopt/hyperopt\n",
    "    \"\"\"\n",
    "    def __init__(self, X_train, X_test):\n",
    "        # Input data\n",
    "        self.train_count = len(X_train)  # 80,000 training series\n",
    "        self.test_data_count = len(X_test)  # 20,000 testing series\n",
    "        self.n_steps = len(X_train[0])  # 140 time_steps (features) per series\n",
    "\n",
    "        # Training\n",
    "        self.learning_rate = 0.0001\n",
    "        self.lambda_loss_amount = 0.005\n",
    "        self.training_epochs = self.train_count * 100\n",
    "        self.batch_size = 1000\n",
    "        self.total_batches = self.train_count // self.batch_size\n",
    "        self.display_iter = 20000  # To show test set accuracy during training\n",
    "\n",
    "        # LSTM structure\n",
    "        # Features count is of 1 (1*3D sensors features over time)\n",
    "        self.n_inputs = len(X_train[0][0]) # 1\n",
    "        self.n_hidden = 128  # nb of neurons inside the neural network\n",
    "        self.n_classes = 14  # Final output classes\n",
    "        \n",
    "        self.W = {\n",
    "            'hidden': tf.Variable(tf.random_normal([self.n_inputs, self.n_hidden])),\n",
    "            'output': tf.Variable(tf.random_normal([self.n_hidden, self.n_classes]))}\n",
    "        self.biases = {\n",
    "            'hidden': tf.Variable(tf.random_normal([self.n_hidden], mean=1.0)),\n",
    "            'output': tf.Variable(tf.random_normal([self.n_classes]))}\n",
    "        \n",
    "config = Config(X_train, X_test)\n",
    "print(\"test features shape, labels shape, each features mean, each features standard deviation\")\n",
    "print(X_test.shape, y_test.shape,\n",
    "      np.mean(X_test), np.std(X_test))\n",
    "print(\"the dataset is properly normalised, as expected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lstm network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LSTM_Network(_X, config):\n",
    "    \"\"\"\n",
    "    Two LSTM cells are stacked which adds deepness to the neural network.\n",
    "    Args:\n",
    "        _X: ndarray feature matrix, shape: [batch_size, time_steps, n_inputs]\n",
    "        return: matrix  output shape [batch_size,n_classes]\n",
    "    \"\"\"\n",
    "    # (NOTE: This step could be greatly optimised by shaping the dataset once\n",
    "    # input shape: (batch_size, n_steps, n_input)\n",
    "    _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
    "    # Reshape to prepare input to hidden activation\n",
    "    _X = tf.reshape(_X, [-1, config.n_inputs])\n",
    "    # new shape: (n_steps*batch_size, n_input)\n",
    "\n",
    "    # Linear activation\n",
    "    _X = tf.nn.relu(tf.matmul(_X, config.W['hidden']) + config.biases['hidden'])\n",
    "    # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
    "    print(_X.shape)\n",
    "    _X = tf.split(_X, config.n_steps, 0)\n",
    "    # new shape: n_steps * (batch_size, n_hidden)sr\n",
    "\n",
    "    # Define two stacked LSTM cells (two recurrent layers deep) with tensorflow\n",
    "    lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(config.n_hidden, forget_bias=1.0, \n",
    "                  state_is_tuple=True)\n",
    "    lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(config.n_hidden, forget_bias=1.0, \n",
    "                  state_is_tuple=True)\n",
    "    lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], \n",
    "                  state_is_tuple=True)\n",
    "    # Get LSTM cell output\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
    "\n",
    "    # Get last time step's output feature for a \"many to one\" style classifier,\n",
    "    # as in the image describing RNNs at the top of this page\n",
    "    lstm_last_output = outputs[-1]\n",
    "    print(lstm_last_output.shape)\n",
    "\n",
    "    # Linear activation\n",
    "    return tf.matmul(lstm_last_output, config.W['output']) + config.biases['output']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 128)\n",
      "(?, 128)\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, config.n_steps, config.n_inputs])\n",
    "y = tf.placeholder(tf.float32, shape=[None, config.n_classes])\n",
    "\n",
    "pred_y = LSTM_Network(x, config)\n",
    "\n",
    "# Loss,optimizer,evaluation\n",
    "l2 = config.lambda_loss_amount * \\\n",
    "    sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
    "    \n",
    "# Softmax loss and L2\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred_y)) + l2\n",
    "optimizer = tf.train.AdamOptimizer(\n",
    "    learning_rate=config.learning_rate).minimize(cost)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(pred_y, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(y_):\n",
    "    \"\"\"\n",
    "    Function to encode output labels from number indexes.\n",
    "    E.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "    \"\"\"\n",
    "    y_ = y_.reshape(len(y_))\n",
    "    #n_values = int(np.max(y_)) + 1\n",
    "    n_values = config.n_classes\n",
    "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batch extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_batch_size(_train, step, batch_size):   \n",
    "    # Function to fetch a \"batch_size\" amount of data from \"(X|y)_train\" data.    \n",
    "    shape = list(_train.shape)\n",
    "    shape[0] = batch_size\n",
    "    batch_s = np.empty(shape)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Loop index\n",
    "        index = ((step-1)*batch_size + i) % len(_train)\n",
    "        batch_s[i] = _train[index] \n",
    "\n",
    "    return batch_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #1000:   Batch Loss = 10.259197, Accuracy = 0.141000002623\n",
      "Performance on test set: Batch Loss = 9.61208248138, Accuracy = 0.62600004673\n",
      "Training epochs #2000:   Batch Loss = 9.575900, Accuracy = 0.64200001955\n",
      "Performance on test set: Batch Loss = 9.41447067261, Accuracy = 0.639999985695\n",
      "Training epochs #4000:   Batch Loss = 9.345415, Accuracy = 0.633000016212\n",
      "Performance on test set: Batch Loss = 9.34604644775, Accuracy = 0.620999991894\n",
      "Training epochs #6000:   Batch Loss = 9.322807, Accuracy = 0.604000031948\n",
      "Performance on test set: Batch Loss = 9.02978801727, Accuracy = 0.646999955177\n",
      "Training epochs #8000:   Batch Loss = 9.141677, Accuracy = 0.59500002861\n",
      "Performance on test set: Batch Loss = 9.07680034637, Accuracy = 0.614000022411\n",
      "Training epochs #10000:   Batch Loss = 9.031722, Accuracy = 0.633999943733\n",
      "Performance on test set: Batch Loss = 8.95463466644, Accuracy = 0.631000041962\n",
      "Training epochs #12000:   Batch Loss = 8.926466, Accuracy = 0.618000030518\n",
      "Performance on test set: Batch Loss = 8.94066905975, Accuracy = 0.60900002718\n",
      "Training epochs #14000:   Batch Loss = 8.896339, Accuracy = 0.620000004768\n",
      "Performance on test set: Batch Loss = 8.91795158386, Accuracy = 0.620000004768\n",
      "Training epochs #16000:   Batch Loss = 8.910842, Accuracy = 0.620999991894\n",
      "Performance on test set: Batch Loss = 8.86010360718, Accuracy = 0.637000024319\n",
      "Training epochs #18000:   Batch Loss = 8.931223, Accuracy = 0.610000014305\n",
      "Performance on test set: Batch Loss = 8.82970142365, Accuracy = 0.639999985695\n",
      "Training epochs #20000:   Batch Loss = 8.825126, Accuracy = 0.631999969482\n",
      "Performance on test set: Batch Loss = 8.89001846313, Accuracy = 0.610000014305\n",
      "Training epochs #22000:   Batch Loss = 8.843361, Accuracy = 0.613999962807\n",
      "Performance on test set: Batch Loss = 8.7892036438, Accuracy = 0.641000032425\n",
      "Training epochs #24000:   Batch Loss = 8.807811, Accuracy = 0.627999961376\n",
      "Performance on test set: Batch Loss = 8.79653358459, Accuracy = 0.621000051498\n",
      "Training epochs #26000:   Batch Loss = 8.836459, Accuracy = 0.620999991894\n",
      "Performance on test set: Batch Loss = 8.72550392151, Accuracy = 0.648000001907\n",
      "Training epochs #28000:   Batch Loss = 8.766006, Accuracy = 0.620000004768\n",
      "Performance on test set: Batch Loss = 8.78790569305, Accuracy = 0.613999962807\n",
      "Training epochs #30000:   Batch Loss = 8.741990, Accuracy = 0.628000020981\n",
      "Performance on test set: Batch Loss = 8.70732307434, Accuracy = 0.633000016212\n",
      "Training epochs #32000:   Batch Loss = 8.714991, Accuracy = 0.622000038624\n",
      "Performance on test set: Batch Loss = 8.76614665985, Accuracy = 0.611999988556\n",
      "Training epochs #34000:   Batch Loss = 8.700456, Accuracy = 0.642999947071\n",
      "Performance on test set: Batch Loss = 8.74366760254, Accuracy = 0.635999977589\n",
      "Training epochs #36000:   Batch Loss = 8.679567, Accuracy = 0.650999963284\n",
      "Performance on test set: Batch Loss = 8.63106536865, Accuracy = 0.65600001812\n",
      "Training epochs #38000:   Batch Loss = 8.669144, Accuracy = 0.659999966621\n",
      "Performance on test set: Batch Loss = 8.62802791595, Accuracy = 0.656999945641\n",
      "Training epochs #40000:   Batch Loss = 8.578390, Accuracy = 0.670000016689\n",
      "Performance on test set: Batch Loss = 8.68255329132, Accuracy = 0.624999940395\n",
      "Training epochs #42000:   Batch Loss = 8.610157, Accuracy = 0.65499997139\n",
      "Performance on test set: Batch Loss = 8.59067153931, Accuracy = 0.657000005245\n",
      "Training epochs #44000:   Batch Loss = 8.604300, Accuracy = 0.648000001907\n",
      "Performance on test set: Batch Loss = 8.5813703537, Accuracy = 0.637000024319\n",
      "Training epochs #46000:   Batch Loss = 8.542478, Accuracy = 0.649999976158\n",
      "Performance on test set: Batch Loss = 8.50704193115, Accuracy = 0.676000058651\n",
      "Training epochs #48000:   Batch Loss = 8.557493, Accuracy = 0.637999951839\n",
      "Performance on test set: Batch Loss = 8.52674674988, Accuracy = 0.746999979019\n",
      "Training epochs #50000:   Batch Loss = 8.490117, Accuracy = 0.759999990463\n",
      "Performance on test set: Batch Loss = 8.43369293213, Accuracy = 0.774000048637\n",
      "Training epochs #52000:   Batch Loss = 8.484571, Accuracy = 0.739000022411\n",
      "Performance on test set: Batch Loss = 8.47689723969, Accuracy = 0.743000030518\n",
      "Training epochs #54000:   Batch Loss = 8.439253, Accuracy = 0.754999995232\n",
      "Performance on test set: Batch Loss = 8.44245243073, Accuracy = 0.743000030518\n",
      "Training epochs #56000:   Batch Loss = 8.341551, Accuracy = 0.763999998569\n",
      "Performance on test set: Batch Loss = 8.31669616699, Accuracy = 0.779000043869\n",
      "Training epochs #58000:   Batch Loss = 8.296917, Accuracy = 0.773000001907\n",
      "Performance on test set: Batch Loss = 8.28907775879, Accuracy = 0.768000006676\n",
      "Training epochs #60000:   Batch Loss = 8.312859, Accuracy = 0.757999956608\n",
      "Performance on test set: Batch Loss = 8.38943576813, Accuracy = 0.741999983788\n",
      "Training epochs #62000:   Batch Loss = 8.273218, Accuracy = 0.788999974728\n",
      "Performance on test set: Batch Loss = 8.39239215851, Accuracy = 0.756000041962\n",
      "Training epochs #64000:   Batch Loss = 8.433437, Accuracy = 0.745999991894\n",
      "Performance on test set: Batch Loss = 8.36804389954, Accuracy = 0.761000037193\n",
      "Training epochs #66000:   Batch Loss = 8.376552, Accuracy = 0.755999982357\n",
      "Performance on test set: Batch Loss = 8.29785346985, Accuracy = 0.764999985695\n",
      "Training epochs #68000:   Batch Loss = 8.318493, Accuracy = 0.768000006676\n",
      "Performance on test set: Batch Loss = 8.36431694031, Accuracy = 0.749000012875\n",
      "Training epochs #70000:   Batch Loss = 8.377679, Accuracy = 0.735999941826\n",
      "Performance on test set: Batch Loss = 8.25230789185, Accuracy = 0.773000061512\n",
      "Training epochs #72000:   Batch Loss = 8.282158, Accuracy = 0.762000024319\n",
      "Performance on test set: Batch Loss = 8.3124256134, Accuracy = 0.745000004768\n",
      "Training epochs #74000:   Batch Loss = 8.249831, Accuracy = 0.774000048637\n",
      "Performance on test set: Batch Loss = 8.28418445587, Accuracy = 0.753000020981\n",
      "Training epochs #76000:   Batch Loss = 8.254161, Accuracy = 0.761999964714\n",
      "Performance on test set: Batch Loss = 8.16918373108, Accuracy = 0.782999992371\n",
      "Training epochs #78000:   Batch Loss = 8.186664, Accuracy = 0.772000014782\n",
      "Performance on test set: Batch Loss = 8.16957855225, Accuracy = 0.773000001907\n",
      "Training epochs #80000:   Batch Loss = 8.202194, Accuracy = 0.763999998569\n",
      "Performance on test set: Batch Loss = 8.22845554352, Accuracy = 0.743000030518\n",
      "Training epochs #82000:   Batch Loss = 8.159583, Accuracy = 0.779000043869\n",
      "Performance on test set: Batch Loss = 8.18708515167, Accuracy = 0.776000022888\n",
      "Training epochs #84000:   Batch Loss = 8.134279, Accuracy = 0.777999997139\n",
      "Performance on test set: Batch Loss = 8.169962883, Accuracy = 0.768000006676\n",
      "Training epochs #86000:   Batch Loss = 8.202068, Accuracy = 0.736000001431\n",
      "Performance on test set: Batch Loss = 8.1289434433, Accuracy = 0.784000039101\n",
      "Training epochs #88000:   Batch Loss = 8.326897, Accuracy = 0.740999996662\n",
      "Performance on test set: Batch Loss = 8.27436542511, Accuracy = 0.75100004673\n",
      "Training epochs #90000:   Batch Loss = 8.289984, Accuracy = 0.766999959946\n",
      "Performance on test set: Batch Loss = 8.13015079498, Accuracy = 0.779000043869\n",
      "Training epochs #92000:   Batch Loss = 8.400246, Accuracy = 0.773999989033\n",
      "Performance on test set: Batch Loss = 8.23275756836, Accuracy = 0.750999987125\n",
      "Training epochs #94000:   Batch Loss = 8.304064, Accuracy = 0.745999991894\n",
      "Performance on test set: Batch Loss = 8.35754966736, Accuracy = 0.749000012875\n",
      "Training epochs #96000:   Batch Loss = 8.411859, Accuracy = 0.738999962807\n",
      "Performance on test set: Batch Loss = 8.2188835144, Accuracy = 0.78200006485\n",
      "Training epochs #98000:   Batch Loss = 8.298314, Accuracy = 0.739000022411\n",
      "Performance on test set: Batch Loss = 8.13836765289, Accuracy = 0.768999934196\n",
      "Training epochs #100000:   Batch Loss = 8.172478, Accuracy = 0.763000011444\n",
      "Performance on test set: Batch Loss = 8.28814029694, Accuracy = 0.740000009537\n",
      "Training epochs #102000:   Batch Loss = 8.187816, Accuracy = 0.744999945164\n",
      "Performance on test set: Batch Loss = 8.17284488678, Accuracy = 0.759000003338\n",
      "Training epochs #104000:   Batch Loss = 8.169846, Accuracy = 0.758000016212\n",
      "Performance on test set: Batch Loss = 8.18954467773, Accuracy = 0.764000058174\n",
      "Training epochs #106000:   Batch Loss = 8.216012, Accuracy = 0.740999937057\n",
      "Performance on test set: Batch Loss = 8.09543228149, Accuracy = 0.774999976158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #108000:   Batch Loss = 8.161005, Accuracy = 0.759999990463\n",
      "Performance on test set: Batch Loss = 8.1341085434, Accuracy = 0.753000020981\n",
      "Training epochs #110000:   Batch Loss = 8.107346, Accuracy = 0.763000011444\n",
      "Performance on test set: Batch Loss = 8.05840396881, Accuracy = 0.77999997139\n",
      "Training epochs #112000:   Batch Loss = 8.089188, Accuracy = 0.763999938965\n",
      "Performance on test set: Batch Loss = 8.09570503235, Accuracy = 0.766000032425\n",
      "Training epochs #114000:   Batch Loss = 8.061478, Accuracy = 0.78200006485\n",
      "Performance on test set: Batch Loss = 8.06059837341, Accuracy = 0.7650000453\n",
      "Training epochs #116000:   Batch Loss = 8.028507, Accuracy = 0.779000043869\n",
      "Performance on test set: Batch Loss = 7.98470926285, Accuracy = 0.800000011921\n",
      "Training epochs #118000:   Batch Loss = 8.012827, Accuracy = 0.774999976158\n",
      "Performance on test set: Batch Loss = 7.95600605011, Accuracy = 0.795000016689\n",
      "Training epochs #120000:   Batch Loss = 7.943536, Accuracy = 0.802999973297\n",
      "Performance on test set: Batch Loss = 8.01372528076, Accuracy = 0.791000008583\n",
      "Training epochs #122000:   Batch Loss = 7.982808, Accuracy = 0.819000065327\n",
      "Performance on test set: Batch Loss = 7.96062469482, Accuracy = 0.828000068665\n",
      "Training epochs #124000:   Batch Loss = 7.985790, Accuracy = 0.81200003624\n",
      "Performance on test set: Batch Loss = 7.96248722076, Accuracy = 0.82699996233\n",
      "Training epochs #126000:   Batch Loss = 7.929448, Accuracy = 0.824000060558\n",
      "Performance on test set: Batch Loss = 7.92600917816, Accuracy = 0.805000066757\n",
      "Training epochs #128000:   Batch Loss = 8.105714, Accuracy = 0.776000022888\n",
      "Performance on test set: Batch Loss = 8.05773162842, Accuracy = 0.771000027657\n",
      "Training epochs #130000:   Batch Loss = 8.145412, Accuracy = 0.771000087261\n",
      "Performance on test set: Batch Loss = 8.2279624939, Accuracy = 0.77999997139\n",
      "Training epochs #132000:   Batch Loss = 8.246230, Accuracy = 0.75100004673\n",
      "Performance on test set: Batch Loss = 8.09546279907, Accuracy = 0.753000020981\n",
      "Training epochs #134000:   Batch Loss = 7.990165, Accuracy = 0.765999913216\n",
      "Performance on test set: Batch Loss = 7.96333551407, Accuracy = 0.759000003338\n",
      "Training epochs #136000:   Batch Loss = 8.005041, Accuracy = 0.831999957561\n",
      "Performance on test set: Batch Loss = 8.00545883179, Accuracy = 0.819999992847\n",
      "Training epochs #138000:   Batch Loss = 7.946789, Accuracy = 0.81099998951\n",
      "Performance on test set: Batch Loss = 7.9453163147, Accuracy = 0.811999976635\n",
      "Training epochs #140000:   Batch Loss = 8.017965, Accuracy = 0.777999997139\n",
      "Performance on test set: Batch Loss = 8.05448722839, Accuracy = 0.75100004673\n",
      "Training epochs #142000:   Batch Loss = 7.913786, Accuracy = 0.805000007153\n",
      "Performance on test set: Batch Loss = 7.97712802887, Accuracy = 0.763999998569\n",
      "Training epochs #144000:   Batch Loss = 9.871390, Accuracy = 0.148000001907\n",
      "Performance on test set: Batch Loss = 8.23026275635, Accuracy = 0.768999934196\n",
      "Training epochs #146000:   Batch Loss = 9.167212, Accuracy = 0.664000034332\n",
      "Performance on test set: Batch Loss = 9.23763847351, Accuracy = 0.675000011921\n",
      "Training epochs #148000:   Batch Loss = 9.242233, Accuracy = 0.674000024796\n",
      "Performance on test set: Batch Loss = 9.31268024445, Accuracy = 0.652999997139\n",
      "Training epochs #150000:   Batch Loss = 9.190633, Accuracy = 0.647000014782\n",
      "Performance on test set: Batch Loss = 9.07129859924, Accuracy = 0.667999982834\n",
      "Training epochs #152000:   Batch Loss = 9.031832, Accuracy = 0.65600001812\n",
      "Performance on test set: Batch Loss = 9.0019493103, Accuracy = 0.629000008106\n",
      "Training epochs #154000:   Batch Loss = 8.714035, Accuracy = 0.673999965191\n",
      "Performance on test set: Batch Loss = 8.69463348389, Accuracy = 0.641999959946\n",
      "Training epochs #156000:   Batch Loss = 8.643626, Accuracy = 0.626999974251\n",
      "Performance on test set: Batch Loss = 8.4775018692, Accuracy = 0.658000051975\n",
      "Training epochs #158000:   Batch Loss = 8.448975, Accuracy = 0.667999982834\n",
      "Performance on test set: Batch Loss = 8.45875453949, Accuracy = 0.657999992371\n",
      "Training epochs #160000:   Batch Loss = 8.521160, Accuracy = 0.754000067711\n",
      "Performance on test set: Batch Loss = 8.63926696777, Accuracy = 0.740999996662\n",
      "Training epochs #162000:   Batch Loss = 8.579736, Accuracy = 0.767999947071\n",
      "Performance on test set: Batch Loss = 8.54558181763, Accuracy = 0.758000016212\n",
      "Training epochs #164000:   Batch Loss = 8.472248, Accuracy = 0.777999997139\n",
      "Performance on test set: Batch Loss = 8.51414680481, Accuracy = 0.643000006676\n",
      "Training epochs #166000:   Batch Loss = 8.591084, Accuracy = 0.620999991894\n",
      "Performance on test set: Batch Loss = 8.41242790222, Accuracy = 0.669999957085\n",
      "Training epochs #168000:   Batch Loss = 8.508122, Accuracy = 0.619000017643\n",
      "Performance on test set: Batch Loss = 8.46333408356, Accuracy = 0.636000037193\n",
      "Training epochs #170000:   Batch Loss = 8.445271, Accuracy = 0.650999963284\n",
      "Performance on test set: Batch Loss = 8.40256595612, Accuracy = 0.663999974728\n",
      "Training epochs #172000:   Batch Loss = 8.448033, Accuracy = 0.637999951839\n",
      "Performance on test set: Batch Loss = 8.45308685303, Accuracy = 0.627000033855\n",
      "Training epochs #174000:   Batch Loss = 8.431623, Accuracy = 0.635999977589\n",
      "Performance on test set: Batch Loss = 8.41898536682, Accuracy = 0.641999959946\n",
      "Training epochs #176000:   Batch Loss = 8.412728, Accuracy = 0.633000016212\n",
      "Performance on test set: Batch Loss = 8.31209468842, Accuracy = 0.667999982834\n",
      "Training epochs #178000:   Batch Loss = 8.420054, Accuracy = 0.635999977589\n",
      "Performance on test set: Batch Loss = 8.33317756653, Accuracy = 0.657999992371\n",
      "Training epochs #180000:   Batch Loss = 8.341024, Accuracy = 0.650999963284\n",
      "Performance on test set: Batch Loss = 8.39156627655, Accuracy = 0.741999983788\n",
      "Training epochs #182000:   Batch Loss = 8.372326, Accuracy = 0.745999991894\n",
      "Performance on test set: Batch Loss = 8.33092498779, Accuracy = 0.759000062943\n",
      "Training epochs #184000:   Batch Loss = 8.329753, Accuracy = 0.758000016212\n",
      "Performance on test set: Batch Loss = 8.33234596252, Accuracy = 0.762000083923\n",
      "Training epochs #186000:   Batch Loss = 8.383622, Accuracy = 0.740000009537\n",
      "Performance on test set: Batch Loss = 8.28271865845, Accuracy = 0.770000040531\n",
      "Training epochs #188000:   Batch Loss = 8.332467, Accuracy = 0.738999962807\n",
      "Performance on test set: Batch Loss = 8.3338394165, Accuracy = 0.750000059605\n",
      "Training epochs #190000:   Batch Loss = 8.299954, Accuracy = 0.755000054836\n",
      "Performance on test set: Batch Loss = 8.24837589264, Accuracy = 0.777999997139\n",
      "Training epochs #192000:   Batch Loss = 8.275385, Accuracy = 0.753000020981\n",
      "Performance on test set: Batch Loss = 8.31636714935, Accuracy = 0.747999966145\n",
      "Training epochs #194000:   Batch Loss = 8.270186, Accuracy = 0.757000029087\n",
      "Performance on test set: Batch Loss = 8.30704021454, Accuracy = 0.748000025749\n",
      "Training epochs #196000:   Batch Loss = 8.240271, Accuracy = 0.76700001955\n",
      "Performance on test set: Batch Loss = 8.18897247314, Accuracy = 0.781999945641\n",
      "Training epochs #198000:   Batch Loss = 8.237371, Accuracy = 0.760000050068\n",
      "Performance on test set: Batch Loss = 8.20505142212, Accuracy = 0.768000006676\n",
      "Training epochs #200000:   Batch Loss = 8.149476, Accuracy = 0.777999997139\n",
      "Performance on test set: Batch Loss = 8.25259399414, Accuracy = 0.742000043392\n",
      "Training epochs #202000:   Batch Loss = 8.186853, Accuracy = 0.772999942303\n",
      "Performance on test set: Batch Loss = 8.1899394989, Accuracy = 0.757999956608\n",
      "Training epochs #204000:   Batch Loss = 8.187196, Accuracy = 0.767999947071\n",
      "Performance on test set: Batch Loss = 8.18793296814, Accuracy = 0.763000011444\n",
      "Training epochs #206000:   Batch Loss = 8.155451, Accuracy = 0.768999993801\n",
      "Performance on test set: Batch Loss = 8.13763141632, Accuracy = 0.769000053406\n",
      "Training epochs #208000:   Batch Loss = 8.193819, Accuracy = 0.745999991894\n",
      "Performance on test set: Batch Loss = 8.17194271088, Accuracy = 0.75\n",
      "Training epochs #210000:   Batch Loss = 8.142125, Accuracy = 0.764000058174\n",
      "Performance on test set: Batch Loss = 8.09735774994, Accuracy = 0.777999997139\n",
      "Training epochs #212000:   Batch Loss = 8.158607, Accuracy = 0.742999970913\n",
      "Performance on test set: Batch Loss = 8.16805934906, Accuracy = 0.747999966145\n",
      "Training epochs #214000:   Batch Loss = 8.126364, Accuracy = 0.759999990463\n",
      "Performance on test set: Batch Loss = 8.15623188019, Accuracy = 0.749000072479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #216000:   Batch Loss = 8.057223, Accuracy = 0.771999955177\n",
      "Performance on test set: Batch Loss = 8.03833961487, Accuracy = 0.782999992371\n",
      "Training epochs #218000:   Batch Loss = 8.029189, Accuracy = 0.777000010014\n",
      "Performance on test set: Batch Loss = 8.02750873566, Accuracy = 0.766999959946\n",
      "Training epochs #220000:   Batch Loss = 8.049044, Accuracy = 0.759999990463\n",
      "Performance on test set: Batch Loss = 8.08484649658, Accuracy = 0.740999996662\n",
      "Training epochs #222000:   Batch Loss = 7.946832, Accuracy = 0.793999969959\n",
      "Performance on test set: Batch Loss = 8.02042388916, Accuracy = 0.763000011444\n",
      "Training epochs #224000:   Batch Loss = 8.014565, Accuracy = 0.755000054836\n",
      "Performance on test set: Batch Loss = 8.00082683563, Accuracy = 0.769000053406\n",
      "Training epochs #226000:   Batch Loss = 7.974049, Accuracy = 0.763000011444\n",
      "Performance on test set: Batch Loss = 7.92757606506, Accuracy = 0.771999955177\n",
      "Training epochs #228000:   Batch Loss = 7.915447, Accuracy = 0.782999992371\n",
      "Performance on test set: Batch Loss = 7.92719459534, Accuracy = 0.769000053406\n",
      "Training epochs #230000:   Batch Loss = 7.970973, Accuracy = 0.756000041962\n",
      "Performance on test set: Batch Loss = 7.87406826019, Accuracy = 0.78100001812\n",
      "Training epochs #232000:   Batch Loss = 7.863224, Accuracy = 0.784999966621\n",
      "Performance on test set: Batch Loss = 7.92163276672, Accuracy = 0.755999922752\n",
      "Training epochs #234000:   Batch Loss = 7.831137, Accuracy = 0.777000069618\n",
      "Performance on test set: Batch Loss = 7.89198541641, Accuracy = 0.757000029087\n",
      "Training epochs #236000:   Batch Loss = 7.876583, Accuracy = 0.769999980927\n",
      "Performance on test set: Batch Loss = 7.81319713593, Accuracy = 0.791000008583\n",
      "Training epochs #238000:   Batch Loss = 7.812798, Accuracy = 0.78200006485\n",
      "Performance on test set: Batch Loss = 7.8001332283, Accuracy = 0.787999987602\n",
      "Training epochs #240000:   Batch Loss = 7.848306, Accuracy = 0.766000032425\n",
      "Performance on test set: Batch Loss = 7.8373131752, Accuracy = 0.749999940395\n",
      "Training epochs #242000:   Batch Loss = 7.765264, Accuracy = 0.779000043869\n",
      "Performance on test set: Batch Loss = 7.8083820343, Accuracy = 0.774999976158\n",
      "Training epochs #244000:   Batch Loss = 7.766762, Accuracy = 0.790000021458\n",
      "Performance on test set: Batch Loss = 7.81641244888, Accuracy = 0.794000029564\n",
      "Training epochs #246000:   Batch Loss = 7.805614, Accuracy = 0.791999995708\n",
      "Performance on test set: Batch Loss = 7.76576042175, Accuracy = 0.791000008583\n",
      "Training epochs #248000:   Batch Loss = 7.806305, Accuracy = 0.799000024796\n",
      "Performance on test set: Batch Loss = 7.7833070755, Accuracy = 0.817999958992\n",
      "Training epochs #250000:   Batch Loss = 7.798148, Accuracy = 0.816999971867\n",
      "Performance on test set: Batch Loss = 7.74006938934, Accuracy = 0.816999971867\n",
      "Training epochs #252000:   Batch Loss = 7.743478, Accuracy = 0.827000021935\n",
      "Performance on test set: Batch Loss = 7.79622936249, Accuracy = 0.787000000477\n",
      "Training epochs #254000:   Batch Loss = 7.736182, Accuracy = 0.818000018597\n",
      "Performance on test set: Batch Loss = 7.74141550064, Accuracy = 0.809000015259\n",
      "Training epochs #256000:   Batch Loss = 7.770491, Accuracy = 0.809000015259\n",
      "Performance on test set: Batch Loss = 7.69620990753, Accuracy = 0.822999954224\n",
      "Training epochs #258000:   Batch Loss = 7.774805, Accuracy = 0.820999979973\n",
      "Performance on test set: Batch Loss = 7.6681728363, Accuracy = 0.848000049591\n",
      "Training epochs #260000:   Batch Loss = 7.727288, Accuracy = 0.861000061035\n",
      "Performance on test set: Batch Loss = 7.70242452621, Accuracy = 0.827000021935\n",
      "Training epochs #262000:   Batch Loss = 7.739419, Accuracy = 0.828999996185\n",
      "Performance on test set: Batch Loss = 7.69716405869, Accuracy = 0.824999988079\n",
      "Training epochs #264000:   Batch Loss = 7.656284, Accuracy = 0.861999988556\n",
      "Performance on test set: Batch Loss = 7.71898078918, Accuracy = 0.851000070572\n",
      "Training epochs #266000:   Batch Loss = 7.745946, Accuracy = 0.828999996185\n",
      "Performance on test set: Batch Loss = 7.66086387634, Accuracy = 0.847000002861\n",
      "Training epochs #268000:   Batch Loss = 7.696990, Accuracy = 0.847999930382\n",
      "Performance on test set: Batch Loss = 7.67623329163, Accuracy = 0.846999943256\n",
      "Training epochs #270000:   Batch Loss = 7.653760, Accuracy = 0.851000010967\n",
      "Performance on test set: Batch Loss = 7.6435251236, Accuracy = 0.852000057697\n",
      "Training epochs #272000:   Batch Loss = 7.693341, Accuracy = 0.830999970436\n",
      "Performance on test set: Batch Loss = 7.70280742645, Accuracy = 0.828000068665\n",
      "Training epochs #274000:   Batch Loss = 7.638699, Accuracy = 0.85799998045\n",
      "Performance on test set: Batch Loss = 7.67146205902, Accuracy = 0.837999999523\n",
      "Training epochs #276000:   Batch Loss = 7.654503, Accuracy = 0.84500002861\n",
      "Performance on test set: Batch Loss = 7.63707065582, Accuracy = 0.851999998093\n",
      "Training epochs #278000:   Batch Loss = 7.648749, Accuracy = 0.844000101089\n",
      "Performance on test set: Batch Loss = 7.6130862236, Accuracy = 0.865999937057\n",
      "Training epochs #280000:   Batch Loss = 7.599010, Accuracy = 0.85799998045\n",
      "Performance on test set: Batch Loss = 7.64540529251, Accuracy = 0.846000015736\n",
      "Training epochs #282000:   Batch Loss = 7.651525, Accuracy = 0.851000010967\n",
      "Performance on test set: Batch Loss = 7.6462392807, Accuracy = 0.874000012875\n",
      "Training epochs #284000:   Batch Loss = 7.682370, Accuracy = 0.851999998093\n",
      "Performance on test set: Batch Loss = 7.66817760468, Accuracy = 0.865000009537\n",
      "Training epochs #286000:   Batch Loss = 7.607538, Accuracy = 0.867000102997\n",
      "Performance on test set: Batch Loss = 7.61248493195, Accuracy = 0.847999989986\n",
      "Training epochs #288000:   Batch Loss = 7.625187, Accuracy = 0.847000002861\n",
      "Performance on test set: Batch Loss = 7.63903331757, Accuracy = 0.847000002861\n",
      "Training epochs #290000:   Batch Loss = 7.626267, Accuracy = 0.850000083447\n",
      "Performance on test set: Batch Loss = 7.59867477417, Accuracy = 0.852999985218\n",
      "Training epochs #292000:   Batch Loss = 7.628423, Accuracy = 0.837000012398\n",
      "Performance on test set: Batch Loss = 7.66861057281, Accuracy = 0.825999975204\n",
      "Training epochs #294000:   Batch Loss = 7.638269, Accuracy = 0.830999910831\n",
      "Performance on test set: Batch Loss = 7.62045526505, Accuracy = 0.841000020504\n",
      "Training epochs #296000:   Batch Loss = 7.594287, Accuracy = 0.850000083447\n",
      "Performance on test set: Batch Loss = 7.59793710709, Accuracy = 0.852000057697\n",
      "Training epochs #298000:   Batch Loss = 7.609205, Accuracy = 0.864000022411\n",
      "Performance on test set: Batch Loss = 7.57733774185, Accuracy = 0.887000024319\n",
      "Training epochs #300000:   Batch Loss = 7.597580, Accuracy = 0.866999983788\n",
      "Performance on test set: Batch Loss = 7.60378456116, Accuracy = 0.843000054359\n",
      "Training epochs #302000:   Batch Loss = 7.529530, Accuracy = 0.869000017643\n",
      "Performance on test set: Batch Loss = 7.61966705322, Accuracy = 0.856000065804\n",
      "Training epochs #304000:   Batch Loss = 7.613211, Accuracy = 0.842999994755\n",
      "Performance on test set: Batch Loss = 7.65689945221, Accuracy = 0.851000010967\n",
      "Training epochs #306000:   Batch Loss = 7.661051, Accuracy = 0.815999984741\n",
      "Performance on test set: Batch Loss = 7.58967208862, Accuracy = 0.847000062466\n",
      "Training epochs #308000:   Batch Loss = 7.558445, Accuracy = 0.852999985218\n",
      "Performance on test set: Batch Loss = 7.60740470886, Accuracy = 0.849000036716\n",
      "Training epochs #310000:   Batch Loss = 7.731804, Accuracy = 0.84600007534\n",
      "Performance on test set: Batch Loss = 7.6389465332, Accuracy = 0.863999962807\n",
      "Training epochs #312000:   Batch Loss = 7.925249, Accuracy = 0.803000032902\n",
      "Performance on test set: Batch Loss = 7.96311378479, Accuracy = 0.773999989033\n",
      "Training epochs #314000:   Batch Loss = 7.654348, Accuracy = 0.815999984741\n",
      "Performance on test set: Batch Loss = 7.58852720261, Accuracy = 0.863000035286\n",
      "Training epochs #316000:   Batch Loss = 7.616424, Accuracy = 0.848000049591\n",
      "Performance on test set: Batch Loss = 8.30858802795, Accuracy = 0.204999983311\n",
      "Training epochs #318000:   Batch Loss = 10.800433, Accuracy = 0.152999997139\n",
      "Performance on test set: Batch Loss = 8.31219577789, Accuracy = 0.787000060081\n",
      "Training epochs #320000:   Batch Loss = 9.140041, Accuracy = 0.657999992371\n",
      "Performance on test set: Batch Loss = 9.83969783783, Accuracy = 0.636999964714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #322000:   Batch Loss = 9.794882, Accuracy = 0.663999974728\n",
      "Performance on test set: Batch Loss = 9.64311599731, Accuracy = 0.759000003338\n",
      "Training epochs #324000:   Batch Loss = 9.344605, Accuracy = 0.777999997139\n",
      "Performance on test set: Batch Loss = 9.25193595886, Accuracy = 0.761999964714\n",
      "Training epochs #326000:   Batch Loss = 9.183348, Accuracy = 0.728999972343\n",
      "Performance on test set: Batch Loss = 8.6382932663, Accuracy = 0.769000053406\n",
      "Training epochs #328000:   Batch Loss = 8.505051, Accuracy = 0.742000043392\n",
      "Performance on test set: Batch Loss = 8.27048110962, Accuracy = 0.750999987125\n",
      "Training epochs #330000:   Batch Loss = 8.318761, Accuracy = 0.771999955177\n",
      "Performance on test set: Batch Loss = 8.49013996124, Accuracy = 0.782999992371\n",
      "Training epochs #332000:   Batch Loss = 8.502356, Accuracy = 0.760999977589\n",
      "Performance on test set: Batch Loss = 8.4412317276, Accuracy = 0.750999987125\n",
      "Training epochs #334000:   Batch Loss = 8.325209, Accuracy = 0.745999991894\n",
      "Performance on test set: Batch Loss = 8.29452419281, Accuracy = 0.74899995327\n",
      "Training epochs #336000:   Batch Loss = 8.205015, Accuracy = 0.738000035286\n",
      "Performance on test set: Batch Loss = 8.02380752563, Accuracy = 0.780000090599\n",
      "Training epochs #338000:   Batch Loss = 8.112241, Accuracy = 0.739000022411\n",
      "Performance on test set: Batch Loss = 8.04890346527, Accuracy = 0.768000006676\n",
      "Training epochs #340000:   Batch Loss = 8.130946, Accuracy = 0.771999955177\n",
      "Performance on test set: Batch Loss = 8.16606235504, Accuracy = 0.770000040531\n",
      "Training epochs #342000:   Batch Loss = 8.202562, Accuracy = 0.766000032425\n",
      "Performance on test set: Batch Loss = 8.13488101959, Accuracy = 0.784000039101\n",
      "Training epochs #344000:   Batch Loss = 8.049298, Accuracy = 0.788999974728\n",
      "Performance on test set: Batch Loss = 8.0612859726, Accuracy = 0.770000040531\n",
      "Training epochs #346000:   Batch Loss = 8.105860, Accuracy = 0.740000009537\n",
      "Performance on test set: Batch Loss = 7.98097848892, Accuracy = 0.769000053406\n",
      "Training epochs #348000:   Batch Loss = 8.123588, Accuracy = 0.740000069141\n",
      "Performance on test set: Batch Loss = 7.99749088287, Accuracy = 0.75\n",
      "Training epochs #350000:   Batch Loss = 8.024555, Accuracy = 0.756000041962\n",
      "Performance on test set: Batch Loss = 7.93807506561, Accuracy = 0.779000043869\n",
      "Training epochs #352000:   Batch Loss = 8.057685, Accuracy = 0.753000020981\n",
      "Performance on test set: Batch Loss = 8.03687572479, Accuracy = 0.75\n",
      "Training epochs #354000:   Batch Loss = 8.011005, Accuracy = 0.758000016212\n",
      "Performance on test set: Batch Loss = 8.03915596008, Accuracy = 0.747999966145\n",
      "Training epochs #356000:   Batch Loss = 7.956997, Accuracy = 0.768000006676\n",
      "Performance on test set: Batch Loss = 7.93109512329, Accuracy = 0.77999997139\n",
      "Training epochs #358000:   Batch Loss = 7.985783, Accuracy = 0.759000003338\n",
      "Performance on test set: Batch Loss = 7.94405460358, Accuracy = 0.768999993801\n",
      "Training epochs #360000:   Batch Loss = 7.895111, Accuracy = 0.777000010014\n",
      "Performance on test set: Batch Loss = 7.99325466156, Accuracy = 0.741000056267\n",
      "Training epochs #362000:   Batch Loss = 7.919729, Accuracy = 0.773000001907\n",
      "Performance on test set: Batch Loss = 7.97895765305, Accuracy = 0.758000075817\n",
      "Training epochs #364000:   Batch Loss = 7.933280, Accuracy = 0.768000006676\n",
      "Performance on test set: Batch Loss = 7.94399881363, Accuracy = 0.761999964714\n",
      "Training epochs #366000:   Batch Loss = 7.897547, Accuracy = 0.768000006676\n",
      "Performance on test set: Batch Loss = 7.90878105164, Accuracy = 0.769000053406\n",
      "Training epochs #368000:   Batch Loss = 7.987054, Accuracy = 0.748000025749\n",
      "Performance on test set: Batch Loss = 7.93650913239, Accuracy = 0.760000050068\n",
      "Training epochs #370000:   Batch Loss = 7.926832, Accuracy = 0.780000030994\n",
      "Performance on test set: Batch Loss = 7.88446187973, Accuracy = 0.791999995708\n",
      "Training epochs #372000:   Batch Loss = 7.960402, Accuracy = 0.763000011444\n",
      "Performance on test set: Batch Loss = 7.97011566162, Accuracy = 0.773000001907\n",
      "Training epochs #374000:   Batch Loss = 7.925400, Accuracy = 0.756000041962\n",
      "Performance on test set: Batch Loss = 7.97257947922, Accuracy = 0.745000004768\n",
      "Training epochs #376000:   Batch Loss = 7.878367, Accuracy = 0.769999980927\n",
      "Performance on test set: Batch Loss = 7.85942459106, Accuracy = 0.78100001812\n",
      "Training epochs #378000:   Batch Loss = 7.864211, Accuracy = 0.776000022888\n",
      "Performance on test set: Batch Loss = 7.87665843964, Accuracy = 0.768999993801\n",
      "Training epochs #380000:   Batch Loss = 7.911723, Accuracy = 0.760999977589\n",
      "Performance on test set: Batch Loss = 7.93499946594, Accuracy = 0.741999983788\n",
      "Training epochs #382000:   Batch Loss = 7.834009, Accuracy = 0.790000021458\n",
      "Performance on test set: Batch Loss = 7.91779279709, Accuracy = 0.761999964714\n",
      "Training epochs #384000:   Batch Loss = 7.927766, Accuracy = 0.750000059605\n",
      "Performance on test set: Batch Loss = 7.8998632431, Accuracy = 0.759999990463\n",
      "Training epochs #386000:   Batch Loss = 7.929428, Accuracy = 0.756000041962\n",
      "Performance on test set: Batch Loss = 7.87003803253, Accuracy = 0.76700001955\n",
      "Training epochs #388000:   Batch Loss = 7.874940, Accuracy = 0.76700001955\n",
      "Performance on test set: Batch Loss = 7.89467573166, Accuracy = 0.754000008106\n",
      "Training epochs #390000:   Batch Loss = 7.967422, Accuracy = 0.737999975681\n",
      "Performance on test set: Batch Loss = 7.85127496719, Accuracy = 0.774999976158\n",
      "Training epochs #392000:   Batch Loss = 7.877107, Accuracy = 0.768999993801\n",
      "Performance on test set: Batch Loss = 7.92922115326, Accuracy = 0.745000004768\n",
      "Training epochs #394000:   Batch Loss = 7.843641, Accuracy = 0.775000035763\n",
      "Performance on test set: Batch Loss = 7.92806529999, Accuracy = 0.746000051498\n",
      "Training epochs #396000:   Batch Loss = 7.905040, Accuracy = 0.756999969482\n",
      "Performance on test set: Batch Loss = 7.82145929337, Accuracy = 0.784000098705\n",
      "Training epochs #398000:   Batch Loss = 7.843743, Accuracy = 0.76599997282\n",
      "Performance on test set: Batch Loss = 7.83763504028, Accuracy = 0.768999993801\n",
      "Training epochs #400000:   Batch Loss = 7.890278, Accuracy = 0.754999995232\n",
      "Performance on test set: Batch Loss = 7.89470767975, Accuracy = 0.743999958038\n",
      "Training epochs #402000:   Batch Loss = 7.852374, Accuracy = 0.769000053406\n",
      "Performance on test set: Batch Loss = 7.8803730011, Accuracy = 0.763000071049\n",
      "Training epochs #404000:   Batch Loss = 7.795016, Accuracy = 0.779000043869\n",
      "Performance on test set: Batch Loss = 7.86546516418, Accuracy = 0.761000037193\n",
      "Training epochs #406000:   Batch Loss = 7.893832, Accuracy = 0.731000006199\n",
      "Performance on test set: Batch Loss = 7.83088254929, Accuracy = 0.768000006676\n",
      "Training epochs #408000:   Batch Loss = 7.882763, Accuracy = 0.743000030518\n",
      "Performance on test set: Batch Loss = 7.85200166702, Accuracy = 0.755999982357\n",
      "Training epochs #410000:   Batch Loss = 7.877649, Accuracy = 0.764000058174\n",
      "Performance on test set: Batch Loss = 7.81037712097, Accuracy = 0.774999976158\n",
      "Training epochs #412000:   Batch Loss = 7.860630, Accuracy = 0.759000003338\n",
      "Performance on test set: Batch Loss = 7.8870973587, Accuracy = 0.745000064373\n",
      "Training epochs #414000:   Batch Loss = 7.866238, Accuracy = 0.742000043392\n",
      "Performance on test set: Batch Loss = 7.87596225739, Accuracy = 0.746000051498\n",
      "Training epochs #416000:   Batch Loss = 7.895924, Accuracy = 0.739000022411\n",
      "Performance on test set: Batch Loss = 7.77445650101, Accuracy = 0.786000013351\n",
      "Training epochs #418000:   Batch Loss = 7.865579, Accuracy = 0.739000022411\n",
      "Performance on test set: Batch Loss = 7.77940559387, Accuracy = 0.768999934196\n",
      "Training epochs #420000:   Batch Loss = 7.839011, Accuracy = 0.76700001955\n",
      "Performance on test set: Batch Loss = 7.83492612839, Accuracy = 0.743000030518\n",
      "Training epochs #422000:   Batch Loss = 7.855214, Accuracy = 0.745999991894\n",
      "Performance on test set: Batch Loss = 7.8236579895, Accuracy = 0.762999951839\n",
      "Training epochs #424000:   Batch Loss = 7.774004, Accuracy = 0.758000016212\n",
      "Performance on test set: Batch Loss = 7.80658149719, Accuracy = 0.760000050068\n",
      "Training epochs #426000:   Batch Loss = 7.857131, Accuracy = 0.745000004768\n",
      "Performance on test set: Batch Loss = 7.77211046219, Accuracy = 0.768000006676\n",
      "Training epochs #428000:   Batch Loss = 7.840425, Accuracy = 0.740999996662\n",
      "Performance on test set: Batch Loss = 7.78936576843, Accuracy = 0.754000008106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #430000:   Batch Loss = 7.804076, Accuracy = 0.759000062943\n",
      "Performance on test set: Batch Loss = 7.74363183975, Accuracy = 0.774999976158\n",
      "Training epochs #432000:   Batch Loss = 7.803142, Accuracy = 0.756000041962\n",
      "Performance on test set: Batch Loss = 7.81162786484, Accuracy = 0.745999991894\n",
      "Training epochs #434000:   Batch Loss = 7.770368, Accuracy = 0.762000083923\n",
      "Performance on test set: Batch Loss = 7.78718852997, Accuracy = 0.746999979019\n",
      "Training epochs #436000:   Batch Loss = 7.735490, Accuracy = 0.76700001955\n",
      "Performance on test set: Batch Loss = 7.69552564621, Accuracy = 0.785999953747\n",
      "Training epochs #438000:   Batch Loss = 7.738898, Accuracy = 0.761000037193\n",
      "Performance on test set: Batch Loss = 7.68194627762, Accuracy = 0.788000047207\n",
      "Training epochs #440000:   Batch Loss = 7.659553, Accuracy = 0.78100001812\n",
      "Performance on test set: Batch Loss = 7.73155403137, Accuracy = 0.75\n",
      "Training epochs #442000:   Batch Loss = 7.700771, Accuracy = 0.786999940872\n",
      "Performance on test set: Batch Loss = 7.71854782104, Accuracy = 0.767999947071\n",
      "Training epochs #444000:   Batch Loss = 7.709339, Accuracy = 0.773000001907\n",
      "Performance on test set: Batch Loss = 7.69739437103, Accuracy = 0.768000006676\n",
      "Training epochs #446000:   Batch Loss = 7.651376, Accuracy = 0.777999997139\n",
      "Performance on test set: Batch Loss = 7.65377807617, Accuracy = 0.773999989033\n",
      "Training epochs #448000:   Batch Loss = 7.699652, Accuracy = 0.766000032425\n",
      "Performance on test set: Batch Loss = 7.66842937469, Accuracy = 0.774999976158\n",
      "Training epochs #450000:   Batch Loss = 7.671841, Accuracy = 0.778999984264\n",
      "Performance on test set: Batch Loss = 7.6143989563, Accuracy = 0.791999936104\n",
      "Training epochs #452000:   Batch Loss = 7.645347, Accuracy = 0.757000029087\n",
      "Performance on test set: Batch Loss = 7.6708316803, Accuracy = 0.769999980927\n",
      "Training epochs #454000:   Batch Loss = 7.608277, Accuracy = 0.787999987602\n",
      "Performance on test set: Batch Loss = 7.62249040604, Accuracy = 0.804000079632\n",
      "Training epochs #456000:   Batch Loss = 7.584278, Accuracy = 0.819999992847\n",
      "Performance on test set: Batch Loss = 7.56931877136, Accuracy = 0.819000005722\n",
      "Training epochs #458000:   Batch Loss = 7.549954, Accuracy = 0.819000005722\n",
      "Performance on test set: Batch Loss = 7.52878713608, Accuracy = 0.845999956131\n",
      "Training epochs #460000:   Batch Loss = 7.556282, Accuracy = 0.817999958992\n",
      "Performance on test set: Batch Loss = 7.56514167786, Accuracy = 0.80999994278\n",
      "Training epochs #462000:   Batch Loss = 7.496657, Accuracy = 0.849000096321\n",
      "Performance on test set: Batch Loss = 7.55546998978, Accuracy = 0.82800000906\n",
      "Training epochs #464000:   Batch Loss = 7.529447, Accuracy = 0.817000031471\n",
      "Performance on test set: Batch Loss = 7.55107212067, Accuracy = 0.831000030041\n",
      "Training epochs #466000:   Batch Loss = 7.554004, Accuracy = 0.818000018597\n",
      "Performance on test set: Batch Loss = 7.50273942947, Accuracy = 0.819000005722\n",
      "Training epochs #468000:   Batch Loss = 7.498022, Accuracy = 0.828000068665\n",
      "Performance on test set: Batch Loss = 7.51826524734, Accuracy = 0.819000005722\n",
      "Training epochs #470000:   Batch Loss = 7.573155, Accuracy = 0.81099998951\n",
      "Performance on test set: Batch Loss = 7.4732093811, Accuracy = 0.829999983311\n",
      "Training epochs #472000:   Batch Loss = 7.460112, Accuracy = 0.844000041485\n",
      "Performance on test set: Batch Loss = 7.53069162369, Accuracy = 0.827000021935\n",
      "Training epochs #474000:   Batch Loss = 7.460752, Accuracy = 0.863000035286\n",
      "Performance on test set: Batch Loss = 7.52667140961, Accuracy = 0.816000044346\n",
      "Training epochs #476000:   Batch Loss = 7.529829, Accuracy = 0.848000049591\n",
      "Performance on test set: Batch Loss = 7.46091461182, Accuracy = 0.867000102997\n",
      "Training epochs #478000:   Batch Loss = 7.454664, Accuracy = 0.838000059128\n",
      "Performance on test set: Batch Loss = 7.43096494675, Accuracy = 0.856999993324\n",
      "Training epochs #480000:   Batch Loss = 7.485964, Accuracy = 0.853000044823\n",
      "Performance on test set: Batch Loss = 7.46866178513, Accuracy = 0.855000019073\n",
      "Training epochs #482000:   Batch Loss = 7.435608, Accuracy = 0.863000035286\n",
      "Performance on test set: Batch Loss = 7.47697067261, Accuracy = 0.865000069141\n",
      "Training epochs #484000:   Batch Loss = 7.430286, Accuracy = 0.849000096321\n",
      "Performance on test set: Batch Loss = 7.48107528687, Accuracy = 0.862000048161\n",
      "Training epochs #486000:   Batch Loss = 7.442018, Accuracy = 0.862000048161\n",
      "Performance on test set: Batch Loss = 7.42234992981, Accuracy = 0.868000030518\n",
      "Training epochs #488000:   Batch Loss = 7.472078, Accuracy = 0.861999988556\n",
      "Performance on test set: Batch Loss = 7.45899009705, Accuracy = 0.865000009537\n",
      "Training epochs #490000:   Batch Loss = 7.498859, Accuracy = 0.861000001431\n",
      "Performance on test set: Batch Loss = 7.4062333107, Accuracy = 0.885000050068\n",
      "Training epochs #492000:   Batch Loss = 7.488362, Accuracy = 0.869000077248\n",
      "Performance on test set: Batch Loss = 8.99823760986, Accuracy = 0.197999984026\n",
      "Training epochs #494000:   Batch Loss = 8.839154, Accuracy = 0.202999979258\n",
      "Performance on test set: Batch Loss = 8.47347068787, Accuracy = 0.74899995327\n",
      "Training epochs #496000:   Batch Loss = 8.311349, Accuracy = 0.738000035286\n",
      "Performance on test set: Batch Loss = 8.11954593658, Accuracy = 0.785999953747\n",
      "Training epochs #498000:   Batch Loss = 8.368535, Accuracy = 0.738000035286\n",
      "Performance on test set: Batch Loss = 8.3438835144, Accuracy = 0.76700001955\n",
      "Training epochs #500000:   Batch Loss = 8.436701, Accuracy = 0.763000011444\n",
      "Performance on test set: Batch Loss = 8.52515602112, Accuracy = 0.742000043392\n",
      "Training epochs #502000:   Batch Loss = 8.407433, Accuracy = 0.744000017643\n",
      "Performance on test set: Batch Loss = 8.20583629608, Accuracy = 0.762999951839\n",
      "Training epochs #504000:   Batch Loss = 8.038124, Accuracy = 0.759000003338\n",
      "Performance on test set: Batch Loss = 7.95638132095, Accuracy = 0.761000037193\n",
      "Training epochs #506000:   Batch Loss = 7.944034, Accuracy = 0.744000017643\n",
      "Performance on test set: Batch Loss = 7.81692075729, Accuracy = 0.768000006676\n",
      "Training epochs #508000:   Batch Loss = 7.909069, Accuracy = 0.75\n",
      "Performance on test set: Batch Loss = 7.97186374664, Accuracy = 0.817000031471\n",
      "Training epochs #510000:   Batch Loss = 8.007662, Accuracy = 0.806000053883\n",
      "Performance on test set: Batch Loss = 7.81171226501, Accuracy = 0.787000000477\n",
      "Training epochs #512000:   Batch Loss = 7.776890, Accuracy = 0.768000006676\n",
      "Performance on test set: Batch Loss = 7.74130582809, Accuracy = 0.756999969482\n",
      "Training epochs #514000:   Batch Loss = 7.714942, Accuracy = 0.762000024319\n",
      "Performance on test set: Batch Loss = 7.75834798813, Accuracy = 0.756000041962\n",
      "Training epochs #516000:   Batch Loss = 7.698744, Accuracy = 0.785000026226\n",
      "Performance on test set: Batch Loss = 7.66066408157, Accuracy = 0.804000079632\n",
      "Training epochs #518000:   Batch Loss = 7.681279, Accuracy = 0.77999997139\n",
      "Performance on test set: Batch Loss = 7.58264446259, Accuracy = 0.800000011921\n",
      "Training epochs #520000:   Batch Loss = 7.525079, Accuracy = 0.801999986172\n",
      "Performance on test set: Batch Loss = 7.58528995514, Accuracy = 0.766000032425\n",
      "Training epochs #522000:   Batch Loss = 7.569273, Accuracy = 0.800000011921\n",
      "Performance on test set: Batch Loss = 7.58735656738, Accuracy = 0.832000017166\n",
      "Training epochs #524000:   Batch Loss = 7.572361, Accuracy = 0.851000070572\n",
      "Performance on test set: Batch Loss = 7.57442903519, Accuracy = 0.850000023842\n",
      "Training epochs #526000:   Batch Loss = 7.534301, Accuracy = 0.847000002861\n",
      "Performance on test set: Batch Loss = 7.53149318695, Accuracy = 0.850000023842\n",
      "Training epochs #528000:   Batch Loss = 7.538357, Accuracy = 0.858000040054\n",
      "Performance on test set: Batch Loss = 7.48316860199, Accuracy = 0.876000106335\n",
      "Training epochs #530000:   Batch Loss = 7.473471, Accuracy = 0.873000025749\n",
      "Performance on test set: Batch Loss = 7.41085100174, Accuracy = 0.884000062943\n",
      "Training epochs #532000:   Batch Loss = 7.435367, Accuracy = 0.865000009537\n",
      "Performance on test set: Batch Loss = 7.46466350555, Accuracy = 0.820999979973\n",
      "Training epochs #534000:   Batch Loss = 7.409864, Accuracy = 0.815999984741\n",
      "Performance on test set: Batch Loss = 7.42026853561, Accuracy = 0.816000103951\n",
      "Training epochs #536000:   Batch Loss = 7.397579, Accuracy = 0.844999969006\n",
      "Performance on test set: Batch Loss = 7.40953922272, Accuracy = 0.847000002861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #538000:   Batch Loss = 7.383328, Accuracy = 0.853999912739\n",
      "Performance on test set: Batch Loss = 7.5006737709, Accuracy = 0.847000002861\n",
      "Training epochs #540000:   Batch Loss = 8.788016, Accuracy = 0.207000002265\n",
      "Performance on test set: Batch Loss = 8.71130561829, Accuracy = 0.223000004888\n",
      "Training epochs #542000:   Batch Loss = 8.318854, Accuracy = 0.851999998093\n",
      "Performance on test set: Batch Loss = 8.08092021942, Accuracy = 0.784000039101\n",
      "Training epochs #544000:   Batch Loss = 8.025238, Accuracy = 0.757000088692\n",
      "Performance on test set: Batch Loss = 8.12131881714, Accuracy = 0.762000024319\n",
      "Training epochs #546000:   Batch Loss = 8.224986, Accuracy = 0.756999969482\n",
      "Performance on test set: Batch Loss = 8.18080329895, Accuracy = 0.770999968052\n",
      "Training epochs #548000:   Batch Loss = 8.128498, Accuracy = 0.769000053406\n",
      "Performance on test set: Batch Loss = 8.15836143494, Accuracy = 0.75\n",
      "Training epochs #550000:   Batch Loss = 8.143463, Accuracy = 0.739000022411\n",
      "Performance on test set: Batch Loss = 7.90833377838, Accuracy = 0.780000090599\n",
      "Training epochs #552000:   Batch Loss = 8.125757, Accuracy = 0.768000006676\n",
      "Performance on test set: Batch Loss = 8.02844047546, Accuracy = 0.752999961376\n",
      "Training epochs #554000:   Batch Loss = 7.794741, Accuracy = 0.779000043869\n",
      "Performance on test set: Batch Loss = 7.94513511658, Accuracy = 0.769999980927\n",
      "Training epochs #556000:   Batch Loss = 7.948523, Accuracy = 0.789000034332\n",
      "Performance on test set: Batch Loss = 7.84418487549, Accuracy = 0.801000058651\n",
      "Training epochs #558000:   Batch Loss = 7.853916, Accuracy = 0.799000024796\n",
      "Performance on test set: Batch Loss = 7.75822210312, Accuracy = 0.811000049114\n",
      "Training epochs #560000:   Batch Loss = 7.730453, Accuracy = 0.810000002384\n",
      "Performance on test set: Batch Loss = 7.67025375366, Accuracy = 0.828999996185\n",
      "Training epochs #562000:   Batch Loss = 7.616376, Accuracy = 0.82699996233\n",
      "Performance on test set: Batch Loss = 7.62417316437, Accuracy = 0.828999996185\n",
      "Training epochs #564000:   Batch Loss = 7.511722, Accuracy = 0.840000033379\n",
      "Performance on test set: Batch Loss = 7.57967472076, Accuracy = 0.82699996233\n",
      "Training epochs #566000:   Batch Loss = 7.569569, Accuracy = 0.830999970436\n",
      "Performance on test set: Batch Loss = 7.551425457, Accuracy = 0.840000033379\n",
      "Training epochs #568000:   Batch Loss = 7.532048, Accuracy = 0.824000000954\n",
      "Performance on test set: Batch Loss = 7.66673946381, Accuracy = 0.826000034809\n",
      "Training epochs #570000:   Batch Loss = 7.865268, Accuracy = 0.715000092983\n",
      "Performance on test set: Batch Loss = 7.99571657181, Accuracy = 0.699999988079\n",
      "Training epochs #572000:   Batch Loss = 8.101121, Accuracy = 0.651000022888\n",
      "Performance on test set: Batch Loss = 8.10912418365, Accuracy = 0.641999959946\n",
      "Training epochs #574000:   Batch Loss = 7.948523, Accuracy = 0.768000006676\n",
      "Performance on test set: Batch Loss = 7.95541143417, Accuracy = 0.766000032425\n",
      "Training epochs #576000:   Batch Loss = 8.020649, Accuracy = 0.739999949932\n",
      "Performance on test set: Batch Loss = 7.83470535278, Accuracy = 0.782999992371\n",
      "Training epochs #578000:   Batch Loss = 7.993840, Accuracy = 0.740999996662\n",
      "Performance on test set: Batch Loss = 7.92588281631, Accuracy = 0.769999980927\n",
      "Training epochs #580000:   Batch Loss = 8.005476, Accuracy = 0.763000011444\n",
      "Performance on test set: Batch Loss = 7.99688053131, Accuracy = 0.740000009537\n",
      "Training epochs #582000:   Batch Loss = 8.043817, Accuracy = 0.746000051498\n",
      "Performance on test set: Batch Loss = 7.98341846466, Accuracy = 0.758000016212\n",
      "Training epochs #584000:   Batch Loss = 7.913580, Accuracy = 0.756999969482\n",
      "Performance on test set: Batch Loss = 7.93177175522, Accuracy = 0.763000011444\n",
      "Training epochs #586000:   Batch Loss = 7.998561, Accuracy = 0.740000009537\n",
      "Performance on test set: Batch Loss = 7.85407543182, Accuracy = 0.769999980927\n",
      "Training epochs #588000:   Batch Loss = 8.004726, Accuracy = 0.740000009537\n",
      "Performance on test set: Batch Loss = 7.85555362701, Accuracy = 0.747999966145\n",
      "Training epochs #590000:   Batch Loss = 7.878342, Accuracy = 0.756000041962\n",
      "Performance on test set: Batch Loss = 7.78675842285, Accuracy = 0.78200006485\n",
      "Training epochs #592000:   Batch Loss = 7.931447, Accuracy = 0.754000067711\n",
      "Performance on test set: Batch Loss = 7.89864253998, Accuracy = 0.750999987125\n",
      "Training epochs #594000:   Batch Loss = 7.861879, Accuracy = 0.761000037193\n",
      "Performance on test set: Batch Loss = 7.88484096527, Accuracy = 0.757000029087\n",
      "Training epochs #596000:   Batch Loss = 7.768573, Accuracy = 0.780999958515\n",
      "Performance on test set: Batch Loss = 7.75075531006, Accuracy = 0.79699999094\n",
      "Training epochs #598000:   Batch Loss = 7.775363, Accuracy = 0.787999987602\n",
      "Performance on test set: Batch Loss = 7.73164558411, Accuracy = 0.796000003815\n",
      "Training epochs #600000:   Batch Loss = 7.681225, Accuracy = 0.805000007153\n",
      "Performance on test set: Batch Loss = 7.77809000015, Accuracy = 0.76700001955\n",
      "Training epochs #602000:   Batch Loss = 7.715259, Accuracy = 0.797999978065\n",
      "Performance on test set: Batch Loss = 7.77286624908, Accuracy = 0.769999980927\n",
      "Training epochs #604000:   Batch Loss = 7.715981, Accuracy = 0.780000030994\n",
      "Performance on test set: Batch Loss = 7.73951721191, Accuracy = 0.767000079155\n",
      "Training epochs #606000:   Batch Loss = 7.683191, Accuracy = 0.784000039101\n",
      "Performance on test set: Batch Loss = 7.68923330307, Accuracy = 0.771000027657\n",
      "Training epochs #608000:   Batch Loss = 7.777654, Accuracy = 0.746999979019\n",
      "Performance on test set: Batch Loss = 7.70232963562, Accuracy = 0.755999982357\n",
      "Training epochs #610000:   Batch Loss = 7.691662, Accuracy = 0.763000011444\n",
      "Performance on test set: Batch Loss = 7.65439462662, Accuracy = 0.78200006485\n",
      "Training epochs #612000:   Batch Loss = 7.732451, Accuracy = 0.752000033855\n",
      "Performance on test set: Batch Loss = 7.7473192215, Accuracy = 0.755000054836\n",
      "Training epochs #614000:   Batch Loss = 7.689917, Accuracy = 0.758000016212\n",
      "Performance on test set: Batch Loss = 7.74228525162, Accuracy = 0.757999956608\n",
      "Training epochs #616000:   Batch Loss = 7.641339, Accuracy = 0.770999968052\n",
      "Performance on test set: Batch Loss = 7.62354660034, Accuracy = 0.796000003815\n",
      "Training epochs #618000:   Batch Loss = 7.643933, Accuracy = 0.788999974728\n",
      "Performance on test set: Batch Loss = 7.61718797684, Accuracy = 0.773999989033\n",
      "Training epochs #620000:   Batch Loss = 7.679216, Accuracy = 0.764999985695\n",
      "Performance on test set: Batch Loss = 7.67622327805, Accuracy = 0.749000012875\n",
      "Training epochs #622000:   Batch Loss = 7.579433, Accuracy = 0.800999999046\n",
      "Performance on test set: Batch Loss = 7.66720914841, Accuracy = 0.767999947071\n",
      "Training epochs #624000:   Batch Loss = 7.690451, Accuracy = 0.754999995232\n",
      "Performance on test set: Batch Loss = 7.64135122299, Accuracy = 0.763000011444\n",
      "Training epochs #626000:   Batch Loss = 7.668532, Accuracy = 0.758000016212\n",
      "Performance on test set: Batch Loss = 7.60627841949, Accuracy = 0.771000027657\n",
      "Training epochs #628000:   Batch Loss = 7.612712, Accuracy = 0.769000053406\n",
      "Performance on test set: Batch Loss = 7.61194801331, Accuracy = 0.752000033855\n",
      "Training epochs #630000:   Batch Loss = 7.696171, Accuracy = 0.743000030518\n",
      "Performance on test set: Batch Loss = 7.5651049614, Accuracy = 0.778999984264\n",
      "Training epochs #632000:   Batch Loss = 7.619696, Accuracy = 0.780000030994\n",
      "Performance on test set: Batch Loss = 7.66221141815, Accuracy = 0.757000029087\n",
      "Training epochs #634000:   Batch Loss = 7.545514, Accuracy = 0.786000013351\n",
      "Performance on test set: Batch Loss = 7.63699865341, Accuracy = 0.754999935627\n",
      "Training epochs #636000:   Batch Loss = 7.610959, Accuracy = 0.774999976158\n",
      "Performance on test set: Batch Loss = 7.53892278671, Accuracy = 0.805000066757\n",
      "Training epochs #638000:   Batch Loss = 7.541721, Accuracy = 0.791000008583\n",
      "Performance on test set: Batch Loss = 7.50143623352, Accuracy = 0.797999978065\n",
      "Training epochs #640000:   Batch Loss = 7.592480, Accuracy = 0.775999963284\n",
      "Performance on test set: Batch Loss = 7.56589984894, Accuracy = 0.76600009203\n",
      "Training epochs #642000:   Batch Loss = 7.528544, Accuracy = 0.786999940872\n",
      "Performance on test set: Batch Loss = 7.55935001373, Accuracy = 0.778000056744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #644000:   Batch Loss = 7.456668, Accuracy = 0.800999999046\n",
      "Performance on test set: Batch Loss = 7.53966903687, Accuracy = 0.774000048637\n",
      "Training epochs #646000:   Batch Loss = 7.548226, Accuracy = 0.759999990463\n",
      "Performance on test set: Batch Loss = 7.49427938461, Accuracy = 0.787999987602\n",
      "Training epochs #648000:   Batch Loss = 7.536959, Accuracy = 0.778999984264\n",
      "Performance on test set: Batch Loss = 7.49171066284, Accuracy = 0.79699999094\n",
      "Training epochs #650000:   Batch Loss = 7.529676, Accuracy = 0.79699999094\n",
      "Performance on test set: Batch Loss = 7.45665931702, Accuracy = 0.809000015259\n",
      "Training epochs #652000:   Batch Loss = 7.502697, Accuracy = 0.785000026226\n",
      "Performance on test set: Batch Loss = 7.54945707321, Accuracy = 0.776000022888\n",
      "Training epochs #654000:   Batch Loss = 7.476267, Accuracy = 0.795000076294\n",
      "Performance on test set: Batch Loss = 7.51101922989, Accuracy = 0.784999966621\n",
      "Training epochs #656000:   Batch Loss = 7.531690, Accuracy = 0.783000051975\n",
      "Performance on test set: Batch Loss = 7.42581415176, Accuracy = 0.814999997616\n",
      "Training epochs #658000:   Batch Loss = 7.486145, Accuracy = 0.793999969959\n",
      "Performance on test set: Batch Loss = 7.39142942429, Accuracy = 0.831000030041\n",
      "Training epochs #660000:   Batch Loss = 7.469543, Accuracy = 0.805999994278\n",
      "Performance on test set: Batch Loss = 7.43347120285, Accuracy = 0.794000029564\n",
      "Training epochs #662000:   Batch Loss = 7.461145, Accuracy = 0.783000051975\n",
      "Performance on test set: Batch Loss = 7.43617677689, Accuracy = 0.800000071526\n",
      "Training epochs #664000:   Batch Loss = 7.374816, Accuracy = 0.814999938011\n",
      "Performance on test set: Batch Loss = 7.42203521729, Accuracy = 0.806999981403\n",
      "Training epochs #666000:   Batch Loss = 7.474174, Accuracy = 0.792999982834\n",
      "Performance on test set: Batch Loss = 7.37623357773, Accuracy = 0.822000026703\n",
      "Training epochs #668000:   Batch Loss = 7.465086, Accuracy = 0.813999950886\n",
      "Performance on test set: Batch Loss = 7.38009881973, Accuracy = 0.838000059128\n",
      "Training epochs #670000:   Batch Loss = 7.382908, Accuracy = 0.828999996185\n",
      "Performance on test set: Batch Loss = 7.34707832336, Accuracy = 0.84600007534\n",
      "Training epochs #672000:   Batch Loss = 7.434818, Accuracy = 0.805999994278\n",
      "Performance on test set: Batch Loss = 7.42088603973, Accuracy = 0.788000047207\n",
      "Training epochs #674000:   Batch Loss = 7.369389, Accuracy = 0.81400001049\n",
      "Performance on test set: Batch Loss = 7.38389253616, Accuracy = 0.808000087738\n",
      "Training epochs #676000:   Batch Loss = 7.333348, Accuracy = 0.82800000906\n",
      "Performance on test set: Batch Loss = 7.33053922653, Accuracy = 0.837000012398\n",
      "Training epochs #678000:   Batch Loss = 7.333760, Accuracy = 0.837999999523\n",
      "Performance on test set: Batch Loss = 7.29569244385, Accuracy = 0.858999967575\n",
      "Training epochs #680000:   Batch Loss = 7.277358, Accuracy = 0.841000020504\n",
      "Performance on test set: Batch Loss = 7.32137680054, Accuracy = 0.833999991417\n",
      "Training epochs #682000:   Batch Loss = 7.311509, Accuracy = 0.841999948025\n",
      "Performance on test set: Batch Loss = 7.32940244675, Accuracy = 0.842999994755\n",
      "Training epochs #684000:   Batch Loss = 7.342459, Accuracy = 0.842000007629\n",
      "Performance on test set: Batch Loss = 7.33125305176, Accuracy = 0.848000049591\n",
      "Training epochs #686000:   Batch Loss = 7.276428, Accuracy = 0.851999998093\n",
      "Performance on test set: Batch Loss = 7.27856111526, Accuracy = 0.844999969006\n",
      "Training epochs #688000:   Batch Loss = 7.319375, Accuracy = 0.840999960899\n",
      "Performance on test set: Batch Loss = 7.29765510559, Accuracy = 0.846000015736\n",
      "Training epochs #690000:   Batch Loss = 7.294279, Accuracy = 0.838000059128\n",
      "Performance on test set: Batch Loss = 7.26176452637, Accuracy = 0.877000033855\n",
      "Training epochs #692000:   Batch Loss = 7.298953, Accuracy = 0.864000022411\n",
      "Performance on test set: Batch Loss = 7.33649587631, Accuracy = 0.848000049591\n",
      "Training epochs #694000:   Batch Loss = 7.273476, Accuracy = 0.855000078678\n",
      "Performance on test set: Batch Loss = 7.2947936058, Accuracy = 0.855999946594\n",
      "Training epochs #696000:   Batch Loss = 7.265244, Accuracy = 0.872999966145\n",
      "Performance on test set: Batch Loss = 7.25995159149, Accuracy = 0.85900002718\n",
      "Training epochs #698000:   Batch Loss = 7.266325, Accuracy = 0.881999969482\n",
      "Performance on test set: Batch Loss = 7.22365236282, Accuracy = 0.893000006676\n",
      "Training epochs #700000:   Batch Loss = 7.254202, Accuracy = 0.864000082016\n",
      "Performance on test set: Batch Loss = 7.24490594864, Accuracy = 0.878000020981\n",
      "Training epochs #702000:   Batch Loss = 7.194130, Accuracy = 0.892000079155\n",
      "Performance on test set: Batch Loss = 7.25398302078, Accuracy = 0.887000024319\n",
      "Training epochs #704000:   Batch Loss = 7.267786, Accuracy = 0.870999991894\n",
      "Performance on test set: Batch Loss = 7.27472162247, Accuracy = 0.871999979019\n",
      "Training epochs #706000:   Batch Loss = 7.263226, Accuracy = 0.859000086784\n",
      "Performance on test set: Batch Loss = 7.22077608109, Accuracy = 0.875\n",
      "Training epochs #708000:   Batch Loss = 7.220809, Accuracy = 0.868999958038\n",
      "Performance on test set: Batch Loss = 7.23555374146, Accuracy = 0.862000048161\n",
      "Training epochs #710000:   Batch Loss = 7.297154, Accuracy = 0.866999983788\n",
      "Performance on test set: Batch Loss = 7.20486068726, Accuracy = 0.898000001907\n",
      "Training epochs #712000:   Batch Loss = 7.222314, Accuracy = 0.894999980927\n",
      "Performance on test set: Batch Loss = 7.28287696838, Accuracy = 0.865000009537\n",
      "Training epochs #714000:   Batch Loss = 7.187728, Accuracy = 0.89300006628\n",
      "Performance on test set: Batch Loss = 7.24193096161, Accuracy = 0.882000029087\n",
      "Training epochs #716000:   Batch Loss = 7.254850, Accuracy = 0.871999979019\n",
      "Performance on test set: Batch Loss = 7.2148065567, Accuracy = 0.880999982357\n",
      "Training epochs #718000:   Batch Loss = 7.184287, Accuracy = 0.89099997282\n",
      "Performance on test set: Batch Loss = 7.18440437317, Accuracy = 0.895000040531\n",
      "Training epochs #720000:   Batch Loss = 7.242855, Accuracy = 0.879000067711\n",
      "Performance on test set: Batch Loss = 7.20356798172, Accuracy = 0.891999959946\n",
      "Training epochs #722000:   Batch Loss = 7.176003, Accuracy = 0.896000087261\n",
      "Performance on test set: Batch Loss = 7.21199369431, Accuracy = 0.874000012875\n",
      "Training epochs #724000:   Batch Loss = 7.173795, Accuracy = 0.87399995327\n",
      "Performance on test set: Batch Loss = 7.23381567001, Accuracy = 0.869000017643\n",
      "Training epochs #726000:   Batch Loss = 7.197157, Accuracy = 0.864000022411\n",
      "Performance on test set: Batch Loss = 7.18029260635, Accuracy = 0.872000038624\n",
      "Training epochs #728000:   Batch Loss = 7.213342, Accuracy = 0.860000014305\n",
      "Performance on test set: Batch Loss = 7.20077371597, Accuracy = 0.863000035286\n",
      "Training epochs #730000:   Batch Loss = 7.250625, Accuracy = 0.848999977112\n",
      "Performance on test set: Batch Loss = 7.1658244133, Accuracy = 0.879000067711\n",
      "Training epochs #732000:   Batch Loss = 7.193575, Accuracy = 0.855999946594\n",
      "Performance on test set: Batch Loss = 7.24452590942, Accuracy = 0.865999937057\n",
      "Training epochs #734000:   Batch Loss = 7.173320, Accuracy = 0.902000010014\n",
      "Performance on test set: Batch Loss = 7.20543575287, Accuracy = 0.884000062943\n",
      "Training epochs #736000:   Batch Loss = 7.225362, Accuracy = 0.87600004673\n",
      "Performance on test set: Batch Loss = 7.18128061295, Accuracy = 0.881999969482\n",
      "Training epochs #738000:   Batch Loss = 7.230188, Accuracy = 0.873000025749\n",
      "Performance on test set: Batch Loss = 7.15393590927, Accuracy = 0.895000040531\n",
      "Training epochs #740000:   Batch Loss = 7.212885, Accuracy = 0.883000075817\n",
      "Performance on test set: Batch Loss = 7.16930580139, Accuracy = 0.891000032425\n",
      "Training epochs #742000:   Batch Loss = 7.218354, Accuracy = 0.873000025749\n",
      "Performance on test set: Batch Loss = 7.17696714401, Accuracy = 0.894000053406\n",
      "Training epochs #744000:   Batch Loss = 7.143821, Accuracy = 0.887000083923\n",
      "Performance on test set: Batch Loss = 7.2022356987, Accuracy = 0.887000083923\n",
      "Training epochs #746000:   Batch Loss = 7.232900, Accuracy = 0.868000030518\n",
      "Performance on test set: Batch Loss = 7.15056800842, Accuracy = 0.873000025749\n",
      "Training epochs #748000:   Batch Loss = 7.211477, Accuracy = 0.861000061035\n",
      "Performance on test set: Batch Loss = 7.17376184464, Accuracy = 0.877000033855\n",
      "Training epochs #750000:   Batch Loss = 7.148902, Accuracy = 0.871000051498\n",
      "Performance on test set: Batch Loss = 7.13590192795, Accuracy = 0.880000054836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #752000:   Batch Loss = 7.221322, Accuracy = 0.852999925613\n",
      "Performance on test set: Batch Loss = 7.21480083466, Accuracy = 0.866000056267\n",
      "Training epochs #754000:   Batch Loss = 7.149994, Accuracy = 0.872000038624\n",
      "Performance on test set: Batch Loss = 7.17594385147, Accuracy = 0.868000090122\n",
      "Training epochs #756000:   Batch Loss = 7.140350, Accuracy = 0.885000050068\n",
      "Performance on test set: Batch Loss = 7.15675544739, Accuracy = 0.871000051498\n",
      "Training epochs #758000:   Batch Loss = 7.146517, Accuracy = 0.885000109673\n",
      "Performance on test set: Batch Loss = 7.12916517258, Accuracy = 0.888000011444\n",
      "Training epochs #760000:   Batch Loss = 7.108995, Accuracy = 0.897000014782\n",
      "Performance on test set: Batch Loss = 7.1414141655, Accuracy = 0.877000033855\n",
      "Training epochs #762000:   Batch Loss = 7.156905, Accuracy = 0.880000114441\n",
      "Performance on test set: Batch Loss = 7.14947414398, Accuracy = 0.887000024319\n",
      "Training epochs #764000:   Batch Loss = 7.186025, Accuracy = 0.871000051498\n",
      "Performance on test set: Batch Loss = 7.17442798615, Accuracy = 0.895000040531\n",
      "Training epochs #766000:   Batch Loss = 7.116367, Accuracy = 0.884000003338\n",
      "Performance on test set: Batch Loss = 7.12155723572, Accuracy = 0.889000058174\n",
      "Training epochs #768000:   Batch Loss = 7.142492, Accuracy = 0.883000016212\n",
      "Performance on test set: Batch Loss = 7.1598534584, Accuracy = 0.879999995232\n",
      "Training epochs #770000:   Batch Loss = 7.145624, Accuracy = 0.883000075817\n",
      "Performance on test set: Batch Loss = 7.10458230972, Accuracy = 0.899999976158\n",
      "Training epochs #772000:   Batch Loss = 7.143120, Accuracy = 0.877000033855\n",
      "Performance on test set: Batch Loss = 7.19150543213, Accuracy = 0.865000009537\n",
      "Training epochs #774000:   Batch Loss = 7.137071, Accuracy = 0.880000054836\n",
      "Performance on test set: Batch Loss = 7.14981031418, Accuracy = 0.884000062943\n",
      "Training epochs #776000:   Batch Loss = 7.129285, Accuracy = 0.889000058174\n",
      "Performance on test set: Batch Loss = 7.13044881821, Accuracy = 0.878000020981\n",
      "Training epochs #778000:   Batch Loss = 7.136919, Accuracy = 0.886000037193\n",
      "Performance on test set: Batch Loss = 7.10650730133, Accuracy = 0.894999980927\n",
      "Training epochs #780000:   Batch Loss = 7.124010, Accuracy = 0.885000050068\n",
      "Performance on test set: Batch Loss = 7.11416769028, Accuracy = 0.889999985695\n",
      "Training epochs #782000:   Batch Loss = 7.066843, Accuracy = 0.899000048637\n",
      "Performance on test set: Batch Loss = 7.12126731873, Accuracy = 0.895000040531\n",
      "Training epochs #784000:   Batch Loss = 7.142769, Accuracy = 0.881000041962\n",
      "Performance on test set: Batch Loss = 7.15130615234, Accuracy = 0.883000016212\n",
      "Training epochs #786000:   Batch Loss = 7.127369, Accuracy = 0.891000032425\n",
      "Performance on test set: Batch Loss = 7.100730896, Accuracy = 0.89100009203\n",
      "Training epochs #788000:   Batch Loss = 7.098678, Accuracy = 0.890000104904\n",
      "Performance on test set: Batch Loss = 7.13199186325, Accuracy = 0.878000020981\n",
      "Training epochs #790000:   Batch Loss = 7.168683, Accuracy = 0.866999983788\n",
      "Performance on test set: Batch Loss = 7.08118629456, Accuracy = 0.901000142097\n",
      "Training epochs #792000:   Batch Loss = 7.101990, Accuracy = 0.894999980927\n",
      "Performance on test set: Batch Loss = 7.16302013397, Accuracy = 0.865000009537\n",
      "Training epochs #794000:   Batch Loss = 7.077858, Accuracy = 0.896000087261\n",
      "Performance on test set: Batch Loss = 7.1230006218, Accuracy = 0.884000003338\n",
      "Training epochs #796000:   Batch Loss = 7.138798, Accuracy = 0.871000051498\n",
      "Performance on test set: Batch Loss = 7.10722780228, Accuracy = 0.877999961376\n",
      "Training epochs #798000:   Batch Loss = 7.072864, Accuracy = 0.889999985695\n",
      "Performance on test set: Batch Loss = 7.08371973038, Accuracy = 0.895000040531\n",
      "Training epochs #800000:   Batch Loss = 7.132375, Accuracy = 0.879000067711\n",
      "Performance on test set: Batch Loss = 7.09318971634, Accuracy = 0.888000011444\n",
      "Training epochs #802000:   Batch Loss = 7.075936, Accuracy = 0.895000040531\n",
      "Performance on test set: Batch Loss = 7.09824562073, Accuracy = 0.894999980927\n",
      "Training epochs #804000:   Batch Loss = 7.071626, Accuracy = 0.893000006676\n",
      "Performance on test set: Batch Loss = 7.12810707092, Accuracy = 0.882000029087\n",
      "Training epochs #806000:   Batch Loss = 7.086395, Accuracy = 0.888000071049\n",
      "Performance on test set: Batch Loss = 7.0763335228, Accuracy = 0.89099997282\n",
      "Training epochs #808000:   Batch Loss = 7.107103, Accuracy = 0.87600004673\n",
      "Performance on test set: Batch Loss = 7.11328983307, Accuracy = 0.879999995232\n",
      "Training epochs #810000:   Batch Loss = 7.139513, Accuracy = 0.874000072479\n",
      "Performance on test set: Batch Loss = 7.05663776398, Accuracy = 0.901000082493\n",
      "Training epochs #812000:   Batch Loss = 7.090831, Accuracy = 0.884000062943\n",
      "Performance on test set: Batch Loss = 7.14278316498, Accuracy = 0.866999983788\n",
      "Training epochs #814000:   Batch Loss = 7.071933, Accuracy = 0.900000035763\n",
      "Performance on test set: Batch Loss = 7.09930753708, Accuracy = 0.884000003338\n",
      "Training epochs #816000:   Batch Loss = 7.120599, Accuracy = 0.874000012875\n",
      "Performance on test set: Batch Loss = 7.08547258377, Accuracy = 0.879000067711\n",
      "Training epochs #818000:   Batch Loss = 7.126532, Accuracy = 0.871999979019\n",
      "Performance on test set: Batch Loss = 7.06124544144, Accuracy = 0.895000040531\n",
      "Training epochs #820000:   Batch Loss = 7.112098, Accuracy = 0.885000050068\n",
      "Performance on test set: Batch Loss = 7.06677961349, Accuracy = 0.891000032425\n",
      "Training epochs #822000:   Batch Loss = 7.118013, Accuracy = 0.873000025749\n",
      "Performance on test set: Batch Loss = 7.0751786232, Accuracy = 0.896000027657\n",
      "Training epochs #824000:   Batch Loss = 7.052423, Accuracy = 0.888000130653\n",
      "Performance on test set: Batch Loss = 7.09963655472, Accuracy = 0.883000016212\n",
      "Training epochs #826000:   Batch Loss = 7.131482, Accuracy = 0.868000030518\n",
      "Performance on test set: Batch Loss = 7.05106544495, Accuracy = 0.89099997282\n",
      "Training epochs #828000:   Batch Loss = 7.105160, Accuracy = 0.895000040531\n",
      "Performance on test set: Batch Loss = 7.09076786041, Accuracy = 0.868999958038\n",
      "Training epochs #830000:   Batch Loss = 7.045362, Accuracy = 0.889000117779\n",
      "Performance on test set: Batch Loss = 7.03804397583, Accuracy = 0.89200001955\n",
      "Training epochs #832000:   Batch Loss = 7.120230, Accuracy = 0.865000069141\n",
      "Performance on test set: Batch Loss = 7.12106561661, Accuracy = 0.868000090122\n",
      "Training epochs #834000:   Batch Loss = 7.059846, Accuracy = 0.883000075817\n",
      "Performance on test set: Batch Loss = 7.07785797119, Accuracy = 0.875\n",
      "Training epochs #836000:   Batch Loss = 7.046903, Accuracy = 0.883000075817\n",
      "Performance on test set: Batch Loss = 7.06424474716, Accuracy = 0.869000017643\n",
      "Training epochs #838000:   Batch Loss = 7.062435, Accuracy = 0.89200001955\n",
      "Performance on test set: Batch Loss = 7.04363965988, Accuracy = 0.889000058174\n",
      "Training epochs #840000:   Batch Loss = 7.017806, Accuracy = 0.899000048637\n",
      "Performance on test set: Batch Loss = 7.04528999329, Accuracy = 0.878000020981\n",
      "Training epochs #842000:   Batch Loss = 7.071453, Accuracy = 0.879999995232\n",
      "Performance on test set: Batch Loss = 7.05188083649, Accuracy = 0.888999938965\n",
      "Training epochs #844000:   Batch Loss = 7.085563, Accuracy = 0.878000020981\n",
      "Performance on test set: Batch Loss = 7.07794475555, Accuracy = 0.902000069618\n",
      "Training epochs #846000:   Batch Loss = 7.031748, Accuracy = 0.884999990463\n",
      "Performance on test set: Batch Loss = 7.02755260468, Accuracy = 0.89100009203\n",
      "Training epochs #848000:   Batch Loss = 7.053703, Accuracy = 0.882999956608\n",
      "Performance on test set: Batch Loss = 7.08148574829, Accuracy = 0.880000054836\n",
      "Training epochs #850000:   Batch Loss = 7.053403, Accuracy = 0.884999990463\n",
      "Performance on test set: Batch Loss = 7.02639436722, Accuracy = 0.901000022888\n",
      "Training epochs #852000:   Batch Loss = 7.052922, Accuracy = 0.877000033855\n",
      "Performance on test set: Batch Loss = 7.10251283646, Accuracy = 0.866999983788\n",
      "Training epochs #854000:   Batch Loss = 7.037937, Accuracy = 0.881000041962\n",
      "Performance on test set: Batch Loss = 7.05821895599, Accuracy = 0.884000003338\n",
      "Training epochs #856000:   Batch Loss = 7.042531, Accuracy = 0.892000079155\n",
      "Performance on test set: Batch Loss = 7.04210042953, Accuracy = 0.878000080585\n",
      "Training epochs #858000:   Batch Loss = 7.049815, Accuracy = 0.885000050068\n",
      "Performance on test set: Batch Loss = 7.02810573578, Accuracy = 0.895000040531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #860000:   Batch Loss = 7.044248, Accuracy = 0.885000050068\n",
      "Performance on test set: Batch Loss = 7.03876066208, Accuracy = 0.89200001955\n",
      "Training epochs #862000:   Batch Loss = 6.979062, Accuracy = 0.900000095367\n",
      "Performance on test set: Batch Loss = 7.03776788712, Accuracy = 0.893000006676\n",
      "Training epochs #864000:   Batch Loss = 7.084791, Accuracy = 0.880000114441\n",
      "Performance on test set: Batch Loss = 7.07683038712, Accuracy = 0.883000016212\n",
      "Training epochs #866000:   Batch Loss = 7.052603, Accuracy = 0.89100009203\n",
      "Performance on test set: Batch Loss = 7.03774452209, Accuracy = 0.891000032425\n",
      "Training epochs #868000:   Batch Loss = 7.034543, Accuracy = 0.890000104904\n",
      "Performance on test set: Batch Loss = 7.06360864639, Accuracy = 0.87700009346\n",
      "Training epochs #870000:   Batch Loss = 7.102031, Accuracy = 0.865999996662\n",
      "Performance on test set: Batch Loss = 7.01984453201, Accuracy = 0.897000074387\n",
      "Training epochs #872000:   Batch Loss = 7.055728, Accuracy = 0.89300006628\n",
      "Performance on test set: Batch Loss = 7.10807323456, Accuracy = 0.864000082016\n",
      "Training epochs #874000:   Batch Loss = 7.019351, Accuracy = 0.896000027657\n",
      "Performance on test set: Batch Loss = 7.06137990952, Accuracy = 0.883000016212\n",
      "Training epochs #876000:   Batch Loss = 7.076139, Accuracy = 0.872000038624\n",
      "Performance on test set: Batch Loss = 7.04188966751, Accuracy = 0.878999948502\n",
      "Training epochs #878000:   Batch Loss = 6.994751, Accuracy = 0.8900000453\n",
      "Performance on test set: Batch Loss = 7.02908706665, Accuracy = 0.894999980927\n",
      "Training epochs #880000:   Batch Loss = 7.051947, Accuracy = 0.883000016212\n",
      "Performance on test set: Batch Loss = 7.05016183853, Accuracy = 0.889000058174\n",
      "Training epochs #882000:   Batch Loss = 7.025826, Accuracy = 0.889999985695\n",
      "Performance on test set: Batch Loss = 7.04980516434, Accuracy = 0.895000100136\n",
      "Training epochs #884000:   Batch Loss = 7.027518, Accuracy = 0.893000006676\n",
      "Performance on test set: Batch Loss = 7.10758256912, Accuracy = 0.883000016212\n",
      "Training epochs #886000:   Batch Loss = 7.047651, Accuracy = 0.888999998569\n",
      "Performance on test set: Batch Loss = 7.03275585175, Accuracy = 0.892000079155\n",
      "Training epochs #888000:   Batch Loss = 7.066933, Accuracy = 0.87600004673\n",
      "Performance on test set: Batch Loss = 7.04890537262, Accuracy = 0.879000008106\n",
      "Training epochs #890000:   Batch Loss = 7.088538, Accuracy = 0.874000012875\n",
      "Performance on test set: Batch Loss = 6.99556493759, Accuracy = 0.902999997139\n",
      "Training epochs #892000:   Batch Loss = 7.021916, Accuracy = 0.884000062943\n",
      "Performance on test set: Batch Loss = 7.06913566589, Accuracy = 0.869000077248\n",
      "Training epochs #894000:   Batch Loss = 7.115630, Accuracy = 0.900999963284\n",
      "Performance on test set: Batch Loss = 7.70773601532, Accuracy = 0.749000012875\n",
      "Training epochs #896000:   Batch Loss = 7.840939, Accuracy = 0.779000043869\n",
      "Performance on test set: Batch Loss = 7.9438829422, Accuracy = 0.791000008583\n",
      "Training epochs #898000:   Batch Loss = 8.321205, Accuracy = 0.739000022411\n",
      "Performance on test set: Batch Loss = 8.26813793182, Accuracy = 0.76800006628\n",
      "Training epochs #900000:   Batch Loss = 8.253901, Accuracy = 0.762000024319\n",
      "Performance on test set: Batch Loss = 8.350481987, Accuracy = 0.743000030518\n",
      "Training epochs #902000:   Batch Loss = 8.168144, Accuracy = 0.747000038624\n",
      "Performance on test set: Batch Loss = 7.91590166092, Accuracy = 0.760999917984\n",
      "Training epochs #904000:   Batch Loss = 7.769699, Accuracy = 0.754999995232\n",
      "Performance on test set: Batch Loss = 7.70500659943, Accuracy = 0.759999990463\n",
      "Training epochs #906000:   Batch Loss = 7.855888, Accuracy = 0.745000004768\n",
      "Performance on test set: Batch Loss = 7.83361101151, Accuracy = 0.769999980927\n",
      "Training epochs #908000:   Batch Loss = 7.877114, Accuracy = 0.740999996662\n",
      "Performance on test set: Batch Loss = 7.85643434525, Accuracy = 0.759999990463\n",
      "Training epochs #910000:   Batch Loss = 7.786516, Accuracy = 0.759000062943\n",
      "Performance on test set: Batch Loss = 7.61324977875, Accuracy = 0.778999984264\n",
      "Training epochs #912000:   Batch Loss = 7.602284, Accuracy = 0.754999995232\n",
      "Performance on test set: Batch Loss = 7.61526203156, Accuracy = 0.743999958038\n",
      "Training epochs #914000:   Batch Loss = 7.565088, Accuracy = 0.762999951839\n",
      "Performance on test set: Batch Loss = 7.64614915848, Accuracy = 0.753000020981\n",
      "Training epochs #916000:   Batch Loss = 7.512845, Accuracy = 0.794999957085\n",
      "Performance on test set: Batch Loss = 7.46189928055, Accuracy = 0.804000020027\n",
      "Training epochs #918000:   Batch Loss = 7.519745, Accuracy = 0.791999995708\n",
      "Performance on test set: Batch Loss = 7.47298908234, Accuracy = 0.796000063419\n",
      "Training epochs #920000:   Batch Loss = 7.410711, Accuracy = 0.805000066757\n",
      "Performance on test set: Batch Loss = 7.50138044357, Accuracy = 0.772000014782\n",
      "Training epochs #922000:   Batch Loss = 7.422661, Accuracy = 0.800999999046\n",
      "Performance on test set: Batch Loss = 7.46418476105, Accuracy = 0.789000034332\n",
      "Training epochs #924000:   Batch Loss = 7.441288, Accuracy = 0.797999978065\n",
      "Performance on test set: Batch Loss = 7.46694898605, Accuracy = 0.787999987602\n",
      "Training epochs #926000:   Batch Loss = 7.386442, Accuracy = 0.803000032902\n",
      "Performance on test set: Batch Loss = 7.42663764954, Accuracy = 0.799000024796\n",
      "Training epochs #928000:   Batch Loss = 7.470405, Accuracy = 0.792000055313\n",
      "Performance on test set: Batch Loss = 7.41166496277, Accuracy = 0.800999939442\n",
      "Training epochs #930000:   Batch Loss = 7.442082, Accuracy = 0.791000008583\n",
      "Performance on test set: Batch Loss = 7.3777217865, Accuracy = 0.809000015259\n",
      "Training epochs #932000:   Batch Loss = 7.433387, Accuracy = 0.776000022888\n",
      "Performance on test set: Batch Loss = 7.44976902008, Accuracy = 0.777999997139\n",
      "Training epochs #934000:   Batch Loss = 7.369280, Accuracy = 0.790000081062\n",
      "Performance on test set: Batch Loss = 7.43786525726, Accuracy = 0.771000027657\n",
      "Training epochs #936000:   Batch Loss = 7.368458, Accuracy = 0.802000105381\n",
      "Performance on test set: Batch Loss = 7.31178665161, Accuracy = 0.81200003624\n",
      "Training epochs #938000:   Batch Loss = 7.318602, Accuracy = 0.817999958992\n",
      "Performance on test set: Batch Loss = 7.31047916412, Accuracy = 0.82699996233\n",
      "Training epochs #940000:   Batch Loss = 7.361656, Accuracy = 0.806000053883\n",
      "Performance on test set: Batch Loss = 7.37808561325, Accuracy = 0.789999961853\n",
      "Training epochs #942000:   Batch Loss = 7.268813, Accuracy = 0.831000030041\n",
      "Performance on test set: Batch Loss = 7.37587118149, Accuracy = 0.795000016689\n",
      "Training epochs #944000:   Batch Loss = 7.366521, Accuracy = 0.782999992371\n",
      "Performance on test set: Batch Loss = 7.35937690735, Accuracy = 0.788999974728\n",
      "Training epochs #946000:   Batch Loss = 7.377721, Accuracy = 0.78100001812\n",
      "Performance on test set: Batch Loss = 7.31619262695, Accuracy = 0.799000024796\n",
      "Training epochs #948000:   Batch Loss = 7.309997, Accuracy = 0.805999994278\n",
      "Performance on test set: Batch Loss = 7.31186771393, Accuracy = 0.803000092506\n",
      "Training epochs #950000:   Batch Loss = 7.390563, Accuracy = 0.772000014782\n",
      "Performance on test set: Batch Loss = 7.28672885895, Accuracy = 0.809000015259\n",
      "Training epochs #952000:   Batch Loss = 7.304630, Accuracy = 0.802999973297\n",
      "Performance on test set: Batch Loss = 7.37026309967, Accuracy = 0.78100001812\n",
      "Training epochs #954000:   Batch Loss = 7.262256, Accuracy = 0.807999968529\n",
      "Performance on test set: Batch Loss = 7.33527946472, Accuracy = 0.773000001907\n",
      "Training epochs #956000:   Batch Loss = 7.327398, Accuracy = 0.785000026226\n",
      "Performance on test set: Batch Loss = 7.24447536469, Accuracy = 0.812000095844\n",
      "Training epochs #958000:   Batch Loss = 7.243216, Accuracy = 0.809000015259\n",
      "Performance on test set: Batch Loss = 7.22182035446, Accuracy = 0.816000044346\n",
      "Training epochs #960000:   Batch Loss = 7.307185, Accuracy = 0.794000029564\n",
      "Performance on test set: Batch Loss = 7.28182792664, Accuracy = 0.779000043869\n",
      "Training epochs #962000:   Batch Loss = 7.236459, Accuracy = 0.795000016689\n",
      "Performance on test set: Batch Loss = 7.28486537933, Accuracy = 0.796000003815\n",
      "Training epochs #964000:   Batch Loss = 7.185445, Accuracy = 0.815999984741\n",
      "Performance on test set: Batch Loss = 7.27256345749, Accuracy = 0.791999995708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #966000:   Batch Loss = 7.270699, Accuracy = 0.773999989033\n",
      "Performance on test set: Batch Loss = 7.22060346603, Accuracy = 0.7990000844\n",
      "Training epochs #968000:   Batch Loss = 7.258980, Accuracy = 0.782999992371\n",
      "Performance on test set: Batch Loss = 7.2182393074, Accuracy = 0.802999973297\n",
      "Training epochs #970000:   Batch Loss = 7.267017, Accuracy = 0.79699999094\n",
      "Performance on test set: Batch Loss = 7.19352817535, Accuracy = 0.807000041008\n",
      "Training epochs #972000:   Batch Loss = 7.214615, Accuracy = 0.790000081062\n",
      "Performance on test set: Batch Loss = 7.25614690781, Accuracy = 0.778999984264\n",
      "Training epochs #974000:   Batch Loss = 7.193152, Accuracy = 0.795000016689\n",
      "Performance on test set: Batch Loss = 7.2091422081, Accuracy = 0.779000043869\n",
      "Training epochs #976000:   Batch Loss = 7.232849, Accuracy = 0.780999958515\n",
      "Performance on test set: Batch Loss = 7.14479780197, Accuracy = 0.817000031471\n",
      "Training epochs #978000:   Batch Loss = 7.197292, Accuracy = 0.786000013351\n",
      "Performance on test set: Batch Loss = 7.10655164719, Accuracy = 0.830000042915\n",
      "Training epochs #980000:   Batch Loss = 7.177424, Accuracy = 0.81099998951\n",
      "Performance on test set: Batch Loss = 7.13596391678, Accuracy = 0.797999978065\n",
      "Training epochs #982000:   Batch Loss = 7.180360, Accuracy = 0.784999966621\n",
      "Performance on test set: Batch Loss = 7.13680362701, Accuracy = 0.810000002384\n",
      "Training epochs #984000:   Batch Loss = 7.083493, Accuracy = 0.811999976635\n",
      "Performance on test set: Batch Loss = 7.14006280899, Accuracy = 0.814999938011\n",
      "Training epochs #986000:   Batch Loss = 7.168055, Accuracy = 0.799000024796\n",
      "Performance on test set: Batch Loss = 7.07832622528, Accuracy = 0.829999983311\n",
      "Training epochs #988000:   Batch Loss = 7.130092, Accuracy = 0.825999975204\n",
      "Performance on test set: Batch Loss = 7.08746957779, Accuracy = 0.843000054359\n",
      "Training epochs #990000:   Batch Loss = 7.066598, Accuracy = 0.840000033379\n",
      "Performance on test set: Batch Loss = 7.05673313141, Accuracy = 0.878000020981\n",
      "Training epochs #992000:   Batch Loss = 7.118868, Accuracy = 0.855000019073\n",
      "Performance on test set: Batch Loss = 7.12185955048, Accuracy = 0.850000023842\n",
      "Training epochs #994000:   Batch Loss = 7.069771, Accuracy = 0.870000004768\n",
      "Performance on test set: Batch Loss = 7.06971025467, Accuracy = 0.858000040054\n",
      "Training epochs #996000:   Batch Loss = 7.039093, Accuracy = 0.873000025749\n",
      "Performance on test set: Batch Loss = 7.04333353043, Accuracy = 0.873000025749\n",
      "Training epochs #998000:   Batch Loss = 7.040192, Accuracy = 0.87600004673\n",
      "Performance on test set: Batch Loss = 7.00230646133, Accuracy = 0.896000087261\n",
      "Training epochs #1000000:   Batch Loss = 6.986878, Accuracy = 0.887000024319\n",
      "Performance on test set: Batch Loss = 7.0256934166, Accuracy = 0.876000106335\n",
      "Training epochs #1002000:   Batch Loss = 7.036216, Accuracy = 0.877000033855\n",
      "Performance on test set: Batch Loss = 7.03679609299, Accuracy = 0.883000075817\n",
      "Training epochs #1004000:   Batch Loss = 7.068731, Accuracy = 0.865000069141\n",
      "Performance on test set: Batch Loss = 7.05842590332, Accuracy = 0.872000098228\n",
      "Training epochs #1006000:   Batch Loss = 6.998021, Accuracy = 0.878999948502\n",
      "Performance on test set: Batch Loss = 7.00344610214, Accuracy = 0.874999940395\n",
      "Training epochs #1008000:   Batch Loss = 7.028952, Accuracy = 0.875\n",
      "Performance on test set: Batch Loss = 7.02609968185, Accuracy = 0.881000101566\n",
      "Training epochs #1010000:   Batch Loss = 7.028100, Accuracy = 0.876000106335\n",
      "Performance on test set: Batch Loss = 6.98987627029, Accuracy = 0.882000088692\n",
      "Training epochs #1012000:   Batch Loss = 7.025227, Accuracy = 0.863000035286\n",
      "Performance on test set: Batch Loss = 7.07060337067, Accuracy = 0.851000070572\n",
      "Training epochs #1014000:   Batch Loss = 7.011169, Accuracy = 0.854000031948\n",
      "Performance on test set: Batch Loss = 7.01552629471, Accuracy = 0.85900002718\n",
      "Training epochs #1016000:   Batch Loss = 6.997577, Accuracy = 0.878000020981\n",
      "Performance on test set: Batch Loss = 6.99678468704, Accuracy = 0.870000064373\n",
      "Training epochs #1018000:   Batch Loss = 6.991978, Accuracy = 0.887000083923\n",
      "Performance on test set: Batch Loss = 6.96216058731, Accuracy = 0.89400011301\n",
      "Training epochs #1020000:   Batch Loss = 6.995659, Accuracy = 0.865000009537\n",
      "Performance on test set: Batch Loss = 6.98483896255, Accuracy = 0.875000059605\n",
      "Training epochs #1022000:   Batch Loss = 6.932357, Accuracy = 0.892000079155\n",
      "Performance on test set: Batch Loss = 7.00187158585, Accuracy = 0.882999956608\n",
      "Training epochs #1024000:   Batch Loss = 6.999319, Accuracy = 0.871000051498\n",
      "Performance on test set: Batch Loss = 7.02501487732, Accuracy = 0.872999966145\n",
      "Training epochs #1026000:   Batch Loss = 7.007689, Accuracy = 0.861000061035\n",
      "Performance on test set: Batch Loss = 6.97634029388, Accuracy = 0.875999987125\n",
      "Training epochs #1028000:   Batch Loss = 6.967491, Accuracy = 0.876999974251\n",
      "Performance on test set: Batch Loss = 6.99420499802, Accuracy = 0.881000041962\n",
      "Training epochs #1030000:   Batch Loss = 7.045271, Accuracy = 0.856000065804\n",
      "Performance on test set: Batch Loss = 6.95406150818, Accuracy = 0.884999990463\n",
      "Training epochs #1032000:   Batch Loss = 6.958762, Accuracy = 0.89099997282\n",
      "Performance on test set: Batch Loss = 7.03663825989, Accuracy = 0.854999959469\n",
      "Training epochs #1034000:   Batch Loss = 6.943433, Accuracy = 0.900000035763\n",
      "Performance on test set: Batch Loss = 6.98323059082, Accuracy = 0.885000050068\n",
      "Training epochs #1036000:   Batch Loss = 7.007384, Accuracy = 0.870999991894\n",
      "Performance on test set: Batch Loss = 6.96970415115, Accuracy = 0.879000067711\n",
      "Training epochs #1038000:   Batch Loss = 6.938348, Accuracy = 0.888000011444\n",
      "Performance on test set: Batch Loss = 6.94038677216, Accuracy = 0.898999989033\n",
      "Training epochs #1040000:   Batch Loss = 6.987321, Accuracy = 0.881000101566\n",
      "Performance on test set: Batch Loss = 6.9544377327, Accuracy = 0.893000006676\n",
      "Training epochs #1042000:   Batch Loss = 6.937111, Accuracy = 0.862999975681\n",
      "Performance on test set: Batch Loss = 6.96556425095, Accuracy = 0.884000003338\n",
      "Training epochs #1044000:   Batch Loss = 6.925832, Accuracy = 0.887000083923\n",
      "Performance on test set: Batch Loss = 6.99110174179, Accuracy = 0.875000119209\n",
      "Training epochs #1046000:   Batch Loss = 6.947286, Accuracy = 0.875000059605\n",
      "Performance on test set: Batch Loss = 6.94097900391, Accuracy = 0.880000054836\n",
      "Training epochs #1048000:   Batch Loss = 6.972753, Accuracy = 0.872000098228\n",
      "Performance on test set: Batch Loss = 6.96828889847, Accuracy = 0.880000054836\n",
      "Training epochs #1050000:   Batch Loss = 6.996446, Accuracy = 0.861000061035\n",
      "Performance on test set: Batch Loss = 6.92233085632, Accuracy = 0.885000050068\n",
      "Training epochs #1052000:   Batch Loss = 6.954916, Accuracy = 0.869000077248\n",
      "Performance on test set: Batch Loss = 7.00561380386, Accuracy = 0.866999983788\n",
      "Training epochs #1054000:   Batch Loss = 6.929858, Accuracy = 0.902000069618\n",
      "Performance on test set: Batch Loss = 6.95033407211, Accuracy = 0.883000016212\n",
      "Training epochs #1056000:   Batch Loss = 6.986704, Accuracy = 0.875\n",
      "Performance on test set: Batch Loss = 6.93738269806, Accuracy = 0.879000008106\n",
      "Training epochs #1058000:   Batch Loss = 6.980906, Accuracy = 0.871000051498\n",
      "Performance on test set: Batch Loss = 6.90918636322, Accuracy = 0.896000027657\n",
      "Training epochs #1060000:   Batch Loss = 6.957700, Accuracy = 0.885000050068\n",
      "Performance on test set: Batch Loss = 6.92339944839, Accuracy = 0.893000006676\n",
      "Training epochs #1062000:   Batch Loss = 6.978845, Accuracy = 0.87600004673\n",
      "Performance on test set: Batch Loss = 6.93248176575, Accuracy = 0.898000001907\n",
      "Training epochs #1064000:   Batch Loss = 6.896992, Accuracy = 0.891999959946\n",
      "Performance on test set: Batch Loss = 6.96204566956, Accuracy = 0.875\n",
      "Training epochs #1066000:   Batch Loss = 6.988022, Accuracy = 0.856000065804\n",
      "Performance on test set: Batch Loss = 6.9146900177, Accuracy = 0.879000008106\n",
      "Training epochs #1068000:   Batch Loss = 6.957565, Accuracy = 0.874000012875\n",
      "Performance on test set: Batch Loss = 6.94185829163, Accuracy = 0.880000054836\n",
      "Training epochs #1070000:   Batch Loss = 6.901248, Accuracy = 0.872000038624\n",
      "Performance on test set: Batch Loss = 6.89502716064, Accuracy = 0.89200001955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #1072000:   Batch Loss = 6.979323, Accuracy = 0.868000030518\n",
      "Performance on test set: Batch Loss = 6.98050546646, Accuracy = 0.869000077248\n",
      "Training epochs #1074000:   Batch Loss = 6.920989, Accuracy = 0.874000012875\n",
      "Performance on test set: Batch Loss = 6.92492294312, Accuracy = 0.866999983788\n",
      "Training epochs #1076000:   Batch Loss = 6.898855, Accuracy = 0.883000075817\n",
      "Performance on test set: Batch Loss = 6.91277742386, Accuracy = 0.868000030518\n",
      "Training epochs #1078000:   Batch Loss = 6.914050, Accuracy = 0.884000003338\n",
      "Performance on test set: Batch Loss = 6.88736438751, Accuracy = 0.892000079155\n",
      "Training epochs #1080000:   Batch Loss = 6.865600, Accuracy = 0.897999942303\n",
      "Performance on test set: Batch Loss = 6.89958620071, Accuracy = 0.878000020981\n",
      "Training epochs #1082000:   Batch Loss = 6.923104, Accuracy = 0.878000020981\n",
      "Performance on test set: Batch Loss = 6.90493822098, Accuracy = 0.888000011444\n",
      "Training epochs #1084000:   Batch Loss = 6.940403, Accuracy = 0.875000059605\n",
      "Performance on test set: Batch Loss = 6.93619632721, Accuracy = 0.883000016212\n",
      "Training epochs #1086000:   Batch Loss = 6.872127, Accuracy = 0.889000058174\n",
      "Performance on test set: Batch Loss = 6.88406658173, Accuracy = 0.890000104904\n",
      "Training epochs #1088000:   Batch Loss = 6.901170, Accuracy = 0.87700009346\n",
      "Performance on test set: Batch Loss = 6.92320919037, Accuracy = 0.879999995232\n",
      "Training epochs #1090000:   Batch Loss = 6.900769, Accuracy = 0.886000096798\n",
      "Performance on test set: Batch Loss = 6.87500286102, Accuracy = 0.902000069618\n",
      "Training epochs #1092000:   Batch Loss = 6.981564, Accuracy = 0.87600004673\n",
      "Performance on test set: Batch Loss = 9.06409549713, Accuracy = 0.117000006139\n",
      "Training epochs #1094000:   Batch Loss = 8.969350, Accuracy = 0.082999996841\n",
      "Performance on test set: Batch Loss = 8.35442638397, Accuracy = 0.0879999995232\n",
      "Training epochs #1096000:   Batch Loss = 7.655223, Accuracy = 0.710999906063\n",
      "Performance on test set: Batch Loss = 7.43617296219, Accuracy = 0.797999978065\n",
      "Training epochs #1098000:   Batch Loss = 7.657584, Accuracy = 0.771999955177\n",
      "Performance on test set: Batch Loss = 7.84345579147, Accuracy = 0.76599997282\n",
      "Training epochs #1100000:   Batch Loss = 7.979133, Accuracy = 0.755999982357\n",
      "Performance on test set: Batch Loss = 8.0816450119, Accuracy = 0.738000035286\n",
      "Training epochs #1102000:   Batch Loss = 7.719853, Accuracy = 0.788000047207\n",
      "Performance on test set: Batch Loss = 7.71143865585, Accuracy = 0.756999969482\n",
      "Training epochs #1104000:   Batch Loss = 7.617800, Accuracy = 0.770000040531\n",
      "Performance on test set: Batch Loss = 7.59956741333, Accuracy = 0.837999999523\n",
      "Training epochs #1106000:   Batch Loss = 7.838279, Accuracy = 0.836000025272\n",
      "Performance on test set: Batch Loss = 7.46488809586, Accuracy = 0.835000038147\n",
      "Training epochs #1108000:   Batch Loss = 7.383512, Accuracy = 0.838000059128\n",
      "Performance on test set: Batch Loss = 7.48126745224, Accuracy = 0.789999961853\n",
      "Training epochs #1110000:   Batch Loss = 7.485671, Accuracy = 0.756000041962\n",
      "Performance on test set: Batch Loss = 7.2811589241, Accuracy = 0.794000029564\n",
      "Training epochs #1112000:   Batch Loss = 7.327189, Accuracy = 0.785999953747\n",
      "Performance on test set: Batch Loss = 7.33143043518, Accuracy = 0.778000056744\n",
      "Training epochs #1114000:   Batch Loss = 7.146327, Accuracy = 0.836000025272\n",
      "Performance on test set: Batch Loss = 7.1837978363, Accuracy = 0.819999992847\n",
      "Training epochs #1116000:   Batch Loss = 7.223732, Accuracy = 0.821000039577\n",
      "Performance on test set: Batch Loss = 7.17066860199, Accuracy = 0.826000034809\n",
      "Training epochs #1118000:   Batch Loss = 7.238187, Accuracy = 0.811000049114\n",
      "Performance on test set: Batch Loss = 7.21181917191, Accuracy = 0.830999970436\n",
      "Training epochs #1120000:   Batch Loss = 7.208553, Accuracy = 0.826000034809\n",
      "Performance on test set: Batch Loss = 7.17443752289, Accuracy = 0.828000068665\n",
      "Training epochs #1122000:   Batch Loss = 7.239260, Accuracy = 0.728999972343\n",
      "Performance on test set: Batch Loss = 7.13209533691, Accuracy = 0.844000041485\n",
      "Training epochs #1124000:   Batch Loss = 7.098922, Accuracy = 0.822999954224\n",
      "Performance on test set: Batch Loss = 7.16779279709, Accuracy = 0.819999992847\n",
      "Training epochs #1126000:   Batch Loss = 7.109113, Accuracy = 0.81200003624\n",
      "Performance on test set: Batch Loss = 7.07050037384, Accuracy = 0.842999994755\n",
      "Training epochs #1128000:   Batch Loss = 7.283024, Accuracy = 0.703000068665\n",
      "Performance on test set: Batch Loss = 7.10259628296, Accuracy = 0.826000094414\n",
      "Training epochs #1130000:   Batch Loss = 7.110056, Accuracy = 0.81400001049\n",
      "Performance on test set: Batch Loss = 7.04625892639, Accuracy = 0.832000017166\n",
      "Training epochs #1132000:   Batch Loss = 7.090885, Accuracy = 0.820999979973\n",
      "Performance on test set: Batch Loss = 7.13274097443, Accuracy = 0.808000028133\n",
      "Training epochs #1134000:   Batch Loss = 7.064151, Accuracy = 0.842999994755\n",
      "Performance on test set: Batch Loss = 7.05384254456, Accuracy = 0.821999967098\n",
      "Training epochs #1136000:   Batch Loss = 7.145113, Accuracy = 0.805999994278\n",
      "Performance on test set: Batch Loss = 7.03602409363, Accuracy = 0.832000017166\n",
      "Training epochs #1138000:   Batch Loss = 7.324842, Accuracy = 0.785000026226\n",
      "Performance on test set: Batch Loss = 7.30931520462, Accuracy = 0.79599994421\n",
      "Training epochs #1140000:   Batch Loss = 7.391174, Accuracy = 0.774999976158\n",
      "Performance on test set: Batch Loss = 7.29369735718, Accuracy = 0.768999993801\n",
      "Training epochs #1142000:   Batch Loss = 7.240151, Accuracy = 0.805999934673\n",
      "Performance on test set: Batch Loss = 7.18965911865, Accuracy = 0.817999958992\n",
      "Training epochs #1144000:   Batch Loss = 7.197400, Accuracy = 0.816000044346\n",
      "Performance on test set: Batch Loss = 7.38320159912, Accuracy = 0.81400001049\n",
      "Training epochs #1146000:   Batch Loss = 7.289141, Accuracy = 0.758000016212\n",
      "Performance on test set: Batch Loss = 7.3317399025, Accuracy = 0.797000050545\n",
      "Training epochs #1148000:   Batch Loss = 7.935905, Accuracy = 0.641000032425\n",
      "Performance on test set: Batch Loss = 7.96656227112, Accuracy = 0.638999998569\n",
      "Training epochs #1150000:   Batch Loss = 8.080129, Accuracy = 0.639999985695\n",
      "Performance on test set: Batch Loss = 8.2128238678, Accuracy = 0.669000029564\n",
      "Training epochs #1152000:   Batch Loss = 8.245031, Accuracy = 0.646000027657\n",
      "Performance on test set: Batch Loss = 8.26113986969, Accuracy = 0.627999961376\n",
      "Training epochs #1154000:   Batch Loss = 7.959208, Accuracy = 0.65499997139\n",
      "Performance on test set: Batch Loss = 8.02101516724, Accuracy = 0.646000027657\n",
      "Training epochs #1156000:   Batch Loss = 7.790478, Accuracy = 0.659000039101\n",
      "Performance on test set: Batch Loss = 7.38090133667, Accuracy = 0.799000024796\n",
      "Training epochs #1158000:   Batch Loss = 7.455385, Accuracy = 0.758000016212\n",
      "Performance on test set: Batch Loss = 7.4483423233, Accuracy = 0.768000006676\n",
      "Training epochs #1160000:   Batch Loss = 7.405810, Accuracy = 0.777999997139\n",
      "Performance on test set: Batch Loss = 7.49740600586, Accuracy = 0.743000030518\n",
      "Training epochs #1162000:   Batch Loss = 7.398564, Accuracy = 0.774000048637\n",
      "Performance on test set: Batch Loss = 7.37318372726, Accuracy = 0.759000003338\n",
      "Training epochs #1164000:   Batch Loss = 7.309347, Accuracy = 0.772000074387\n",
      "Performance on test set: Batch Loss = 7.24137783051, Accuracy = 0.764000058174\n",
      "Training epochs #1166000:   Batch Loss = 7.098377, Accuracy = 0.800000071526\n",
      "Performance on test set: Batch Loss = 7.20444774628, Accuracy = 0.831999957561\n",
      "Training epochs #1168000:   Batch Loss = 7.245450, Accuracy = 0.728999972343\n",
      "Performance on test set: Batch Loss = 7.29222869873, Accuracy = 0.712000012398\n",
      "Training epochs #1170000:   Batch Loss = 7.313675, Accuracy = 0.709999978542\n",
      "Performance on test set: Batch Loss = 7.22326040268, Accuracy = 0.738999962807\n",
      "Training epochs #1172000:   Batch Loss = 7.238348, Accuracy = 0.827000021935\n",
      "Performance on test set: Batch Loss = 7.26610565186, Accuracy = 0.788999974728\n",
      "Training epochs #1174000:   Batch Loss = 7.181061, Accuracy = 0.791000068188\n",
      "Performance on test set: Batch Loss = 7.16170310974, Accuracy = 0.790000021458\n",
      "Training epochs #1176000:   Batch Loss = 7.108672, Accuracy = 0.811999976635\n",
      "Performance on test set: Batch Loss = 7.11572551727, Accuracy = 0.805000007153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #1178000:   Batch Loss = 7.092468, Accuracy = 0.800000011921\n",
      "Performance on test set: Batch Loss = 7.0701508522, Accuracy = 0.810000061989\n",
      "Training epochs #1180000:   Batch Loss = 7.091714, Accuracy = 0.790000021458\n",
      "Performance on test set: Batch Loss = 7.1393494606, Accuracy = 0.777000069618\n",
      "Training epochs #1182000:   Batch Loss = 7.035289, Accuracy = 0.816999971867\n",
      "Performance on test set: Batch Loss = 7.09982299805, Accuracy = 0.807000041008\n",
      "Training epochs #1184000:   Batch Loss = 7.124402, Accuracy = 0.797000050545\n",
      "Performance on test set: Batch Loss = 7.10915851593, Accuracy = 0.808999955654\n",
      "Training epochs #1186000:   Batch Loss = 7.075414, Accuracy = 0.802999973297\n",
      "Performance on test set: Batch Loss = 7.07476854324, Accuracy = 0.803000032902\n",
      "Training epochs #1188000:   Batch Loss = 7.042560, Accuracy = 0.80999994278\n",
      "Performance on test set: Batch Loss = 7.10290145874, Accuracy = 0.811999976635\n",
      "Training epochs #1190000:   Batch Loss = 7.106665, Accuracy = 0.819000005722\n",
      "Performance on test set: Batch Loss = 7.02829551697, Accuracy = 0.838999986649\n",
      "Training epochs #1192000:   Batch Loss = 6.998997, Accuracy = 0.840000033379\n",
      "Performance on test set: Batch Loss = 7.10107564926, Accuracy = 0.803000032902\n",
      "Training epochs #1194000:   Batch Loss = 6.981666, Accuracy = 0.845999956131\n",
      "Performance on test set: Batch Loss = 7.01284646988, Accuracy = 0.830000042915\n",
      "Training epochs #1196000:   Batch Loss = 7.041224, Accuracy = 0.819999933243\n",
      "Performance on test set: Batch Loss = 6.99379301071, Accuracy = 0.839999973774\n",
      "Training epochs #1198000:   Batch Loss = 6.964969, Accuracy = 0.835000038147\n",
      "Performance on test set: Batch Loss = 6.94197273254, Accuracy = 0.847999989986\n",
      "Training epochs #1200000:   Batch Loss = 6.989888, Accuracy = 0.824000000954\n",
      "Performance on test set: Batch Loss = 6.97408437729, Accuracy = 0.818000078201\n",
      "Training epochs #1202000:   Batch Loss = 6.915240, Accuracy = 0.85799998045\n",
      "Performance on test set: Batch Loss = 6.94102287292, Accuracy = 0.844999969006\n",
      "Training epochs #1204000:   Batch Loss = 6.926401, Accuracy = 0.838999986649\n",
      "Performance on test set: Batch Loss = 6.97358989716, Accuracy = 0.838999986649\n",
      "Training epochs #1206000:   Batch Loss = 6.938698, Accuracy = 0.831000089645\n",
      "Performance on test set: Batch Loss = 6.92710971832, Accuracy = 0.860999941826\n",
      "Training epochs #1208000:   Batch Loss = 6.962429, Accuracy = 0.839000046253\n",
      "Performance on test set: Batch Loss = 6.95082855225, Accuracy = 0.849999964237\n",
      "Training epochs #1210000:   Batch Loss = 6.962312, Accuracy = 0.833000004292\n",
      "Performance on test set: Batch Loss = 6.90998697281, Accuracy = 0.848000049591\n",
      "Training epochs #1212000:   Batch Loss = 6.911592, Accuracy = 0.855000078678\n",
      "Performance on test set: Batch Loss = 6.97740793228, Accuracy = 0.827000021935\n",
      "Training epochs #1214000:   Batch Loss = 6.908195, Accuracy = 0.849000036716\n",
      "Performance on test set: Batch Loss = 6.91432714462, Accuracy = 0.837000012398\n",
      "Training epochs #1216000:   Batch Loss = 6.943459, Accuracy = 0.842999994755\n",
      "Performance on test set: Batch Loss = 6.90189695358, Accuracy = 0.856000006199\n",
      "Training epochs #1218000:   Batch Loss = 6.949292, Accuracy = 0.832000017166\n",
      "Performance on test set: Batch Loss = 6.87731647491, Accuracy = 0.865999937057\n",
      "Training epochs #1220000:   Batch Loss = 6.909210, Accuracy = 0.854000091553\n",
      "Performance on test set: Batch Loss = 6.90518045425, Accuracy = 0.845000088215\n",
      "Training epochs #1222000:   Batch Loss = 6.945916, Accuracy = 0.837000012398\n",
      "Performance on test set: Batch Loss = 6.89348220825, Accuracy = 0.861999988556\n",
      "Training epochs #1224000:   Batch Loss = 6.877017, Accuracy = 0.870000064373\n",
      "Performance on test set: Batch Loss = 6.93025970459, Accuracy = 0.867000043392\n",
      "Training epochs #1226000:   Batch Loss = 6.953546, Accuracy = 0.853000044823\n",
      "Performance on test set: Batch Loss = 6.89060974121, Accuracy = 0.864000082016\n",
      "Training epochs #1228000:   Batch Loss = 6.902162, Accuracy = 0.868999958038\n",
      "Performance on test set: Batch Loss = 6.91868543625, Accuracy = 0.850000023842\n",
      "Training epochs #1230000:   Batch Loss = 6.856859, Accuracy = 0.854000031948\n",
      "Performance on test set: Batch Loss = 6.87715768814, Accuracy = 0.848999977112\n",
      "Training epochs #1232000:   Batch Loss = 6.908739, Accuracy = 0.830999970436\n",
      "Performance on test set: Batch Loss = 6.94760751724, Accuracy = 0.827000021935\n",
      "Training epochs #1234000:   Batch Loss = 6.871233, Accuracy = 0.861000001431\n",
      "Performance on test set: Batch Loss = 6.88813877106, Accuracy = 0.837000072002\n",
      "Training epochs #1236000:   Batch Loss = 6.874786, Accuracy = 0.848000049591\n",
      "Performance on test set: Batch Loss = 6.87289619446, Accuracy = 0.856000006199\n",
      "Training epochs #1238000:   Batch Loss = 6.889816, Accuracy = 0.842999994755\n",
      "Performance on test set: Batch Loss = 6.85475111008, Accuracy = 0.865000009537\n",
      "Training epochs #1240000:   Batch Loss = 6.842239, Accuracy = 0.856999993324\n",
      "Performance on test set: Batch Loss = 6.87972211838, Accuracy = 0.851999998093\n",
      "Training epochs #1242000:   Batch Loss = 6.893434, Accuracy = 0.864000082016\n",
      "Performance on test set: Batch Loss = 6.86756944656, Accuracy = 0.867000043392\n",
      "Training epochs #1244000:   Batch Loss = 6.914043, Accuracy = 0.840000033379\n",
      "Performance on test set: Batch Loss = 6.90674495697, Accuracy = 0.872000098228\n",
      "Training epochs #1246000:   Batch Loss = 6.853817, Accuracy = 0.860000014305\n",
      "Performance on test set: Batch Loss = 6.86045265198, Accuracy = 0.861000061035\n",
      "Training epochs #1248000:   Batch Loss = 6.863200, Accuracy = 0.853999972343\n",
      "Performance on test set: Batch Loss = 6.89549970627, Accuracy = 0.853000044823\n",
      "Training epochs #1250000:   Batch Loss = 6.892234, Accuracy = 0.868000030518\n",
      "Performance on test set: Batch Loss = 6.84500217438, Accuracy = 0.872000038624\n",
      "Training epochs #1252000:   Batch Loss = 6.877229, Accuracy = 0.855000019073\n",
      "Performance on test set: Batch Loss = 6.91925954819, Accuracy = 0.847000062466\n",
      "Training epochs #1254000:   Batch Loss = 6.873553, Accuracy = 0.861000001431\n",
      "Performance on test set: Batch Loss = 6.8623380661, Accuracy = 0.862000107765\n",
      "Training epochs #1256000:   Batch Loss = 6.840094, Accuracy = 0.865000069141\n",
      "Performance on test set: Batch Loss = 6.84877252579, Accuracy = 0.863000035286\n",
      "Training epochs #1258000:   Batch Loss = 6.857742, Accuracy = 0.86000007391\n",
      "Performance on test set: Batch Loss = 6.86150550842, Accuracy = 0.868000030518\n",
      "Training epochs #1260000:   Batch Loss = 8.148760, Accuracy = 0.237999990582\n",
      "Performance on test set: Batch Loss = 7.7891087532, Accuracy = 0.259000003338\n",
      "Training epochs #1262000:   Batch Loss = 6.834976, Accuracy = 0.882000088692\n",
      "Performance on test set: Batch Loss = 6.97886800766, Accuracy = 0.832000017166\n",
      "Training epochs #1264000:   Batch Loss = 7.127651, Accuracy = 0.818000078201\n",
      "Performance on test set: Batch Loss = 7.18731451035, Accuracy = 0.833000063896\n",
      "Training epochs #1266000:   Batch Loss = 7.167899, Accuracy = 0.819000005722\n",
      "Performance on test set: Batch Loss = 7.24352550507, Accuracy = 0.81099998951\n",
      "Training epochs #1268000:   Batch Loss = 7.219490, Accuracy = 0.817000031471\n",
      "Performance on test set: Batch Loss = 7.32108879089, Accuracy = 0.796000003815\n",
      "Training epochs #1270000:   Batch Loss = 7.292887, Accuracy = 0.790999948978\n",
      "Performance on test set: Batch Loss = 7.11004209518, Accuracy = 0.821000039577\n",
      "Training epochs #1272000:   Batch Loss = 7.096036, Accuracy = 0.827000021935\n",
      "Performance on test set: Batch Loss = 7.1435303688, Accuracy = 0.800999939442\n",
      "Training epochs #1274000:   Batch Loss = 6.973687, Accuracy = 0.839999973774\n",
      "Performance on test set: Batch Loss = 7.01788663864, Accuracy = 0.83000010252\n",
      "Training epochs #1276000:   Batch Loss = 7.042232, Accuracy = 0.81400001049\n",
      "Performance on test set: Batch Loss = 7.01315069199, Accuracy = 0.835999965668\n",
      "Training epochs #1278000:   Batch Loss = 7.004372, Accuracy = 0.834999918938\n",
      "Performance on test set: Batch Loss = 6.98769855499, Accuracy = 0.841000020504\n",
      "Training epochs #1280000:   Batch Loss = 7.044873, Accuracy = 0.817999958992\n",
      "Performance on test set: Batch Loss = 7.03889131546, Accuracy = 0.807000041008\n",
      "Training epochs #1282000:   Batch Loss = 6.948938, Accuracy = 0.84399998188\n",
      "Performance on test set: Batch Loss = 6.97510671616, Accuracy = 0.826000034809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #1284000:   Batch Loss = 6.927911, Accuracy = 0.832000017166\n",
      "Performance on test set: Batch Loss = 7.90134716034, Accuracy = 0.206999987364\n",
      "Training epochs #1286000:   Batch Loss = 7.699704, Accuracy = 0.760000050068\n",
      "Performance on test set: Batch Loss = 8.47776794434, Accuracy = 0.148000001907\n",
      "Training epochs #1288000:   Batch Loss = 9.276636, Accuracy = 0.67199999094\n",
      "Performance on test set: Batch Loss = 9.13684749603, Accuracy = 0.68900001049\n",
      "Training epochs #1290000:   Batch Loss = 9.023153, Accuracy = 0.662999927998\n",
      "Performance on test set: Batch Loss = 8.97841072083, Accuracy = 0.661000013351\n",
      "Training epochs #1292000:   Batch Loss = 8.907187, Accuracy = 0.638999998569\n",
      "Performance on test set: Batch Loss = 9.18665122986, Accuracy = 0.620999991894\n",
      "Training epochs #1294000:   Batch Loss = 9.012928, Accuracy = 0.632000029087\n",
      "Performance on test set: Batch Loss = 8.97581863403, Accuracy = 0.636000037193\n",
      "Training epochs #1296000:   Batch Loss = 8.930285, Accuracy = 0.631999969482\n",
      "Performance on test set: Batch Loss = 8.84892368317, Accuracy = 0.655999958515\n",
      "Training epochs #1298000:   Batch Loss = 8.813758, Accuracy = 0.627000033855\n",
      "Performance on test set: Batch Loss = 8.58023834229, Accuracy = 0.657999992371\n",
      "Training epochs #1300000:   Batch Loss = 8.565685, Accuracy = 0.648000001907\n",
      "Performance on test set: Batch Loss = 8.50964069366, Accuracy = 0.630999982357\n",
      "Training epochs #1302000:   Batch Loss = 8.447835, Accuracy = 0.638999998569\n",
      "Performance on test set: Batch Loss = 8.28349113464, Accuracy = 0.662000000477\n",
      "Training epochs #1304000:   Batch Loss = 8.289865, Accuracy = 0.646000027657\n",
      "Performance on test set: Batch Loss = 8.26838874817, Accuracy = 0.641000032425\n",
      "Training epochs #1306000:   Batch Loss = 8.177003, Accuracy = 0.638000011444\n",
      "Performance on test set: Batch Loss = 7.9444065094, Accuracy = 0.668999969959\n",
      "Training epochs #1308000:   Batch Loss = 7.934594, Accuracy = 0.634999990463\n",
      "Performance on test set: Batch Loss = 7.90848207474, Accuracy = 0.634999990463\n",
      "Training epochs #1310000:   Batch Loss = 7.815036, Accuracy = 0.638000011444\n",
      "Performance on test set: Batch Loss = 7.75742721558, Accuracy = 0.661000013351\n",
      "Training epochs #1312000:   Batch Loss = 7.717903, Accuracy = 0.64200001955\n",
      "Performance on test set: Batch Loss = 7.81292104721, Accuracy = 0.620999991894\n",
      "Training epochs #1314000:   Batch Loss = 7.725606, Accuracy = 0.643999993801\n",
      "Performance on test set: Batch Loss = 7.80794477463, Accuracy = 0.635999977589\n",
      "Training epochs #1316000:   Batch Loss = 7.763650, Accuracy = 0.651000022888\n",
      "Performance on test set: Batch Loss = 7.73128509521, Accuracy = 0.667000055313\n",
      "Training epochs #1318000:   Batch Loss = 7.816218, Accuracy = 0.666000008583\n",
      "Performance on test set: Batch Loss = 7.75095176697, Accuracy = 0.656999945641\n",
      "Training epochs #1320000:   Batch Loss = 7.707983, Accuracy = 0.671000003815\n",
      "Performance on test set: Batch Loss = 7.77695322037, Accuracy = 0.625000059605\n",
      "Training epochs #1322000:   Batch Loss = 7.693783, Accuracy = 0.65499997139\n",
      "Performance on test set: Batch Loss = 7.6845035553, Accuracy = 0.656999945641\n",
      "Training epochs #1324000:   Batch Loss = 7.678460, Accuracy = 0.648000001907\n",
      "Performance on test set: Batch Loss = 7.72387695312, Accuracy = 0.636999964714\n",
      "Training epochs #1326000:   Batch Loss = 7.667429, Accuracy = 0.644999980927\n",
      "Performance on test set: Batch Loss = 7.61652994156, Accuracy = 0.666000008583\n",
      "Training epochs #1328000:   Batch Loss = 7.676682, Accuracy = 0.629999995232\n",
      "Performance on test set: Batch Loss = 7.68950080872, Accuracy = 0.633999943733\n",
      "Training epochs #1330000:   Batch Loss = 7.668637, Accuracy = 0.637000024319\n",
      "Performance on test set: Batch Loss = 7.61892843246, Accuracy = 0.661000013351\n",
      "Training epochs #1332000:   Batch Loss = 7.658549, Accuracy = 0.651999950409\n",
      "Performance on test set: Batch Loss = 7.73055458069, Accuracy = 0.620999991894\n",
      "Training epochs #1334000:   Batch Loss = 7.714775, Accuracy = 0.632000029087\n",
      "Performance on test set: Batch Loss = 7.68797445297, Accuracy = 0.636999964714\n",
      "Training epochs #1336000:   Batch Loss = 7.567670, Accuracy = 0.659999966621\n",
      "Performance on test set: Batch Loss = 7.59025859833, Accuracy = 0.65600001812\n",
      "Training epochs #1338000:   Batch Loss = 7.593167, Accuracy = 0.662000000477\n",
      "Performance on test set: Batch Loss = 7.57106494904, Accuracy = 0.656999945641\n",
      "Training epochs #1340000:   Batch Loss = 7.596558, Accuracy = 0.638000011444\n",
      "Performance on test set: Batch Loss = 7.62263536453, Accuracy = 0.626999974251\n",
      "Training epochs #1342000:   Batch Loss = 7.498865, Accuracy = 0.679000020027\n",
      "Performance on test set: Batch Loss = 7.56570243835, Accuracy = 0.656999945641\n",
      "Training epochs #1344000:   Batch Loss = 7.616951, Accuracy = 0.633999943733\n",
      "Performance on test set: Batch Loss = 7.61223602295, Accuracy = 0.638000011444\n",
      "Training epochs #1346000:   Batch Loss = 7.575814, Accuracy = 0.648999989033\n",
      "Performance on test set: Batch Loss = 7.53828763962, Accuracy = 0.667000055313\n",
      "Training epochs #1348000:   Batch Loss = 7.546017, Accuracy = 0.662000000477\n",
      "Performance on test set: Batch Loss = 7.60356855392, Accuracy = 0.633999943733\n",
      "Training epochs #1350000:   Batch Loss = 7.635259, Accuracy = 0.629999995232\n",
      "Performance on test set: Batch Loss = 7.53641176224, Accuracy = 0.661000013351\n",
      "Training epochs #1352000:   Batch Loss = 7.517271, Accuracy = 0.635999977589\n",
      "Performance on test set: Batch Loss = 7.62891960144, Accuracy = 0.621999979019\n",
      "Training epochs #1354000:   Batch Loss = 7.514561, Accuracy = 0.668999969959\n",
      "Performance on test set: Batch Loss = 7.59790468216, Accuracy = 0.637000024319\n",
      "Training epochs #1356000:   Batch Loss = 7.584034, Accuracy = 0.620999991894\n",
      "Performance on test set: Batch Loss = 7.49133014679, Accuracy = 0.657000005245\n",
      "Training epochs #1358000:   Batch Loss = 7.507633, Accuracy = 0.664999961853\n",
      "Performance on test set: Batch Loss = 7.50291919708, Accuracy = 0.657000005245\n",
      "Training epochs #1360000:   Batch Loss = 7.527767, Accuracy = 0.645000040531\n",
      "Performance on test set: Batch Loss = 7.55439662933, Accuracy = 0.626999974251\n",
      "Training epochs #1362000:   Batch Loss = 7.495212, Accuracy = 0.657999932766\n",
      "Performance on test set: Batch Loss = 7.50489473343, Accuracy = 0.657000005245\n",
      "Training epochs #1364000:   Batch Loss = 7.483492, Accuracy = 0.648999989033\n",
      "Performance on test set: Batch Loss = 7.53410053253, Accuracy = 0.638000011444\n",
      "Training epochs #1366000:   Batch Loss = 7.572063, Accuracy = 0.617999970913\n",
      "Performance on test set: Batch Loss = 7.4657702446, Accuracy = 0.666999995708\n",
      "Training epochs #1368000:   Batch Loss = 7.550768, Accuracy = 0.617999970913\n",
      "Performance on test set: Batch Loss = 7.53106737137, Accuracy = 0.635999917984\n",
      "Training epochs #1370000:   Batch Loss = 7.496993, Accuracy = 0.649999976158\n",
      "Performance on test set: Batch Loss = 7.4627494812, Accuracy = 0.665000021458\n",
      "Training epochs #1372000:   Batch Loss = 7.492678, Accuracy = 0.643999993801\n",
      "Performance on test set: Batch Loss = 7.55099582672, Accuracy = 0.625\n",
      "Training epochs #1374000:   Batch Loss = 7.531517, Accuracy = 0.636000037193\n",
      "Performance on test set: Batch Loss = 7.52891254425, Accuracy = 0.641000032425\n",
      "Training epochs #1376000:   Batch Loss = 7.519349, Accuracy = 0.634999990463\n",
      "Performance on test set: Batch Loss = 7.42027711868, Accuracy = 0.666999995708\n",
      "Training epochs #1378000:   Batch Loss = 7.525584, Accuracy = 0.630000054836\n",
      "Performance on test set: Batch Loss = 7.44125366211, Accuracy = 0.65700006485\n",
      "Training epochs #1380000:   Batch Loss = 7.455967, Accuracy = 0.650000035763\n",
      "Performance on test set: Batch Loss = 7.49279403687, Accuracy = 0.634000003338\n",
      "Training epochs #1382000:   Batch Loss = 7.482963, Accuracy = 0.638000011444\n",
      "Performance on test set: Batch Loss = 7.44506311417, Accuracy = 0.657999992371\n",
      "Training epochs #1384000:   Batch Loss = 7.456146, Accuracy = 0.639999985695\n",
      "Performance on test set: Batch Loss = 7.4627661705, Accuracy = 0.641000032425\n",
      "Training epochs #1386000:   Batch Loss = 7.510177, Accuracy = 0.636000037193\n",
      "Performance on test set: Batch Loss = 7.40635156631, Accuracy = 0.675999939442\n",
      "Training epochs #1388000:   Batch Loss = 7.465344, Accuracy = 0.638000011444\n",
      "Performance on test set: Batch Loss = 7.47047758102, Accuracy = 0.635999977589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #1390000:   Batch Loss = 7.453454, Accuracy = 0.643000006676\n",
      "Performance on test set: Batch Loss = 7.40017843246, Accuracy = 0.665000021458\n",
      "Training epochs #1392000:   Batch Loss = 7.419009, Accuracy = 0.648999989033\n",
      "Performance on test set: Batch Loss = 7.48628902435, Accuracy = 0.625\n",
      "Training epochs #1394000:   Batch Loss = 7.425694, Accuracy = 0.652000010014\n",
      "Performance on test set: Batch Loss = 7.47120475769, Accuracy = 0.641000032425\n",
      "Training epochs #1396000:   Batch Loss = 7.397543, Accuracy = 0.763000011444\n",
      "Performance on test set: Batch Loss = 7.35740566254, Accuracy = 0.775999963284\n",
      "Training epochs #1398000:   Batch Loss = 7.427001, Accuracy = 0.757000029087\n",
      "Performance on test set: Batch Loss = 7.38899707794, Accuracy = 0.76700001955\n",
      "Training epochs #1400000:   Batch Loss = 7.340302, Accuracy = 0.777000010014\n",
      "Performance on test set: Batch Loss = 7.4388256073, Accuracy = 0.740000069141\n",
      "Training epochs #1402000:   Batch Loss = 7.386950, Accuracy = 0.769000053406\n",
      "Performance on test set: Batch Loss = 7.39429712296, Accuracy = 0.756999969482\n",
      "Training epochs #1404000:   Batch Loss = 7.402877, Accuracy = 0.761999964714\n",
      "Performance on test set: Batch Loss = 7.40285253525, Accuracy = 0.759999990463\n",
      "Training epochs #1406000:   Batch Loss = 7.365617, Accuracy = 0.767000079155\n",
      "Performance on test set: Batch Loss = 7.35049629211, Accuracy = 0.766999900341\n",
      "Training epochs #1408000:   Batch Loss = 7.417256, Accuracy = 0.741000056267\n",
      "Performance on test set: Batch Loss = 7.41510868073, Accuracy = 0.749000012875\n",
      "Training epochs #1410000:   Batch Loss = 7.377783, Accuracy = 0.759999990463\n",
      "Performance on test set: Batch Loss = 7.34282255173, Accuracy = 0.775000035763\n",
      "Training epochs #1412000:   Batch Loss = 7.398432, Accuracy = 0.740000009537\n",
      "Performance on test set: Batch Loss = 7.42604255676, Accuracy = 0.742000043392\n",
      "Training epochs #1414000:   Batch Loss = 7.401563, Accuracy = 0.752999961376\n",
      "Performance on test set: Batch Loss = 7.41500329971, Accuracy = 0.742999970913\n",
      "Training epochs #1416000:   Batch Loss = 7.328171, Accuracy = 0.762999951839\n",
      "Performance on test set: Batch Loss = 7.30176734924, Accuracy = 0.781000077724\n",
      "Training epochs #1418000:   Batch Loss = 7.312507, Accuracy = 0.773999989033\n",
      "Performance on test set: Batch Loss = 7.33076238632, Accuracy = 0.76700001955\n",
      "Training epochs #1420000:   Batch Loss = 7.339971, Accuracy = 0.759999990463\n",
      "Performance on test set: Batch Loss = 7.38133144379, Accuracy = 0.740000069141\n",
      "Training epochs #1422000:   Batch Loss = 7.267586, Accuracy = 0.787999987602\n",
      "Performance on test set: Batch Loss = 7.33690261841, Accuracy = 0.757000029087\n",
      "Training epochs #1424000:   Batch Loss = 7.358787, Accuracy = 0.746999979019\n",
      "Performance on test set: Batch Loss = 7.34522533417, Accuracy = 0.758999943733\n",
      "Training epochs #1426000:   Batch Loss = 7.341465, Accuracy = 0.752000033855\n",
      "Performance on test set: Batch Loss = 7.29536485672, Accuracy = 0.766999959946\n",
      "Training epochs #1428000:   Batch Loss = 7.311839, Accuracy = 0.7650000453\n",
      "Performance on test set: Batch Loss = 7.35528421402, Accuracy = 0.749000012875\n",
      "Training epochs #1430000:   Batch Loss = 7.398325, Accuracy = 0.735999941826\n",
      "Performance on test set: Batch Loss = 7.28464508057, Accuracy = 0.775000035763\n",
      "Training epochs #1432000:   Batch Loss = 7.291505, Accuracy = 0.763000011444\n",
      "Performance on test set: Batch Loss = 7.37036848068, Accuracy = 0.741999983788\n",
      "Training epochs #1434000:   Batch Loss = 7.283914, Accuracy = 0.768000006676\n",
      "Performance on test set: Batch Loss = 7.36217546463, Accuracy = 0.743000030518\n",
      "Training epochs #1436000:   Batch Loss = 7.334185, Accuracy = 0.753000020981\n",
      "Performance on test set: Batch Loss = 7.24691963196, Accuracy = 0.781000077724\n",
      "Training epochs #1438000:   Batch Loss = 7.279930, Accuracy = 0.764999985695\n",
      "Performance on test set: Batch Loss = 7.27631187439, Accuracy = 0.76700001955\n",
      "Training epochs #1440000:   Batch Loss = 7.307363, Accuracy = 0.751999974251\n",
      "Performance on test set: Batch Loss = 7.32700252533, Accuracy = 0.738000035286\n",
      "Training epochs #1442000:   Batch Loss = 7.270083, Accuracy = 0.763000011444\n",
      "Performance on test set: Batch Loss = 7.28667831421, Accuracy = 0.756000041962\n",
      "Training epochs #1444000:   Batch Loss = 7.233690, Accuracy = 0.775000035763\n",
      "Performance on test set: Batch Loss = 7.2898516655, Accuracy = 0.759999990463\n",
      "Training epochs #1446000:   Batch Loss = 7.336862, Accuracy = 0.726999998093\n",
      "Performance on test set: Batch Loss = 7.24098539352, Accuracy = 0.766000032425\n",
      "Training epochs #1448000:   Batch Loss = 7.309628, Accuracy = 0.740000009537\n",
      "Performance on test set: Batch Loss = 7.29947185516, Accuracy = 0.748000025749\n",
      "Training epochs #1450000:   Batch Loss = 7.285743, Accuracy = 0.755999982357\n",
      "Performance on test set: Batch Loss = 7.23276901245, Accuracy = 0.773999929428\n",
      "Training epochs #1452000:   Batch Loss = 7.277642, Accuracy = 0.754999995232\n",
      "Performance on test set: Batch Loss = 7.31501245499, Accuracy = 0.742000043392\n",
      "Training epochs #1454000:   Batch Loss = 7.299319, Accuracy = 0.741000056267\n",
      "Performance on test set: Batch Loss = 7.31176614761, Accuracy = 0.743000030518\n",
      "Training epochs #1456000:   Batch Loss = 7.312255, Accuracy = 0.736000001431\n",
      "Performance on test set: Batch Loss = 7.19472694397, Accuracy = 0.782000005245\n",
      "Training epochs #1458000:   Batch Loss = 7.308343, Accuracy = 0.733000040054\n",
      "Performance on test set: Batch Loss = 7.21966838837, Accuracy = 0.766999959946\n",
      "Training epochs #1460000:   Batch Loss = 7.246889, Accuracy = 0.759999990463\n",
      "Performance on test set: Batch Loss = 7.27010440826, Accuracy = 0.737999975681\n",
      "Training epochs #1462000:   Batch Loss = 7.270770, Accuracy = 0.744000017643\n",
      "Performance on test set: Batch Loss = 7.23276329041, Accuracy = 0.755999982357\n",
      "Training epochs #1464000:   Batch Loss = 7.221810, Accuracy = 0.752999961376\n",
      "Performance on test set: Batch Loss = 7.23427391052, Accuracy = 0.760000050068\n",
      "Training epochs #1466000:   Batch Loss = 7.295437, Accuracy = 0.739000022411\n",
      "Performance on test set: Batch Loss = 7.18775749207, Accuracy = 0.76599997282\n",
      "Training epochs #1468000:   Batch Loss = 7.260124, Accuracy = 0.736000061035\n",
      "Performance on test set: Batch Loss = 7.24317264557, Accuracy = 0.746999979019\n",
      "Training epochs #1470000:   Batch Loss = 7.239274, Accuracy = 0.755000054836\n",
      "Performance on test set: Batch Loss = 7.1778883934, Accuracy = 0.773999989033\n",
      "Training epochs #1472000:   Batch Loss = 7.222824, Accuracy = 0.75200009346\n",
      "Performance on test set: Batch Loss = 7.2614440918, Accuracy = 0.741999983788\n",
      "Training epochs #1474000:   Batch Loss = 7.217396, Accuracy = 0.753000020981\n",
      "Performance on test set: Batch Loss = 7.26485061646, Accuracy = 0.743000030518\n",
      "Training epochs #1476000:   Batch Loss = 7.181279, Accuracy = 0.76599997282\n",
      "Performance on test set: Batch Loss = 7.14494562149, Accuracy = 0.780999958515\n",
      "Training epochs #1478000:   Batch Loss = 7.204997, Accuracy = 0.758000016212\n",
      "Performance on test set: Batch Loss = 7.16762447357, Accuracy = 0.767999947071\n",
      "Training epochs #1480000:   Batch Loss = 7.115484, Accuracy = 0.773999929428\n",
      "Performance on test set: Batch Loss = 7.21704101562, Accuracy = 0.738000035286\n",
      "Training epochs #1482000:   Batch Loss = 7.169360, Accuracy = 0.771000087261\n",
      "Performance on test set: Batch Loss = 7.18518972397, Accuracy = 0.755999982357\n",
      "Training epochs #1484000:   Batch Loss = 7.189448, Accuracy = 0.762000024319\n",
      "Performance on test set: Batch Loss = 7.18239879608, Accuracy = 0.759999990463\n",
      "Training epochs #1486000:   Batch Loss = 7.147178, Accuracy = 0.766000032425\n",
      "Performance on test set: Batch Loss = 7.13086795807, Accuracy = 0.76599997282\n",
      "Training epochs #1488000:   Batch Loss = 7.216179, Accuracy = 0.741000056267\n",
      "Performance on test set: Batch Loss = 7.18638086319, Accuracy = 0.747000038624\n",
      "Training epochs #1490000:   Batch Loss = 7.162317, Accuracy = 0.759999990463\n",
      "Performance on test set: Batch Loss = 7.12534284592, Accuracy = 0.773000001907\n",
      "Training epochs #1492000:   Batch Loss = 7.184348, Accuracy = 0.738000035286\n",
      "Performance on test set: Batch Loss = 7.20976781845, Accuracy = 0.741999983788\n",
      "Training epochs #1494000:   Batch Loss = 7.169621, Accuracy = 0.753000020981\n",
      "Performance on test set: Batch Loss = 7.21032619476, Accuracy = 0.743000030518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #1496000:   Batch Loss = 7.117764, Accuracy = 0.7650000453\n",
      "Performance on test set: Batch Loss = 7.09571170807, Accuracy = 0.779000043869\n",
      "Training epochs #1498000:   Batch Loss = 7.095755, Accuracy = 0.772000014782\n",
      "Performance on test set: Batch Loss = 7.10632514954, Accuracy = 0.768000006676\n",
      "Training epochs #1500000:   Batch Loss = 7.123664, Accuracy = 0.759000003338\n",
      "Performance on test set: Batch Loss = 7.1605257988, Accuracy = 0.737000048161\n",
      "Training epochs #1502000:   Batch Loss = 7.053750, Accuracy = 0.788999974728\n",
      "Performance on test set: Batch Loss = 7.12854623795, Accuracy = 0.755999982357\n",
      "Training epochs #1504000:   Batch Loss = 7.137825, Accuracy = 0.745000004768\n",
      "Performance on test set: Batch Loss = 7.12749385834, Accuracy = 0.759999990463\n",
      "Training epochs #1506000:   Batch Loss = 7.129962, Accuracy = 0.754000008106\n",
      "Performance on test set: Batch Loss = 7.07403039932, Accuracy = 0.766000032425\n",
      "Training epochs #1508000:   Batch Loss = 7.097121, Accuracy = 0.76599997282\n",
      "Performance on test set: Batch Loss = 7.12587976456, Accuracy = 0.745000064373\n",
      "Training epochs #1510000:   Batch Loss = 7.199535, Accuracy = 0.736000001431\n",
      "Performance on test set: Batch Loss = 7.06478071213, Accuracy = 0.772000014782\n",
      "Training epochs #1512000:   Batch Loss = 7.086140, Accuracy = 0.759999990463\n",
      "Performance on test set: Batch Loss = 7.14939308167, Accuracy = 0.740999996662\n",
      "Training epochs #1514000:   Batch Loss = 7.055305, Accuracy = 0.76800006628\n",
      "Performance on test set: Batch Loss = 7.13828134537, Accuracy = 0.743000030518\n",
      "Training epochs #1516000:   Batch Loss = 7.118635, Accuracy = 0.75100004673\n",
      "Performance on test set: Batch Loss = 7.03603792191, Accuracy = 0.778000056744\n",
      "Training epochs #1518000:   Batch Loss = 7.048225, Accuracy = 0.762999951839\n",
      "Performance on test set: Batch Loss = 7.03036832809, Accuracy = 0.766000032425\n",
      "Training epochs #1520000:   Batch Loss = 7.082735, Accuracy = 0.749000012875\n",
      "Performance on test set: Batch Loss = 7.07633495331, Accuracy = 0.739000082016\n",
      "Training epochs #1522000:   Batch Loss = 7.021233, Accuracy = 0.766000032425\n",
      "Performance on test set: Batch Loss = 7.04070186615, Accuracy = 0.757000029087\n",
      "Training epochs #1524000:   Batch Loss = 6.980430, Accuracy = 0.777999997139\n",
      "Performance on test set: Batch Loss = 7.0356054306, Accuracy = 0.761000037193\n",
      "Training epochs #1526000:   Batch Loss = 7.053124, Accuracy = 0.731000006199\n",
      "Performance on test set: Batch Loss = 6.97106075287, Accuracy = 0.773000001907\n",
      "Training epochs #1528000:   Batch Loss = 7.028370, Accuracy = 0.754999995232\n",
      "Performance on test set: Batch Loss = 7.01424884796, Accuracy = 0.754000008106\n",
      "Training epochs #1530000:   Batch Loss = 7.022248, Accuracy = 0.768000006676\n",
      "Performance on test set: Batch Loss = 6.94636487961, Accuracy = 0.789000034332\n",
      "Training epochs #1532000:   Batch Loss = 6.973113, Accuracy = 0.794000029564\n",
      "Performance on test set: Batch Loss = 7.0300526619, Accuracy = 0.777000010014\n",
      "Training epochs #1534000:   Batch Loss = 6.953571, Accuracy = 0.802000045776\n",
      "Performance on test set: Batch Loss = 6.97789812088, Accuracy = 0.797999978065\n",
      "Training epochs #1536000:   Batch Loss = 6.980859, Accuracy = 0.792000055313\n",
      "Performance on test set: Batch Loss = 6.97193670273, Accuracy = 0.817000031471\n",
      "Training epochs #1538000:   Batch Loss = 7.223746, Accuracy = 0.720999956131\n",
      "Performance on test set: Batch Loss = 7.23012733459, Accuracy = 0.751999974251\n",
      "Training epochs #1540000:   Batch Loss = 7.331092, Accuracy = 0.745999991894\n",
      "Performance on test set: Batch Loss = 7.32459831238, Accuracy = 0.723999977112\n",
      "Training epochs #1542000:   Batch Loss = 7.357780, Accuracy = 0.726999998093\n",
      "Performance on test set: Batch Loss = 7.28210163116, Accuracy = 0.743000030518\n",
      "Training epochs #1544000:   Batch Loss = 7.216925, Accuracy = 0.743000030518\n",
      "Performance on test set: Batch Loss = 7.2100315094, Accuracy = 0.745000004768\n",
      "Training epochs #1546000:   Batch Loss = 7.205552, Accuracy = 0.744000017643\n",
      "Performance on test set: Batch Loss = 7.0729932785, Accuracy = 0.76800006628\n",
      "Training epochs #1548000:   Batch Loss = 7.153109, Accuracy = 0.743000030518\n",
      "Performance on test set: Batch Loss = 7.10424661636, Accuracy = 0.752000033855\n",
      "Training epochs #1550000:   Batch Loss = 7.126586, Accuracy = 0.758000016212\n",
      "Performance on test set: Batch Loss = 7.0846824646, Accuracy = 0.784000039101\n",
      "Training epochs #1552000:   Batch Loss = 7.124952, Accuracy = 0.763000011444\n",
      "Performance on test set: Batch Loss = 7.1339468956, Accuracy = 0.757999956608\n",
      "Training epochs #1554000:   Batch Loss = 7.081423, Accuracy = 0.767999947071\n",
      "Performance on test set: Batch Loss = 7.12916278839, Accuracy = 0.746999979019\n",
      "Training epochs #1556000:   Batch Loss = 7.029369, Accuracy = 0.773999929428\n",
      "Performance on test set: Batch Loss = 6.98894882202, Accuracy = 0.794000089169\n",
      "Training epochs #1558000:   Batch Loss = 7.051759, Accuracy = 0.760000050068\n",
      "Performance on test set: Batch Loss = 7.00991821289, Accuracy = 0.768999993801\n",
      "Training epochs #1560000:   Batch Loss = 6.948803, Accuracy = 0.777999997139\n",
      "Performance on test set: Batch Loss = 7.06204795837, Accuracy = 0.743000030518\n",
      "Training epochs #1562000:   Batch Loss = 6.990847, Accuracy = 0.774999976158\n",
      "Performance on test set: Batch Loss = 7.03024244308, Accuracy = 0.764000058174\n",
      "Training epochs #1564000:   Batch Loss = 7.009251, Accuracy = 0.768000006676\n",
      "Performance on test set: Batch Loss = 7.00357723236, Accuracy = 0.774999976158\n",
      "Training epochs #1566000:   Batch Loss = 6.929661, Accuracy = 0.787999987602\n",
      "Performance on test set: Batch Loss = 6.92051076889, Accuracy = 0.795000016689\n",
      "Training epochs #1568000:   Batch Loss = 6.987034, Accuracy = 0.780000030994\n",
      "Performance on test set: Batch Loss = 6.9399061203, Accuracy = 0.777000010014\n",
      "Training epochs #1570000:   Batch Loss = 6.943451, Accuracy = 0.768999934196\n",
      "Performance on test set: Batch Loss = 6.91874742508, Accuracy = 0.783000051975\n",
      "Training epochs #1572000:   Batch Loss = 6.954851, Accuracy = 0.759999990463\n",
      "Performance on test set: Batch Loss = 6.94534015656, Accuracy = 0.763000011444\n",
      "Training epochs #1574000:   Batch Loss = 6.954172, Accuracy = 0.76700001955\n",
      "Performance on test set: Batch Loss = 7.08383846283, Accuracy = 0.764999985695\n",
      "Training epochs #1576000:   Batch Loss = 7.066167, Accuracy = 0.784000039101\n",
      "Performance on test set: Batch Loss = 7.12584400177, Accuracy = 0.79800003767\n",
      "Training epochs #1578000:   Batch Loss = 7.093316, Accuracy = 0.786000013351\n",
      "Performance on test set: Batch Loss = 7.10955286026, Accuracy = 0.770999968052\n",
      "Training epochs #1580000:   Batch Loss = 7.142751, Accuracy = 0.759000062943\n",
      "Performance on test set: Batch Loss = 7.20083665848, Accuracy = 0.741000056267\n",
      "Training epochs #1582000:   Batch Loss = 7.031388, Accuracy = 0.795000076294\n",
      "Performance on test set: Batch Loss = 7.12766170502, Accuracy = 0.764000058174\n",
      "Training epochs #1584000:   Batch Loss = 7.129363, Accuracy = 0.754000008106\n",
      "Performance on test set: Batch Loss = 7.11587572098, Accuracy = 0.762000024319\n",
      "Training epochs #1586000:   Batch Loss = 7.088959, Accuracy = 0.754000008106\n",
      "Performance on test set: Batch Loss = 7.03571653366, Accuracy = 0.768999934196\n",
      "Training epochs #1588000:   Batch Loss = 7.060060, Accuracy = 0.766000032425\n",
      "Performance on test set: Batch Loss = 7.05734729767, Accuracy = 0.751999974251\n",
      "Training epochs #1590000:   Batch Loss = 7.162595, Accuracy = 0.745000064373\n",
      "Performance on test set: Batch Loss = 7.0369386673, Accuracy = 0.776000022888\n",
      "Training epochs #1592000:   Batch Loss = 7.027769, Accuracy = 0.773000001907\n",
      "Performance on test set: Batch Loss = 7.10293149948, Accuracy = 0.744000077248\n",
      "Training epochs #1594000:   Batch Loss = 6.996576, Accuracy = 0.774999976158\n",
      "Performance on test set: Batch Loss = 7.07811832428, Accuracy = 0.746999979019\n",
      "Training epochs #1596000:   Batch Loss = 7.052684, Accuracy = 0.756999969482\n",
      "Performance on test set: Batch Loss = 6.96408176422, Accuracy = 0.787000060081\n",
      "Training epochs #1598000:   Batch Loss = 6.980050, Accuracy = 0.76599997282\n",
      "Performance on test set: Batch Loss = 6.9612030983, Accuracy = 0.769999980927\n",
      "Training epochs #1600000:   Batch Loss = 7.018649, Accuracy = 0.754999995232\n",
      "Performance on test set: Batch Loss = 6.99937772751, Accuracy = 0.740999996662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #1602000:   Batch Loss = 6.926245, Accuracy = 0.770000040531\n",
      "Performance on test set: Batch Loss = 6.94938421249, Accuracy = 0.761999964714\n",
      "Training epochs #1604000:   Batch Loss = 6.856914, Accuracy = 0.782000005245\n",
      "Performance on test set: Batch Loss = 6.92906427383, Accuracy = 0.773999929428\n",
      "Training epochs #1606000:   Batch Loss = 6.931858, Accuracy = 0.737999975681\n",
      "Performance on test set: Batch Loss = 6.86778259277, Accuracy = 0.797999978065\n",
      "Training epochs #1608000:   Batch Loss = 6.925056, Accuracy = 0.766999959946\n",
      "Performance on test set: Batch Loss = 6.9030623436, Accuracy = 0.767999947071\n",
      "Training epochs #1610000:   Batch Loss = 6.912362, Accuracy = 0.772000014782\n",
      "Performance on test set: Batch Loss = 6.85202455521, Accuracy = 0.782000005245\n",
      "Training epochs #1612000:   Batch Loss = 6.867047, Accuracy = 0.767999947071\n",
      "Performance on test set: Batch Loss = 6.89791297913, Accuracy = 0.75\n",
      "Training epochs #1614000:   Batch Loss = 6.861632, Accuracy = 0.749000012875\n",
      "Performance on test set: Batch Loss = 6.88455295563, Accuracy = 0.754999995232\n",
      "Training epochs #1616000:   Batch Loss = 6.896175, Accuracy = 0.752000033855\n",
      "Performance on test set: Batch Loss = 6.79025506973, Accuracy = 0.805999994278\n",
      "Training epochs #1618000:   Batch Loss = 6.876250, Accuracy = 0.75\n",
      "Performance on test set: Batch Loss = 6.7704000473, Accuracy = 0.797999978065\n",
      "Training epochs #1620000:   Batch Loss = 6.823275, Accuracy = 0.784000039101\n",
      "Performance on test set: Batch Loss = 6.80898952484, Accuracy = 0.764000058174\n",
      "Training epochs #1622000:   Batch Loss = 6.836609, Accuracy = 0.769999980927\n",
      "Performance on test set: Batch Loss = 6.78449344635, Accuracy = 0.788999974728\n",
      "Training epochs #1624000:   Batch Loss = 6.735979, Accuracy = 0.795000076294\n",
      "Performance on test set: Batch Loss = 6.80098867416, Accuracy = 0.797000050545\n",
      "Training epochs #1626000:   Batch Loss = 6.833735, Accuracy = 0.771999955177\n",
      "Performance on test set: Batch Loss = 6.72923183441, Accuracy = 0.803000032902\n",
      "Training epochs #1628000:   Batch Loss = 6.778826, Accuracy = 0.800999939442\n",
      "Performance on test set: Batch Loss = 6.74480628967, Accuracy = 0.809000015259\n",
      "Training epochs #1630000:   Batch Loss = 6.736141, Accuracy = 0.842000007629\n",
      "Performance on test set: Batch Loss = 6.72125244141, Accuracy = 0.844000041485\n",
      "Training epochs #1632000:   Batch Loss = 6.764042, Accuracy = 0.827000021935\n",
      "Performance on test set: Batch Loss = 6.77906608582, Accuracy = 0.826000034809\n",
      "Training epochs #1634000:   Batch Loss = 6.721121, Accuracy = 0.855999946594\n",
      "Performance on test set: Batch Loss = 6.74993419647, Accuracy = 0.837000012398\n",
      "Training epochs #1636000:   Batch Loss = 6.707827, Accuracy = 0.846000015736\n",
      "Performance on test set: Batch Loss = 6.69133615494, Accuracy = 0.85799998045\n",
      "Training epochs #1638000:   Batch Loss = 6.711090, Accuracy = 0.845999956131\n",
      "Performance on test set: Batch Loss = 6.68369436264, Accuracy = 0.868999958038\n",
      "Training epochs #1640000:   Batch Loss = 6.640562, Accuracy = 0.85699993372\n",
      "Performance on test set: Batch Loss = 6.68843078613, Accuracy = 0.847000062466\n",
      "Training epochs #1642000:   Batch Loss = 6.712026, Accuracy = 0.850000023842\n",
      "Performance on test set: Batch Loss = 6.99081134796, Accuracy = 0.831000089645\n",
      "Training epochs #1644000:   Batch Loss = 6.858334, Accuracy = 0.81400001049\n",
      "Performance on test set: Batch Loss = 6.90474033356, Accuracy = 0.781000077724\n",
      "Training epochs #1646000:   Batch Loss = 6.836815, Accuracy = 0.81199991703\n",
      "Performance on test set: Batch Loss = 8.02357673645, Accuracy = 0.162999987602\n",
      "Training epochs #1648000:   Batch Loss = 7.973790, Accuracy = 0.181999996305\n",
      "Performance on test set: Batch Loss = 7.65153694153, Accuracy = 0.755000054836\n",
      "Training epochs #1650000:   Batch Loss = 7.399846, Accuracy = 0.759000003338\n",
      "Performance on test set: Batch Loss = 7.25071620941, Accuracy = 0.773999989033\n",
      "Training epochs #1652000:   Batch Loss = 7.351233, Accuracy = 0.740000009537\n",
      "Performance on test set: Batch Loss = 7.39073848724, Accuracy = 0.740999996662\n",
      "Training epochs #1654000:   Batch Loss = 7.346595, Accuracy = 0.752999961376\n",
      "Performance on test set: Batch Loss = 7.40680217743, Accuracy = 0.743000030518\n",
      "Training epochs #1656000:   Batch Loss = 7.240777, Accuracy = 0.764999985695\n",
      "Performance on test set: Batch Loss = 7.24461889267, Accuracy = 0.778000056744\n",
      "Training epochs #1658000:   Batch Loss = 7.240499, Accuracy = 0.772000014782\n",
      "Performance on test set: Batch Loss = 7.17301702499, Accuracy = 0.772000014782\n",
      "Training epochs #1660000:   Batch Loss = 7.240840, Accuracy = 0.764999985695\n",
      "Performance on test set: Batch Loss = 7.23284912109, Accuracy = 0.740000069141\n",
      "Training epochs #1662000:   Batch Loss = 7.086832, Accuracy = 0.795000016689\n",
      "Performance on test set: Batch Loss = 7.18692493439, Accuracy = 0.776000022888\n",
      "Training epochs #1664000:   Batch Loss = 7.178912, Accuracy = 0.756999969482\n",
      "Performance on test set: Batch Loss = 7.10152578354, Accuracy = 0.776000082493\n",
      "Training epochs #1666000:   Batch Loss = 7.076433, Accuracy = 0.784000039101\n",
      "Performance on test set: Batch Loss = 7.00090503693, Accuracy = 0.780999958515\n",
      "Training epochs #1668000:   Batch Loss = 7.006670, Accuracy = 0.797999978065\n",
      "Performance on test set: Batch Loss = 7.02617406845, Accuracy = 0.759000062943\n",
      "Training epochs #1670000:   Batch Loss = 7.108531, Accuracy = 0.75100004673\n",
      "Performance on test set: Batch Loss = 6.92715072632, Accuracy = 0.788999974728\n",
      "Training epochs #1672000:   Batch Loss = 6.930608, Accuracy = 0.793999969959\n",
      "Performance on test set: Batch Loss = 7.38148927689, Accuracy = 0.767000079155\n",
      "Training epochs #1674000:   Batch Loss = 7.078109, Accuracy = 0.797999978065\n",
      "Performance on test set: Batch Loss = 7.46243190765, Accuracy = 0.658999979496\n",
      "Training epochs #1676000:   Batch Loss = 7.746990, Accuracy = 0.62600004673\n",
      "Performance on test set: Batch Loss = 8.04244995117, Accuracy = 0.666999995708\n",
      "Training epochs #1678000:   Batch Loss = 7.967269, Accuracy = 0.668999969959\n",
      "Performance on test set: Batch Loss = 7.79300737381, Accuracy = 0.657000005245\n",
      "Training epochs #1680000:   Batch Loss = 7.584182, Accuracy = 0.751999974251\n",
      "Performance on test set: Batch Loss = 7.3672876358, Accuracy = 0.738000035286\n",
      "Training epochs #1682000:   Batch Loss = 7.251904, Accuracy = 0.763999998569\n",
      "Performance on test set: Batch Loss = 7.2897977829, Accuracy = 0.755999982357\n",
      "Training epochs #1684000:   Batch Loss = 7.160106, Accuracy = 0.774000048637\n",
      "Performance on test set: Batch Loss = 7.22141218185, Accuracy = 0.759999990463\n",
      "Training epochs #1686000:   Batch Loss = 7.272665, Accuracy = 0.725999951363\n",
      "Performance on test set: Batch Loss = 7.17066431046, Accuracy = 0.763999998569\n",
      "Training epochs #1688000:   Batch Loss = 7.182401, Accuracy = 0.749000072479\n",
      "Performance on test set: Batch Loss = 7.18526363373, Accuracy = 0.752999961376\n",
      "Training epochs #1690000:   Batch Loss = 7.199869, Accuracy = 0.763999998569\n",
      "Performance on test set: Batch Loss = 7.10307693481, Accuracy = 0.778999984264\n",
      "Training epochs #1692000:   Batch Loss = 7.116825, Accuracy = 0.759999990463\n",
      "Performance on test set: Batch Loss = 7.1503534317, Accuracy = 0.759999990463\n",
      "Training epochs #1694000:   Batch Loss = 7.098298, Accuracy = 0.759000003338\n",
      "Performance on test set: Batch Loss = 7.04160737991, Accuracy = 0.776000022888\n",
      "Training epochs #1696000:   Batch Loss = 7.082059, Accuracy = 0.771999955177\n",
      "Performance on test set: Batch Loss = 6.94057035446, Accuracy = 0.797999978065\n",
      "Training epochs #1698000:   Batch Loss = 7.021094, Accuracy = 0.745000064373\n",
      "Performance on test set: Batch Loss = 6.89240074158, Accuracy = 0.7990000844\n",
      "Training epochs #1700000:   Batch Loss = 6.963138, Accuracy = 0.785000085831\n",
      "Performance on test set: Batch Loss = 6.97912693024, Accuracy = 0.786000013351\n",
      "Training epochs #1702000:   Batch Loss = 6.981926, Accuracy = 0.797000050545\n",
      "Performance on test set: Batch Loss = 6.92120790482, Accuracy = 0.813999950886\n",
      "Training epochs #1704000:   Batch Loss = 6.877220, Accuracy = 0.811999976635\n",
      "Performance on test set: Batch Loss = 6.90985155106, Accuracy = 0.814000070095\n",
      "Training epochs #1706000:   Batch Loss = 6.987358, Accuracy = 0.784999966621\n",
      "Performance on test set: Batch Loss = 6.87542247772, Accuracy = 0.81299996376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #1708000:   Batch Loss = 6.922607, Accuracy = 0.805000066757\n",
      "Performance on test set: Batch Loss = 6.90234947205, Accuracy = 0.793999969959\n",
      "Training epochs #1710000:   Batch Loss = 6.897814, Accuracy = 0.810000002384\n",
      "Performance on test set: Batch Loss = 6.81796169281, Accuracy = 0.819999992847\n",
      "Training epochs #1712000:   Batch Loss = 6.883049, Accuracy = 0.795000016689\n",
      "Performance on test set: Batch Loss = 6.93057250977, Accuracy = 0.777999997139\n",
      "Training epochs #1714000:   Batch Loss = 6.870363, Accuracy = 0.800000011921\n",
      "Performance on test set: Batch Loss = 6.8764333725, Accuracy = 0.795000016689\n",
      "Training epochs #1716000:   Batch Loss = 6.839674, Accuracy = 0.81400001049\n",
      "Performance on test set: Batch Loss = 6.8141617775, Accuracy = 0.823000073433\n",
      "Training epochs #1718000:   Batch Loss = 6.836658, Accuracy = 0.805999934673\n",
      "Performance on test set: Batch Loss = 6.77919054031, Accuracy = 0.831000030041\n",
      "Training epochs #1720000:   Batch Loss = 6.748571, Accuracy = 0.831000030041\n",
      "Performance on test set: Batch Loss = 6.84661960602, Accuracy = 0.796000063419\n",
      "Training epochs #1722000:   Batch Loss = 6.825697, Accuracy = 0.806999981403\n",
      "Performance on test set: Batch Loss = 6.79534816742, Accuracy = 0.822999954224\n",
      "Training epochs #1724000:   Batch Loss = 6.857460, Accuracy = 0.801999986172\n",
      "Performance on test set: Batch Loss = 6.81662082672, Accuracy = 0.814999997616\n",
      "Training epochs #1726000:   Batch Loss = 6.835538, Accuracy = 0.799000024796\n",
      "Performance on test set: Batch Loss = 6.91657972336, Accuracy = 0.781999945641\n",
      "Training epochs #1728000:   Batch Loss = 7.146817, Accuracy = 0.749000072479\n",
      "Performance on test set: Batch Loss = 7.02768325806, Accuracy = 0.755000054836\n",
      "Training epochs #1730000:   Batch Loss = 7.027458, Accuracy = 0.768999993801\n",
      "Performance on test set: Batch Loss = 6.9525270462, Accuracy = 0.787000060081\n",
      "Training epochs #1732000:   Batch Loss = 7.066610, Accuracy = 0.746999979019\n",
      "Performance on test set: Batch Loss = 7.05220031738, Accuracy = 0.749999940395\n",
      "Training epochs #1734000:   Batch Loss = 6.980857, Accuracy = 0.763000011444\n",
      "Performance on test set: Batch Loss = 7.00011396408, Accuracy = 0.755999982357\n",
      "Training epochs #1736000:   Batch Loss = 6.783244, Accuracy = 0.805999994278\n",
      "Performance on test set: Batch Loss = 6.76853084564, Accuracy = 0.81299996376\n",
      "Training epochs #1738000:   Batch Loss = 6.764381, Accuracy = 0.816000044346\n",
      "Performance on test set: Batch Loss = 6.74661970139, Accuracy = 0.830999970436\n",
      "Training epochs #1740000:   Batch Loss = 6.803518, Accuracy = 0.81099998951\n",
      "Performance on test set: Batch Loss = 8.05088424683, Accuracy = 0.18900001049\n",
      "Training epochs #1742000:   Batch Loss = 6.803115, Accuracy = 0.807000041008\n",
      "Performance on test set: Batch Loss = 7.13500785828, Accuracy = 0.757999956608\n",
      "Training epochs #1744000:   Batch Loss = 7.332341, Accuracy = 0.768000006676\n",
      "Performance on test set: Batch Loss = 7.50788068771, Accuracy = 0.64099997282\n",
      "Training epochs #1746000:   Batch Loss = 7.760025, Accuracy = 0.657000005245\n",
      "Performance on test set: Batch Loss = 8.27401256561, Accuracy = 0.675999999046\n",
      "Training epochs #1748000:   Batch Loss = 8.397448, Accuracy = 0.666999936104\n",
      "Performance on test set: Batch Loss = 8.55624198914, Accuracy = 0.634999990463\n",
      "Training epochs #1750000:   Batch Loss = 8.471342, Accuracy = 0.629999995232\n",
      "Performance on test set: Batch Loss = 8.40617752075, Accuracy = 0.661000013351\n",
      "Training epochs #1752000:   Batch Loss = 8.447902, Accuracy = 0.635999977589\n",
      "Performance on test set: Batch Loss = 8.48716640472, Accuracy = 0.626999974251\n",
      "Training epochs #1754000:   Batch Loss = 8.106444, Accuracy = 0.671000003815\n",
      "Performance on test set: Batch Loss = 8.14120864868, Accuracy = 0.645999968052\n",
      "Training epochs #1756000:   Batch Loss = 8.167343, Accuracy = 0.630999982357\n",
      "Performance on test set: Batch Loss = 7.89860200882, Accuracy = 0.667999982834\n",
      "Training epochs #1758000:   Batch Loss = 7.682013, Accuracy = 0.67199999094\n",
      "Performance on test set: Batch Loss = 7.33058834076, Accuracy = 0.76800006628\n",
      "Training epochs #1760000:   Batch Loss = 7.290792, Accuracy = 0.755999982357\n",
      "Performance on test set: Batch Loss = 7.17280769348, Accuracy = 0.741999983788\n",
      "Training epochs #1762000:   Batch Loss = 7.094316, Accuracy = 0.766999959946\n",
      "Performance on test set: Batch Loss = 7.09392881393, Accuracy = 0.758000016212\n",
      "Training epochs #1764000:   Batch Loss = 7.032233, Accuracy = 0.776999950409\n",
      "Performance on test set: Batch Loss = 7.12533760071, Accuracy = 0.763000011444\n",
      "Training epochs #1766000:   Batch Loss = 7.234809, Accuracy = 0.728000044823\n",
      "Performance on test set: Batch Loss = 7.13843679428, Accuracy = 0.766999959946\n",
      "Training epochs #1768000:   Batch Loss = 7.226952, Accuracy = 0.740999996662\n",
      "Performance on test set: Batch Loss = 7.22082328796, Accuracy = 0.75\n",
      "Training epochs #1770000:   Batch Loss = 7.181273, Accuracy = 0.759000003338\n",
      "Performance on test set: Batch Loss = 7.11696195602, Accuracy = 0.777000010014\n",
      "Training epochs #1772000:   Batch Loss = 7.114086, Accuracy = 0.756999969482\n",
      "Performance on test set: Batch Loss = 7.14162158966, Accuracy = 0.74899995327\n",
      "Training epochs #1774000:   Batch Loss = 7.118967, Accuracy = 0.741999983788\n",
      "Performance on test set: Batch Loss = 7.07187891006, Accuracy = 0.745000004768\n",
      "Training epochs #1776000:   Batch Loss = 7.064159, Accuracy = 0.737000048161\n",
      "Performance on test set: Batch Loss = 6.92765474319, Accuracy = 0.778999984264\n",
      "Training epochs #1778000:   Batch Loss = 7.013691, Accuracy = 0.73400002718\n",
      "Performance on test set: Batch Loss = 6.89966392517, Accuracy = 0.76599997282\n",
      "Training epochs #1780000:   Batch Loss = 6.951013, Accuracy = 0.763000011444\n",
      "Performance on test set: Batch Loss = 6.97157239914, Accuracy = 0.742000043392\n",
      "Training epochs #1782000:   Batch Loss = 7.012920, Accuracy = 0.743000030518\n",
      "Performance on test set: Batch Loss = 6.95382881165, Accuracy = 0.766999959946\n",
      "Training epochs #1784000:   Batch Loss = 6.932546, Accuracy = 0.766000032425\n",
      "Performance on test set: Batch Loss = 6.97041845322, Accuracy = 0.78100001812\n",
      "Training epochs #1786000:   Batch Loss = 7.014340, Accuracy = 0.763000071049\n",
      "Performance on test set: Batch Loss = 6.90999889374, Accuracy = 0.795000076294\n",
      "Training epochs #1788000:   Batch Loss = 7.004290, Accuracy = 0.761000037193\n",
      "Performance on test set: Batch Loss = 6.93753194809, Accuracy = 0.759999930859\n",
      "Training epochs #1790000:   Batch Loss = 6.942700, Accuracy = 0.762999951839\n",
      "Performance on test set: Batch Loss = 6.86629199982, Accuracy = 0.776000022888\n",
      "Training epochs #1792000:   Batch Loss = 6.928576, Accuracy = 0.753999948502\n",
      "Performance on test set: Batch Loss = 6.94729948044, Accuracy = 0.746999979019\n",
      "Training epochs #1794000:   Batch Loss = 6.900578, Accuracy = 0.763000071049\n",
      "Performance on test set: Batch Loss = 6.92464160919, Accuracy = 0.753000020981\n",
      "Training epochs #1796000:   Batch Loss = 6.860059, Accuracy = 0.771000027657\n",
      "Performance on test set: Batch Loss = 6.83733987808, Accuracy = 0.792000055313\n",
      "Training epochs #1798000:   Batch Loss = 6.888025, Accuracy = 0.762999951839\n",
      "Performance on test set: Batch Loss = 6.82188034058, Accuracy = 0.775000035763\n",
      "Training epochs #1800000:   Batch Loss = 6.800659, Accuracy = 0.785000026226\n",
      "Performance on test set: Batch Loss = 6.89396476746, Accuracy = 0.749000012875\n",
      "Training epochs #1802000:   Batch Loss = 6.845546, Accuracy = 0.774000048637\n",
      "Performance on test set: Batch Loss = 6.84624958038, Accuracy = 0.766999959946\n",
      "Training epochs #1804000:   Batch Loss = 6.870433, Accuracy = 0.770000040531\n",
      "Performance on test set: Batch Loss = 6.85080766678, Accuracy = 0.772000014782\n",
      "Training epochs #1806000:   Batch Loss = 6.802091, Accuracy = 0.778999984264\n",
      "Performance on test set: Batch Loss = 6.81637382507, Accuracy = 0.780000030994\n",
      "Training epochs #1808000:   Batch Loss = 6.885290, Accuracy = 0.754000008106\n",
      "Performance on test set: Batch Loss = 6.84270524979, Accuracy = 0.761999964714\n",
      "Training epochs #1810000:   Batch Loss = 6.850030, Accuracy = 0.773999989033\n",
      "Performance on test set: Batch Loss = 6.78686952591, Accuracy = 0.792000055313\n",
      "Training epochs #1812000:   Batch Loss = 6.829922, Accuracy = 0.757999956608\n",
      "Performance on test set: Batch Loss = 6.86969852448, Accuracy = 0.754999995232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #1814000:   Batch Loss = 6.804134, Accuracy = 0.772000014782\n",
      "Performance on test set: Batch Loss = 6.83005714417, Accuracy = 0.763000071049\n",
      "Training epochs #1816000:   Batch Loss = 6.779572, Accuracy = 0.781999945641\n",
      "Performance on test set: Batch Loss = 6.7564406395, Accuracy = 0.795000016689\n",
      "Training epochs #1818000:   Batch Loss = 6.751504, Accuracy = 0.791000008583\n",
      "Performance on test set: Batch Loss = 6.7168712616, Accuracy = 0.795000016689\n",
      "Training epochs #1820000:   Batch Loss = 6.767564, Accuracy = 0.791999995708\n",
      "Performance on test set: Batch Loss = 6.78402662277, Accuracy = 0.78100001812\n",
      "Training epochs #1822000:   Batch Loss = 6.672591, Accuracy = 0.829999983311\n",
      "Performance on test set: Batch Loss = 6.73901891708, Accuracy = 0.808999955654\n",
      "Training epochs #1824000:   Batch Loss = 6.742109, Accuracy = 0.808000028133\n",
      "Performance on test set: Batch Loss = 6.74197387695, Accuracy = 0.805999994278\n",
      "Training epochs #1826000:   Batch Loss = 6.740393, Accuracy = 0.799000024796\n",
      "Performance on test set: Batch Loss = 6.71808719635, Accuracy = 0.81400001049\n",
      "Training epochs #1828000:   Batch Loss = 6.697469, Accuracy = 0.817000031471\n",
      "Performance on test set: Batch Loss = 6.73858213425, Accuracy = 0.796000063419\n",
      "Training epochs #1830000:   Batch Loss = 6.779415, Accuracy = 0.79800003767\n",
      "Performance on test set: Batch Loss = 6.68946456909, Accuracy = 0.820999979973\n",
      "Training epochs #1832000:   Batch Loss = 6.681657, Accuracy = 0.811000108719\n",
      "Performance on test set: Batch Loss = 6.77439022064, Accuracy = 0.783999979496\n",
      "Training epochs #1834000:   Batch Loss = 6.668941, Accuracy = 0.817000031471\n",
      "Performance on test set: Batch Loss = 6.71640872955, Accuracy = 0.801999986172\n",
      "Training epochs #1836000:   Batch Loss = 6.728482, Accuracy = 0.797999978065\n",
      "Performance on test set: Batch Loss = 6.65819215775, Accuracy = 0.837999999523\n",
      "Training epochs #1838000:   Batch Loss = 6.657822, Accuracy = 0.823000073433\n",
      "Performance on test set: Batch Loss = 6.61661338806, Accuracy = 0.854000031948\n",
      "Training epochs #1840000:   Batch Loss = 6.688278, Accuracy = 0.825000047684\n",
      "Performance on test set: Batch Loss = 6.68333673477, Accuracy = 0.808999955654\n",
      "Training epochs #1842000:   Batch Loss = 6.619099, Accuracy = 0.833999991417\n",
      "Performance on test set: Batch Loss = 6.65377473831, Accuracy = 0.833000063896\n",
      "Training epochs #1844000:   Batch Loss = 6.604606, Accuracy = 0.836000025272\n",
      "Performance on test set: Batch Loss = 6.65751981735, Accuracy = 0.824999988079\n",
      "Training epochs #1846000:   Batch Loss = 6.658079, Accuracy = 0.805000007153\n",
      "Performance on test set: Batch Loss = 6.63180541992, Accuracy = 0.816000044346\n",
      "Training epochs #1848000:   Batch Loss = 6.672887, Accuracy = 0.801999986172\n",
      "Performance on test set: Batch Loss = 6.64888525009, Accuracy = 0.81200003624\n",
      "Training epochs #1850000:   Batch Loss = 6.672091, Accuracy = 0.813000023365\n",
      "Performance on test set: Batch Loss = 6.60752487183, Accuracy = 0.823000013828\n",
      "Training epochs #1852000:   Batch Loss = 6.622992, Accuracy = 0.819999933243\n",
      "Performance on test set: Batch Loss = 6.69176578522, Accuracy = 0.787999987602\n",
      "Training epochs #1854000:   Batch Loss = 6.614763, Accuracy = 0.813000023365\n",
      "Performance on test set: Batch Loss = 6.63120794296, Accuracy = 0.807999968529\n",
      "Training epochs #1856000:   Batch Loss = 6.649408, Accuracy = 0.810000002384\n",
      "Performance on test set: Batch Loss = 6.58329248428, Accuracy = 0.829999923706\n",
      "Training epochs #1858000:   Batch Loss = 6.642674, Accuracy = 0.805999934673\n",
      "Performance on test set: Batch Loss = 6.54289150238, Accuracy = 0.847999930382\n",
      "Training epochs #1860000:   Batch Loss = 6.592453, Accuracy = 0.829999923706\n",
      "Performance on test set: Batch Loss = 6.5958108902, Accuracy = 0.808000028133\n",
      "Training epochs #1862000:   Batch Loss = 6.620750, Accuracy = 0.808000028133\n",
      "Performance on test set: Batch Loss = 6.57463550568, Accuracy = 0.82800000906\n",
      "Training epochs #1864000:   Batch Loss = 6.544683, Accuracy = 0.832000017166\n",
      "Performance on test set: Batch Loss = 6.5799498558, Accuracy = 0.824000000954\n",
      "Training epochs #1866000:   Batch Loss = 6.630293, Accuracy = 0.797999978065\n",
      "Performance on test set: Batch Loss = 6.54754495621, Accuracy = 0.819000065327\n",
      "Training epochs #1868000:   Batch Loss = 6.574632, Accuracy = 0.818000018597\n",
      "Performance on test set: Batch Loss = 6.56074333191, Accuracy = 0.817999958992\n",
      "Training epochs #1870000:   Batch Loss = 6.539486, Accuracy = 0.819999992847\n",
      "Performance on test set: Batch Loss = 6.52023363113, Accuracy = 0.827000021935\n",
      "Training epochs #1872000:   Batch Loss = 6.581218, Accuracy = 0.811000049114\n",
      "Performance on test set: Batch Loss = 6.60149097443, Accuracy = 0.791000008583\n",
      "Training epochs #1874000:   Batch Loss = 6.548534, Accuracy = 0.819999992847\n",
      "Performance on test set: Batch Loss = 6.54451465607, Accuracy = 0.810000002384\n",
      "Training epochs #1876000:   Batch Loss = 6.515955, Accuracy = 0.817000091076\n",
      "Performance on test set: Batch Loss = 6.50635766983, Accuracy = 0.833000004292\n",
      "Training epochs #1878000:   Batch Loss = 6.509347, Accuracy = 0.832000017166\n",
      "Performance on test set: Batch Loss = 6.47166872025, Accuracy = 0.879999995232\n",
      "Training epochs #1880000:   Batch Loss = 6.453727, Accuracy = 0.870000004768\n",
      "Performance on test set: Batch Loss = 6.50132226944, Accuracy = 0.861000061035\n",
      "Training epochs #1882000:   Batch Loss = 6.500817, Accuracy = 0.858999967575\n",
      "Performance on test set: Batch Loss = 6.49239492416, Accuracy = 0.868000030518\n",
      "Training epochs #1884000:   Batch Loss = 6.532967, Accuracy = 0.847000002861\n",
      "Performance on test set: Batch Loss = 6.50267648697, Accuracy = 0.855000019073\n",
      "Training epochs #1886000:   Batch Loss = 6.464087, Accuracy = 0.869000017643\n",
      "Performance on test set: Batch Loss = 6.46459436417, Accuracy = 0.866000056267\n",
      "Training epochs #1888000:   Batch Loss = 6.505416, Accuracy = 0.856999993324\n",
      "Performance on test set: Batch Loss = 6.49059104919, Accuracy = 0.863000035286\n",
      "Training epochs #1890000:   Batch Loss = 6.489284, Accuracy = 0.862000048161\n",
      "Performance on test set: Batch Loss = 6.44225788116, Accuracy = 0.865000009537\n",
      "Training epochs #1892000:   Batch Loss = 6.477767, Accuracy = 0.864000022411\n",
      "Performance on test set: Batch Loss = 6.52781915665, Accuracy = 0.851000010967\n",
      "Training epochs #1894000:   Batch Loss = 6.465813, Accuracy = 0.851999998093\n",
      "Performance on test set: Batch Loss = 6.47453689575, Accuracy = 0.850000083447\n",
      "Training epochs #1896000:   Batch Loss = 6.448513, Accuracy = 0.858000040054\n",
      "Performance on test set: Batch Loss = 6.44994974136, Accuracy = 0.868000030518\n",
      "Training epochs #1898000:   Batch Loss = 6.453291, Accuracy = 0.882000029087\n",
      "Performance on test set: Batch Loss = 6.42320346832, Accuracy = 0.897000014782\n",
      "Training epochs #1900000:   Batch Loss = 6.458355, Accuracy = 0.861000061035\n",
      "Performance on test set: Batch Loss = 6.43972492218, Accuracy = 0.871000051498\n",
      "Training epochs #1902000:   Batch Loss = 6.381107, Accuracy = 0.892000079155\n",
      "Performance on test set: Batch Loss = 6.44130325317, Accuracy = 0.883000016212\n",
      "Training epochs #1904000:   Batch Loss = 6.442342, Accuracy = 0.872000038624\n",
      "Performance on test set: Batch Loss = 6.45544624329, Accuracy = 0.873000025749\n",
      "Training epochs #1906000:   Batch Loss = 6.447762, Accuracy = 0.857000052929\n",
      "Performance on test set: Batch Loss = 6.41733646393, Accuracy = 0.875999927521\n",
      "Training epochs #1908000:   Batch Loss = 6.411855, Accuracy = 0.87600004673\n",
      "Performance on test set: Batch Loss = 6.44498634338, Accuracy = 0.881000101566\n",
      "Training epochs #1910000:   Batch Loss = 6.495289, Accuracy = 0.856000006199\n",
      "Performance on test set: Batch Loss = 6.39972543716, Accuracy = 0.885999977589\n",
      "Training epochs #1912000:   Batch Loss = 6.416360, Accuracy = 0.8900000453\n",
      "Performance on test set: Batch Loss = 6.4827466011, Accuracy = 0.857000052929\n",
      "Training epochs #1914000:   Batch Loss = 6.404799, Accuracy = 0.877999961376\n",
      "Performance on test set: Batch Loss = 6.43831968307, Accuracy = 0.860000014305\n",
      "Training epochs #1916000:   Batch Loss = 6.464050, Accuracy = 0.85900002718\n",
      "Performance on test set: Batch Loss = 6.41527557373, Accuracy = 0.870000004768\n",
      "Training epochs #1918000:   Batch Loss = 6.389671, Accuracy = 0.878000020981\n",
      "Performance on test set: Batch Loss = 6.39611339569, Accuracy = 0.898000001907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #1920000:   Batch Loss = 6.429719, Accuracy = 0.879999995232\n",
      "Performance on test set: Batch Loss = 6.40596675873, Accuracy = 0.875000059605\n",
      "Training epochs #1922000:   Batch Loss = 6.391442, Accuracy = 0.874000072479\n",
      "Performance on test set: Batch Loss = 6.41197490692, Accuracy = 0.882000029087\n",
      "Training epochs #1924000:   Batch Loss = 6.365104, Accuracy = 0.886000037193\n",
      "Performance on test set: Batch Loss = 6.4369263649, Accuracy = 0.875000119209\n",
      "Training epochs #1926000:   Batch Loss = 6.406790, Accuracy = 0.872000038624\n",
      "Performance on test set: Batch Loss = 6.39989280701, Accuracy = 0.879000008106\n",
      "Training epochs #1928000:   Batch Loss = 6.427517, Accuracy = 0.869000077248\n",
      "Performance on test set: Batch Loss = 6.4176607132, Accuracy = 0.877000033855\n",
      "Training epochs #1930000:   Batch Loss = 6.446441, Accuracy = 0.856000065804\n",
      "Performance on test set: Batch Loss = 6.3717918396, Accuracy = 0.888000071049\n",
      "Training epochs #1932000:   Batch Loss = 6.405066, Accuracy = 0.868000030518\n",
      "Performance on test set: Batch Loss = 6.45876979828, Accuracy = 0.854999959469\n",
      "Training epochs #1934000:   Batch Loss = 6.386117, Accuracy = 0.8900000453\n",
      "Performance on test set: Batch Loss = 6.41136455536, Accuracy = 0.858999967575\n",
      "Training epochs #1936000:   Batch Loss = 6.431071, Accuracy = 0.862000048161\n",
      "Performance on test set: Batch Loss = 6.39720106125, Accuracy = 0.869000017643\n",
      "Training epochs #1938000:   Batch Loss = 6.434591, Accuracy = 0.864000082016\n",
      "Performance on test set: Batch Loss = 6.37387943268, Accuracy = 0.89400011301\n",
      "Training epochs #1940000:   Batch Loss = 6.411798, Accuracy = 0.873000025749\n",
      "Performance on test set: Batch Loss = 6.38630962372, Accuracy = 0.875\n",
      "Training epochs #1942000:   Batch Loss = 6.427900, Accuracy = 0.856999993324\n",
      "Performance on test set: Batch Loss = 6.39380025864, Accuracy = 0.883000016212\n",
      "Training epochs #1944000:   Batch Loss = 6.362123, Accuracy = 0.89100009203\n",
      "Performance on test set: Batch Loss = 6.40558576584, Accuracy = 0.875\n",
      "Training epochs #1946000:   Batch Loss = 6.448289, Accuracy = 0.856000006199\n",
      "Performance on test set: Batch Loss = 6.36376667023, Accuracy = 0.877999961376\n",
      "Training epochs #1948000:   Batch Loss = 6.404514, Accuracy = 0.873000025749\n",
      "Performance on test set: Batch Loss = 6.40003252029, Accuracy = 0.878999948502\n",
      "Training epochs #1950000:   Batch Loss = 6.364158, Accuracy = 0.871999979019\n",
      "Performance on test set: Batch Loss = 6.34877300262, Accuracy = 0.886999964714\n",
      "Training epochs #1952000:   Batch Loss = 6.425858, Accuracy = 0.856999993324\n",
      "Performance on test set: Batch Loss = 6.43768978119, Accuracy = 0.854999959469\n",
      "Training epochs #1954000:   Batch Loss = 6.393561, Accuracy = 0.87600004673\n",
      "Performance on test set: Batch Loss = 6.40052032471, Accuracy = 0.8599999547\n",
      "Training epochs #1956000:   Batch Loss = 6.357493, Accuracy = 0.878000020981\n",
      "Performance on test set: Batch Loss = 6.37441301346, Accuracy = 0.870999991894\n",
      "Training epochs #1958000:   Batch Loss = 6.375597, Accuracy = 0.87600004673\n",
      "Performance on test set: Batch Loss = 6.35528707504, Accuracy = 0.897000074387\n",
      "Training epochs #1960000:   Batch Loss = 6.316569, Accuracy = 0.887000083923\n",
      "Performance on test set: Batch Loss = 6.36267328262, Accuracy = 0.875999987125\n",
      "Training epochs #1962000:   Batch Loss = 6.377283, Accuracy = 0.878000020981\n",
      "Performance on test set: Batch Loss = 6.37050199509, Accuracy = 0.884000003338\n",
      "Training epochs #1964000:   Batch Loss = 6.398755, Accuracy = 0.868000030518\n",
      "Performance on test set: Batch Loss = 6.38931846619, Accuracy = 0.875000119209\n",
      "Training epochs #1966000:   Batch Loss = 6.344573, Accuracy = 0.881000041962\n",
      "Performance on test set: Batch Loss = 6.35309076309, Accuracy = 0.879999995232\n",
      "Training epochs #1968000:   Batch Loss = 6.388411, Accuracy = 0.879999995232\n",
      "Performance on test set: Batch Loss = 6.38783121109, Accuracy = 0.877000033855\n",
      "Training epochs #1970000:   Batch Loss = 6.373949, Accuracy = 0.87600004673\n",
      "Performance on test set: Batch Loss = 6.32934141159, Accuracy = 0.884999990463\n",
      "Training epochs #1972000:   Batch Loss = 6.369902, Accuracy = 0.864000022411\n",
      "Performance on test set: Batch Loss = 6.41531515121, Accuracy = 0.854000031948\n",
      "Training epochs #1974000:   Batch Loss = 6.358335, Accuracy = 0.856999993324\n",
      "Performance on test set: Batch Loss = 6.3710641861, Accuracy = 0.8599999547\n",
      "Training epochs #1976000:   Batch Loss = 6.348908, Accuracy = 0.876999974251\n",
      "Performance on test set: Batch Loss = 6.35255098343, Accuracy = 0.866999983788\n",
      "Training epochs #1978000:   Batch Loss = 6.358004, Accuracy = 0.882000029087\n",
      "Performance on test set: Batch Loss = 6.33231878281, Accuracy = 0.894000053406\n",
      "Training epochs #1980000:   Batch Loss = 6.361531, Accuracy = 0.864000082016\n",
      "Performance on test set: Batch Loss = 6.33701562881, Accuracy = 0.875\n",
      "Training epochs #1982000:   Batch Loss = 6.288057, Accuracy = 0.889000058174\n",
      "Performance on test set: Batch Loss = 6.34769010544, Accuracy = 0.883000016212\n",
      "Training epochs #1984000:   Batch Loss = 6.356825, Accuracy = 0.871000051498\n",
      "Performance on test set: Batch Loss = 6.36454820633, Accuracy = 0.875000059605\n",
      "Training epochs #1986000:   Batch Loss = 6.351370, Accuracy = 0.866000056267\n",
      "Performance on test set: Batch Loss = 6.32188510895, Accuracy = 0.881000041962\n",
      "Training epochs #1988000:   Batch Loss = 6.319847, Accuracy = 0.882000088692\n",
      "Performance on test set: Batch Loss = 6.36182117462, Accuracy = 0.879000008106\n",
      "Training epochs #1990000:   Batch Loss = 6.402621, Accuracy = 0.853000044823\n",
      "Performance on test set: Batch Loss = 6.30988693237, Accuracy = 0.887000024319\n",
      "Training epochs #1992000:   Batch Loss = 6.332432, Accuracy = 0.886000037193\n",
      "Performance on test set: Batch Loss = 6.3913974762, Accuracy = 0.855000019073\n",
      "Training epochs #1994000:   Batch Loss = 6.331303, Accuracy = 0.87600004673\n",
      "Performance on test set: Batch Loss = 6.41778278351, Accuracy = 0.860999941826\n",
      "Training epochs #1996000:   Batch Loss = 7.239756, Accuracy = 0.222999989986\n",
      "Performance on test set: Batch Loss = 6.97558259964, Accuracy = 0.846000015736\n",
      "Training epochs #1998000:   Batch Loss = 6.618387, Accuracy = 0.805000066757\n",
      "Performance on test set: Batch Loss = 6.61574029922, Accuracy = 0.81200003624\n",
      "Training epochs #2000000:   Batch Loss = 6.933368, Accuracy = 0.789000034332\n",
      "Performance on test set: Batch Loss = 6.96944332123, Accuracy = 0.782000005245\n",
      "Training epochs #2002000:   Batch Loss = 6.858651, Accuracy = 0.790000081062\n",
      "Performance on test set: Batch Loss = 7.04141712189, Accuracy = 0.778000056744\n",
      "Training epochs #2004000:   Batch Loss = 6.842487, Accuracy = 0.808999955654\n",
      "Performance on test set: Batch Loss = 6.97890663147, Accuracy = 0.785000026226\n",
      "Training epochs #2006000:   Batch Loss = 7.009165, Accuracy = 0.764999985695\n",
      "Performance on test set: Batch Loss = 6.77770423889, Accuracy = 0.81299996376\n",
      "Training epochs #2008000:   Batch Loss = 6.642909, Accuracy = 0.819000005722\n",
      "Performance on test set: Batch Loss = 6.54457950592, Accuracy = 0.815999984741\n",
      "Training epochs #2010000:   Batch Loss = 6.543828, Accuracy = 0.807000041008\n",
      "Performance on test set: Batch Loss = 6.4537730217, Accuracy = 0.844000041485\n",
      "Training epochs #2012000:   Batch Loss = 6.418623, Accuracy = 0.817000031471\n",
      "Performance on test set: Batch Loss = 6.85973262787, Accuracy = 0.771999955177\n",
      "Training epochs #2014000:   Batch Loss = 6.938254, Accuracy = 0.769000053406\n",
      "Performance on test set: Batch Loss = 6.85743236542, Accuracy = 0.78100001812\n",
      "Training epochs #2016000:   Batch Loss = 6.821086, Accuracy = 0.817000031471\n",
      "Performance on test set: Batch Loss = 6.75542736053, Accuracy = 0.712000012398\n",
      "Training epochs #2018000:   Batch Loss = 6.786646, Accuracy = 0.680000007153\n",
      "Performance on test set: Batch Loss = 6.68153953552, Accuracy = 0.715000033379\n",
      "Training epochs #2020000:   Batch Loss = 6.751158, Accuracy = 0.680000066757\n",
      "Performance on test set: Batch Loss = 6.82885980606, Accuracy = 0.669999957085\n",
      "Training epochs #2022000:   Batch Loss = 6.802497, Accuracy = 0.767999947071\n",
      "Performance on test set: Batch Loss = 6.74256372452, Accuracy = 0.771000027657\n",
      "Training epochs #2024000:   Batch Loss = 6.647917, Accuracy = 0.763999998569\n",
      "Performance on test set: Batch Loss = 6.67753553391, Accuracy = 0.76700001955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #2026000:   Batch Loss = 6.775649, Accuracy = 0.74899995327\n",
      "Performance on test set: Batch Loss = 6.70231771469, Accuracy = 0.768999993801\n",
      "Training epochs #2028000:   Batch Loss = 6.747033, Accuracy = 0.75\n",
      "Performance on test set: Batch Loss = 6.7264342308, Accuracy = 0.768000006676\n",
      "Training epochs #2030000:   Batch Loss = 6.601416, Accuracy = 0.767000079155\n",
      "Performance on test set: Batch Loss = 6.52855491638, Accuracy = 0.787999987602\n",
      "Training epochs #2032000:   Batch Loss = 6.581649, Accuracy = 0.763999998569\n",
      "Performance on test set: Batch Loss = 6.64024591446, Accuracy = 0.746999979019\n",
      "Training epochs #2034000:   Batch Loss = 6.591699, Accuracy = 0.780999958515\n",
      "Performance on test set: Batch Loss = 6.53928375244, Accuracy = 0.773000001907\n",
      "Training epochs #2036000:   Batch Loss = 6.453502, Accuracy = 0.802000045776\n",
      "Performance on test set: Batch Loss = 6.42076873779, Accuracy = 0.804000079632\n",
      "Training epochs #2038000:   Batch Loss = 6.417994, Accuracy = 0.827999949455\n",
      "Performance on test set: Batch Loss = 6.40226221085, Accuracy = 0.863000035286\n",
      "Training epochs #2040000:   Batch Loss = 6.405557, Accuracy = 0.852000057697\n",
      "Performance on test set: Batch Loss = 6.46011066437, Accuracy = 0.852999985218\n",
      "Training epochs #2042000:   Batch Loss = 6.446570, Accuracy = 0.858000040054\n",
      "Performance on test set: Batch Loss = 6.41618442535, Accuracy = 0.863000035286\n",
      "Training epochs #2044000:   Batch Loss = 6.424120, Accuracy = 0.861000001431\n",
      "Performance on test set: Batch Loss = 6.39645242691, Accuracy = 0.861999988556\n",
      "Training epochs #2046000:   Batch Loss = 6.334724, Accuracy = 0.879999995232\n",
      "Performance on test set: Batch Loss = 6.35206222534, Accuracy = 0.887000083923\n",
      "Training epochs #2048000:   Batch Loss = 6.401629, Accuracy = 0.878000020981\n",
      "Performance on test set: Batch Loss = 6.3787150383, Accuracy = 0.87600004673\n",
      "Training epochs #2050000:   Batch Loss = 6.365237, Accuracy = 0.880999982357\n",
      "Performance on test set: Batch Loss = 6.31956958771, Accuracy = 0.896999955177\n",
      "Training epochs #2052000:   Batch Loss = 6.364890, Accuracy = 0.875000059605\n",
      "Performance on test set: Batch Loss = 6.4074754715, Accuracy = 0.865000009537\n",
      "Training epochs #2054000:   Batch Loss = 6.359784, Accuracy = 0.878000020981\n",
      "Performance on test set: Batch Loss = 6.36716461182, Accuracy = 0.882000029087\n",
      "Training epochs #2056000:   Batch Loss = 6.334037, Accuracy = 0.888000011444\n",
      "Performance on test set: Batch Loss = 6.33800411224, Accuracy = 0.877000033855\n",
      "Training epochs #2058000:   Batch Loss = 6.356935, Accuracy = 0.887000083923\n",
      "Performance on test set: Batch Loss = 6.31402206421, Accuracy = 0.895000040531\n",
      "Training epochs #2060000:   Batch Loss = 6.346992, Accuracy = 0.883999943733\n",
      "Performance on test set: Batch Loss = 6.32412624359, Accuracy = 0.868000030518\n",
      "Training epochs #2062000:   Batch Loss = 6.282758, Accuracy = 0.877000033855\n",
      "Performance on test set: Batch Loss = 6.33649635315, Accuracy = 0.879000067711\n",
      "Training epochs #2064000:   Batch Loss = 6.351614, Accuracy = 0.861000001431\n",
      "Performance on test set: Batch Loss = 6.34444618225, Accuracy = 0.874000072479\n",
      "Training epochs #2066000:   Batch Loss = 6.333510, Accuracy = 0.861000001431\n",
      "Performance on test set: Batch Loss = 6.30159044266, Accuracy = 0.878000020981\n",
      "Training epochs #2068000:   Batch Loss = 6.297770, Accuracy = 0.879999995232\n",
      "Performance on test set: Batch Loss = 6.33230257034, Accuracy = 0.877000033855\n",
      "Training epochs #2070000:   Batch Loss = 6.379951, Accuracy = 0.853000044823\n",
      "Performance on test set: Batch Loss = 6.28577232361, Accuracy = 0.884999990463\n",
      "Training epochs #2072000:   Batch Loss = 6.312231, Accuracy = 0.887000083923\n",
      "Performance on test set: Batch Loss = 6.37051391602, Accuracy = 0.855000019073\n",
      "Training epochs #2074000:   Batch Loss = 6.294116, Accuracy = 0.875\n",
      "Performance on test set: Batch Loss = 6.33761453629, Accuracy = 0.8599999547\n",
      "Training epochs #2076000:   Batch Loss = 6.355911, Accuracy = 0.856999993324\n",
      "Performance on test set: Batch Loss = 6.29703760147, Accuracy = 0.871000051498\n",
      "Training epochs #2078000:   Batch Loss = 6.288859, Accuracy = 0.878000020981\n",
      "Performance on test set: Batch Loss = 6.27853345871, Accuracy = 0.896000027657\n",
      "Training epochs #2080000:   Batch Loss = 6.316711, Accuracy = 0.882000029087\n",
      "Performance on test set: Batch Loss = 6.29336071014, Accuracy = 0.875999987125\n",
      "Training epochs #2082000:   Batch Loss = 6.283580, Accuracy = 0.875999987125\n",
      "Performance on test set: Batch Loss = 6.30252504349, Accuracy = 0.884000003338\n",
      "Training epochs #2084000:   Batch Loss = 6.246686, Accuracy = 0.887000083923\n",
      "Performance on test set: Batch Loss = 6.3164563179, Accuracy = 0.875\n",
      "Training epochs #2086000:   Batch Loss = 6.280983, Accuracy = 0.875999987125\n",
      "Performance on test set: Batch Loss = 6.27390289307, Accuracy = 0.879999995232\n",
      "Training epochs #2088000:   Batch Loss = 6.312660, Accuracy = 0.869000077248\n",
      "Performance on test set: Batch Loss = 6.31052875519, Accuracy = 0.876999974251\n",
      "Training epochs #2090000:   Batch Loss = 6.339448, Accuracy = 0.856000065804\n",
      "Performance on test set: Batch Loss = 6.26116800308, Accuracy = 0.886000037193\n",
      "Training epochs #2092000:   Batch Loss = 6.291506, Accuracy = 0.856000006199\n",
      "Performance on test set: Batch Loss = 6.3455119133, Accuracy = 0.847000002861\n",
      "Training epochs #2094000:   Batch Loss = 6.278964, Accuracy = 0.878999948502\n",
      "Performance on test set: Batch Loss = 6.30614423752, Accuracy = 0.851999998093\n",
      "Training epochs #2096000:   Batch Loss = 6.320465, Accuracy = 0.851000010967\n",
      "Performance on test set: Batch Loss = 6.27593183517, Accuracy = 0.86000007391\n",
      "Training epochs #2098000:   Batch Loss = 6.323976, Accuracy = 0.855000019073\n",
      "Performance on test set: Batch Loss = 6.259786129, Accuracy = 0.875\n",
      "Training epochs #2100000:   Batch Loss = 6.298492, Accuracy = 0.863000035286\n",
      "Performance on test set: Batch Loss = 6.27018976212, Accuracy = 0.868000030518\n",
      "Training epochs #2102000:   Batch Loss = 6.320248, Accuracy = 0.849999964237\n",
      "Performance on test set: Batch Loss = 6.28023338318, Accuracy = 0.875999987125\n",
      "Training epochs #2104000:   Batch Loss = 6.255046, Accuracy = 0.87600004673\n",
      "Performance on test set: Batch Loss = 6.29547691345, Accuracy = 0.869000136852\n",
      "Training epochs #2106000:   Batch Loss = 6.329277, Accuracy = 0.854000031948\n",
      "Performance on test set: Batch Loss = 6.2557349205, Accuracy = 0.879999935627\n",
      "Training epochs #2108000:   Batch Loss = 6.313710, Accuracy = 0.870000064373\n",
      "Performance on test set: Batch Loss = 6.29250240326, Accuracy = 0.877999961376\n",
      "Training epochs #2110000:   Batch Loss = 6.257250, Accuracy = 0.867999970913\n",
      "Performance on test set: Batch Loss = 6.24401903152, Accuracy = 0.887000083923\n",
      "Training epochs #2112000:   Batch Loss = 6.323320, Accuracy = 0.855000019073\n",
      "Performance on test set: Batch Loss = 6.37245512009, Accuracy = 0.854000031948\n",
      "Training epochs #2114000:   Batch Loss = 7.166986, Accuracy = 0.208999991417\n",
      "Performance on test set: Batch Loss = 7.05123233795, Accuracy = 0.192000001669\n",
      "Training epochs #2116000:   Batch Loss = 6.687576, Accuracy = 0.822000026703\n",
      "Performance on test set: Batch Loss = 6.59402275085, Accuracy = 0.806000053883\n",
      "Training epochs #2118000:   Batch Loss = 6.820629, Accuracy = 0.699000000954\n",
      "Performance on test set: Batch Loss = 6.86769342422, Accuracy = 0.686999976635\n",
      "Training epochs #2120000:   Batch Loss = 6.977318, Accuracy = 0.67300003767\n",
      "Performance on test set: Batch Loss = 7.29934740067, Accuracy = 0.735000014305\n",
      "Training epochs #2122000:   Batch Loss = 7.119368, Accuracy = 0.769000053406\n",
      "Performance on test set: Batch Loss = 7.17851638794, Accuracy = 0.757000029087\n",
      "Training epochs #2124000:   Batch Loss = 7.074931, Accuracy = 0.763000011444\n",
      "Performance on test set: Batch Loss = 7.11057853699, Accuracy = 0.759999990463\n",
      "Training epochs #2126000:   Batch Loss = 6.976243, Accuracy = 0.768000006676\n",
      "Performance on test set: Batch Loss = 6.89738130569, Accuracy = 0.766999959946\n",
      "Training epochs #2128000:   Batch Loss = 6.838641, Accuracy = 0.743000030518\n",
      "Performance on test set: Batch Loss = 6.82630729675, Accuracy = 0.752000033855\n",
      "Training epochs #2130000:   Batch Loss = 6.797112, Accuracy = 0.769000053406\n",
      "Performance on test set: Batch Loss = 6.72315120697, Accuracy = 0.773999989033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #2132000:   Batch Loss = 6.817274, Accuracy = 0.737999975681\n",
      "Performance on test set: Batch Loss = 6.85524368286, Accuracy = 0.742999970913\n",
      "Training epochs #2134000:   Batch Loss = 6.805813, Accuracy = 0.754999995232\n",
      "Performance on test set: Batch Loss = 6.75762414932, Accuracy = 0.748000025749\n",
      "Training epochs #2136000:   Batch Loss = 6.691527, Accuracy = 0.764999985695\n",
      "Performance on test set: Batch Loss = 6.64582252502, Accuracy = 0.782999992371\n",
      "Training epochs #2138000:   Batch Loss = 6.631972, Accuracy = 0.775000035763\n",
      "Performance on test set: Batch Loss = 6.59449577332, Accuracy = 0.768999934196\n",
      "Training epochs #2140000:   Batch Loss = 6.595601, Accuracy = 0.786000013351\n",
      "Performance on test set: Batch Loss = 6.65962219238, Accuracy = 0.769999980927\n",
      "Training epochs #2142000:   Batch Loss = 6.511483, Accuracy = 0.81099998951\n",
      "Performance on test set: Batch Loss = 6.60847759247, Accuracy = 0.777999997139\n",
      "Training epochs #2144000:   Batch Loss = 6.616944, Accuracy = 0.771000027657\n",
      "Performance on test set: Batch Loss = 6.60478496552, Accuracy = 0.782000005245\n",
      "Training epochs #2146000:   Batch Loss = 6.561375, Accuracy = 0.784000039101\n",
      "Performance on test set: Batch Loss = 6.54283952713, Accuracy = 0.789000034332\n",
      "Training epochs #2148000:   Batch Loss = 6.493957, Accuracy = 0.79699999094\n",
      "Performance on test set: Batch Loss = 6.57811975479, Accuracy = 0.774999976158\n",
      "Training epochs #2150000:   Batch Loss = 6.584372, Accuracy = 0.776000022888\n",
      "Performance on test set: Batch Loss = 6.49014663696, Accuracy = 0.800000011921\n",
      "Training epochs #2152000:   Batch Loss = 6.484871, Accuracy = 0.800999999046\n",
      "Performance on test set: Batch Loss = 6.55495691299, Accuracy = 0.774999976158\n",
      "Training epochs #2154000:   Batch Loss = 6.466393, Accuracy = 0.809000015259\n",
      "Performance on test set: Batch Loss = 6.49501657486, Accuracy = 0.816000044346\n",
      "Training epochs #2156000:   Batch Loss = 6.486703, Accuracy = 0.820999979973\n",
      "Performance on test set: Batch Loss = 6.44184970856, Accuracy = 0.837999999523\n",
      "Training epochs #2158000:   Batch Loss = 6.421841, Accuracy = 0.834000051022\n",
      "Performance on test set: Batch Loss = 6.41176748276, Accuracy = 0.837000131607\n",
      "Training epochs #2160000:   Batch Loss = 6.465415, Accuracy = 0.810000061989\n",
      "Performance on test set: Batch Loss = 6.45580053329, Accuracy = 0.806999981403\n",
      "Training epochs #2162000:   Batch Loss = 6.365509, Accuracy = 0.842000007629\n",
      "Performance on test set: Batch Loss = 6.42279815674, Accuracy = 0.797999978065\n",
      "Training epochs #2164000:   Batch Loss = 6.352227, Accuracy = 0.820000052452\n",
      "Performance on test set: Batch Loss = 6.41465473175, Accuracy = 0.797999978065\n",
      "Training epochs #2166000:   Batch Loss = 6.391389, Accuracy = 0.800000011921\n",
      "Performance on test set: Batch Loss = 6.36005353928, Accuracy = 0.833000063896\n",
      "Training epochs #2168000:   Batch Loss = 6.389596, Accuracy = 0.828000068665\n",
      "Performance on test set: Batch Loss = 6.3884806633, Accuracy = 0.835999965668\n",
      "Training epochs #2170000:   Batch Loss = 6.386927, Accuracy = 0.840999960899\n",
      "Performance on test set: Batch Loss = 6.31272697449, Accuracy = 0.867000102997\n",
      "Training epochs #2172000:   Batch Loss = 6.342240, Accuracy = 0.842000007629\n",
      "Performance on test set: Batch Loss = 6.39489603043, Accuracy = 0.829999983311\n",
      "Training epochs #2174000:   Batch Loss = 6.329605, Accuracy = 0.869000077248\n",
      "Performance on test set: Batch Loss = 6.33999681473, Accuracy = 0.852999985218\n",
      "Training epochs #2176000:   Batch Loss = 6.365992, Accuracy = 0.841000020504\n",
      "Performance on test set: Batch Loss = 6.31521987915, Accuracy = 0.857000052929\n",
      "Training epochs #2178000:   Batch Loss = 6.355814, Accuracy = 0.847999989986\n",
      "Performance on test set: Batch Loss = 6.27396440506, Accuracy = 0.879000008106\n",
      "Training epochs #2180000:   Batch Loss = 6.321226, Accuracy = 0.862000048161\n",
      "Performance on test set: Batch Loss = 6.30631256104, Accuracy = 0.856000065804\n",
      "Training epochs #2182000:   Batch Loss = 6.343976, Accuracy = 0.847000002861\n",
      "Performance on test set: Batch Loss = 6.29536008835, Accuracy = 0.863000035286\n",
      "Training epochs #2184000:   Batch Loss = 6.259984, Accuracy = 0.885000109673\n",
      "Performance on test set: Batch Loss = 6.31020545959, Accuracy = 0.881000101566\n",
      "Training epochs #2186000:   Batch Loss = 6.344985, Accuracy = 0.862999975681\n",
      "Performance on test set: Batch Loss = 6.2582783699, Accuracy = 0.88999992609\n",
      "Training epochs #2188000:   Batch Loss = 6.316186, Accuracy = 0.870000004768\n",
      "Performance on test set: Batch Loss = 6.28339767456, Accuracy = 0.860000014305\n",
      "Training epochs #2190000:   Batch Loss = 6.246490, Accuracy = 0.886000096798\n",
      "Performance on test set: Batch Loss = 6.22159719467, Accuracy = 0.888000011444\n",
      "Training epochs #2192000:   Batch Loss = 6.292142, Accuracy = 0.862000048161\n",
      "Performance on test set: Batch Loss = 6.30545091629, Accuracy = 0.868000030518\n",
      "Training epochs #2194000:   Batch Loss = 6.267821, Accuracy = 0.870000004768\n",
      "Performance on test set: Batch Loss = 6.26426458359, Accuracy = 0.864000082016\n",
      "Training epochs #2196000:   Batch Loss = 6.237577, Accuracy = 0.883000075817\n",
      "Performance on test set: Batch Loss = 6.24409770966, Accuracy = 0.871000051498\n",
      "Training epochs #2198000:   Batch Loss = 6.245222, Accuracy = 0.882000029087\n",
      "Performance on test set: Batch Loss = 6.21902561188, Accuracy = 0.890000104904\n",
      "Training epochs #2200000:   Batch Loss = 6.198451, Accuracy = 0.895000040531\n",
      "Performance on test set: Batch Loss = 6.24408006668, Accuracy = 0.87700009346\n",
      "Training epochs #2202000:   Batch Loss = 6.249506, Accuracy = 0.876000106335\n",
      "Performance on test set: Batch Loss = 6.24064779282, Accuracy = 0.887000024319\n",
      "Training epochs #2204000:   Batch Loss = 6.280200, Accuracy = 0.874000072479\n",
      "Performance on test set: Batch Loss = 6.25532341003, Accuracy = 0.881000041962\n",
      "Training epochs #2206000:   Batch Loss = 6.199776, Accuracy = 0.884999990463\n",
      "Performance on test set: Batch Loss = 6.21160364151, Accuracy = 0.895000040531\n",
      "Training epochs #2208000:   Batch Loss = 6.248188, Accuracy = 0.87600004673\n",
      "Performance on test set: Batch Loss = 6.2544965744, Accuracy = 0.870000004768\n",
      "Training epochs #2210000:   Batch Loss = 6.229785, Accuracy = 0.886000037193\n",
      "Performance on test set: Batch Loss = 6.19119882584, Accuracy = 0.893999934196\n",
      "Training epochs #2212000:   Batch Loss = 6.227555, Accuracy = 0.869000017643\n",
      "Performance on test set: Batch Loss = 6.2743473053, Accuracy = 0.868000090122\n",
      "Training epochs #2214000:   Batch Loss = 6.222184, Accuracy = 0.869000017643\n",
      "Performance on test set: Batch Loss = 6.23191356659, Accuracy = 0.867999970913\n",
      "Training epochs #2216000:   Batch Loss = 6.212605, Accuracy = 0.888999998569\n",
      "Performance on test set: Batch Loss = 6.2135014534, Accuracy = 0.881000041962\n",
      "Training epochs #2218000:   Batch Loss = 6.227746, Accuracy = 0.889000058174\n",
      "Performance on test set: Batch Loss = 6.2138710022, Accuracy = 0.899000048637\n",
      "Training epochs #2220000:   Batch Loss = 6.228375, Accuracy = 0.883000016212\n",
      "Performance on test set: Batch Loss = 6.25181913376, Accuracy = 0.89200001955\n",
      "Training epochs #2222000:   Batch Loss = 6.348921, Accuracy = 0.887000083923\n",
      "Performance on test set: Batch Loss = 6.29577827454, Accuracy = 0.87600004673\n",
      "Training epochs #2224000:   Batch Loss = 6.255584, Accuracy = 0.87600004673\n",
      "Performance on test set: Batch Loss = 6.30777645111, Accuracy = 0.849000096321\n",
      "Training epochs #2226000:   Batch Loss = 6.353341, Accuracy = 0.841999948025\n",
      "Performance on test set: Batch Loss = 6.30831813812, Accuracy = 0.856000006199\n",
      "Training epochs #2228000:   Batch Loss = 6.320515, Accuracy = 0.818000018597\n",
      "Performance on test set: Batch Loss = 6.40881156921, Accuracy = 0.81400001049\n",
      "Training epochs #2230000:   Batch Loss = 6.376331, Accuracy = 0.802000045776\n",
      "Performance on test set: Batch Loss = 6.29646778107, Accuracy = 0.848000049591\n",
      "Training epochs #2232000:   Batch Loss = 6.274796, Accuracy = 0.853999972343\n",
      "Performance on test set: Batch Loss = 6.29673480988, Accuracy = 0.852999985218\n",
      "Training epochs #2234000:   Batch Loss = 6.210542, Accuracy = 0.886000037193\n",
      "Performance on test set: Batch Loss = 6.33491897583, Accuracy = 0.780000030994\n",
      "Training epochs #2236000:   Batch Loss = 6.246525, Accuracy = 0.871999979019\n",
      "Performance on test set: Batch Loss = 6.21942186356, Accuracy = 0.858000040054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #2238000:   Batch Loss = 6.226701, Accuracy = 0.847999989986\n",
      "Performance on test set: Batch Loss = 6.20307636261, Accuracy = 0.87700009346\n",
      "Training epochs #2240000:   Batch Loss = 6.398715, Accuracy = 0.871000051498\n",
      "Performance on test set: Batch Loss = 8.41432094574, Accuracy = 0.179000020027\n",
      "Training epochs #2242000:   Batch Loss = 9.125419, Accuracy = 0.148999989033\n",
      "Performance on test set: Batch Loss = 8.47037410736, Accuracy = 0.138999998569\n",
      "Training epochs #2244000:   Batch Loss = 7.292054, Accuracy = 0.17499999702\n",
      "Performance on test set: Batch Loss = 6.85848474503, Accuracy = 0.763000011444\n",
      "Training epochs #2246000:   Batch Loss = 7.097171, Accuracy = 0.726000010967\n",
      "Performance on test set: Batch Loss = 7.11476421356, Accuracy = 0.766000032425\n",
      "Training epochs #2248000:   Batch Loss = 7.426060, Accuracy = 0.742000043392\n",
      "Performance on test set: Batch Loss = 7.52492952347, Accuracy = 0.639000058174\n",
      "Training epochs #2250000:   Batch Loss = 7.538886, Accuracy = 0.649999976158\n",
      "Performance on test set: Batch Loss = 7.45766115189, Accuracy = 0.666000008583\n",
      "Training epochs #2252000:   Batch Loss = 7.548749, Accuracy = 0.644000053406\n",
      "Performance on test set: Batch Loss = 7.50129699707, Accuracy = 0.627000033855\n",
      "Training epochs #2254000:   Batch Loss = 7.371410, Accuracy = 0.741999983788\n",
      "Performance on test set: Batch Loss = 7.26088905334, Accuracy = 0.743000030518\n",
      "Training epochs #2256000:   Batch Loss = 7.146038, Accuracy = 0.736999988556\n",
      "Performance on test set: Batch Loss = 6.89427042007, Accuracy = 0.783000051975\n",
      "Training epochs #2258000:   Batch Loss = 6.985959, Accuracy = 0.733999967575\n",
      "Performance on test set: Batch Loss = 6.88849639893, Accuracy = 0.768999993801\n",
      "Training epochs #2260000:   Batch Loss = 6.922847, Accuracy = 0.763000011444\n",
      "Performance on test set: Batch Loss = 7.01987266541, Accuracy = 0.742999970913\n",
      "Training epochs #2262000:   Batch Loss = 7.000549, Accuracy = 0.75\n",
      "Performance on test set: Batch Loss = 6.93288803101, Accuracy = 0.759000003338\n",
      "Training epochs #2264000:   Batch Loss = 6.890533, Accuracy = 0.753999948502\n",
      "Performance on test set: Batch Loss = 6.8738732338, Accuracy = 0.763000011444\n",
      "Training epochs #2266000:   Batch Loss = 6.908589, Accuracy = 0.740000009537\n",
      "Performance on test set: Batch Loss = 6.75282239914, Accuracy = 0.766000032425\n",
      "Training epochs #2268000:   Batch Loss = 6.783448, Accuracy = 0.736999928951\n",
      "Performance on test set: Batch Loss = 6.76990175247, Accuracy = 0.749000012875\n",
      "Training epochs #2270000:   Batch Loss = 6.746831, Accuracy = 0.755000054836\n",
      "Performance on test set: Batch Loss = 6.66328573227, Accuracy = 0.774999916553\n",
      "Training epochs #2272000:   Batch Loss = 6.719395, Accuracy = 0.759999990463\n",
      "Performance on test set: Batch Loss = 6.76040029526, Accuracy = 0.768000006676\n",
      "Training epochs #2274000:   Batch Loss = 6.754220, Accuracy = 0.768000006676\n",
      "Performance on test set: Batch Loss = 6.76809549332, Accuracy = 0.760999977589\n",
      "Training epochs #2276000:   Batch Loss = 6.699303, Accuracy = 0.794000029564\n",
      "Performance on test set: Batch Loss = 6.64784383774, Accuracy = 0.791000008583\n",
      "Training epochs #2278000:   Batch Loss = 6.738569, Accuracy = 0.756999969482\n",
      "Performance on test set: Batch Loss = 6.70866775513, Accuracy = 0.769000053406\n",
      "Training epochs #2280000:   Batch Loss = 6.676979, Accuracy = 0.773999989033\n",
      "Performance on test set: Batch Loss = 6.81004714966, Accuracy = 0.738000035286\n",
      "Training epochs #2282000:   Batch Loss = 6.743700, Accuracy = 0.773000061512\n",
      "Performance on test set: Batch Loss = 6.76320838928, Accuracy = 0.756999969482\n",
      "Training epochs #2284000:   Batch Loss = 6.761626, Accuracy = 0.762000024319\n",
      "Performance on test set: Batch Loss = 6.72791147232, Accuracy = 0.761999964714\n",
      "Training epochs #2286000:   Batch Loss = 6.687890, Accuracy = 0.76700001955\n",
      "Performance on test set: Batch Loss = 6.72260141373, Accuracy = 0.763999998569\n",
      "Training epochs #2288000:   Batch Loss = 6.791119, Accuracy = 0.741000056267\n",
      "Performance on test set: Batch Loss = 6.76258611679, Accuracy = 0.749000012875\n",
      "Training epochs #2290000:   Batch Loss = 6.746317, Accuracy = 0.759000062943\n",
      "Performance on test set: Batch Loss = 6.65164995193, Accuracy = 0.773999929428\n",
      "Training epochs #2292000:   Batch Loss = 6.712872, Accuracy = 0.737999975681\n",
      "Performance on test set: Batch Loss = 6.74568128586, Accuracy = 0.744999945164\n",
      "Training epochs #2294000:   Batch Loss = 6.678190, Accuracy = 0.754999995232\n",
      "Performance on test set: Batch Loss = 6.69894218445, Accuracy = 0.743999958038\n",
      "Training epochs #2296000:   Batch Loss = 6.646582, Accuracy = 0.763999998569\n",
      "Performance on test set: Batch Loss = 6.63934135437, Accuracy = 0.779000043869\n",
      "Training epochs #2298000:   Batch Loss = 6.627183, Accuracy = 0.773000001907\n",
      "Performance on test set: Batch Loss = 6.59519767761, Accuracy = 0.768000006676\n",
      "Training epochs #2300000:   Batch Loss = 6.662271, Accuracy = 0.758000016212\n",
      "Performance on test set: Batch Loss = 6.69284152985, Accuracy = 0.740000009537\n",
      "Training epochs #2302000:   Batch Loss = 6.559265, Accuracy = 0.799000024796\n",
      "Performance on test set: Batch Loss = 6.63668870926, Accuracy = 0.763999938965\n",
      "Training epochs #2304000:   Batch Loss = 6.669049, Accuracy = 0.746999979019\n",
      "Performance on test set: Batch Loss = 6.63014125824, Accuracy = 0.768999993801\n",
      "Training epochs #2306000:   Batch Loss = 6.633313, Accuracy = 0.76700001955\n",
      "Performance on test set: Batch Loss = 6.5991973877, Accuracy = 0.771000027657\n",
      "Training epochs #2308000:   Batch Loss = 6.568299, Accuracy = 0.779000043869\n",
      "Performance on test set: Batch Loss = 6.64561748505, Accuracy = 0.756000041962\n",
      "Training epochs #2310000:   Batch Loss = 6.684407, Accuracy = 0.758000016212\n",
      "Performance on test set: Batch Loss = 6.55501890182, Accuracy = 0.800000071526\n",
      "Training epochs #2312000:   Batch Loss = 6.575267, Accuracy = 0.787000000477\n",
      "Performance on test set: Batch Loss = 6.65019893646, Accuracy = 0.759000062943\n",
      "Training epochs #2314000:   Batch Loss = 6.533542, Accuracy = 0.781999945641\n",
      "Performance on test set: Batch Loss = 6.58025121689, Accuracy = 0.754999995232\n",
      "Training epochs #2316000:   Batch Loss = 6.618435, Accuracy = 0.761000037193\n",
      "Performance on test set: Batch Loss = 6.54539060593, Accuracy = 0.781000077724\n",
      "Training epochs #2318000:   Batch Loss = 6.542725, Accuracy = 0.766000032425\n",
      "Performance on test set: Batch Loss = 6.48897171021, Accuracy = 0.783000051975\n",
      "Training epochs #2320000:   Batch Loss = 6.563346, Accuracy = 0.758000016212\n",
      "Performance on test set: Batch Loss = 6.56302452087, Accuracy = 0.754000008106\n",
      "Training epochs #2322000:   Batch Loss = 6.506670, Accuracy = 0.794999957085\n",
      "Performance on test set: Batch Loss = 6.51209259033, Accuracy = 0.793000042439\n",
      "Training epochs #2324000:   Batch Loss = 6.461950, Accuracy = 0.808000087738\n",
      "Performance on test set: Batch Loss = 6.52145290375, Accuracy = 0.808999955654\n",
      "Training epochs #2326000:   Batch Loss = 6.531774, Accuracy = 0.77999997139\n",
      "Performance on test set: Batch Loss = 6.48831605911, Accuracy = 0.795000016689\n",
      "Training epochs #2328000:   Batch Loss = 6.536713, Accuracy = 0.782000005245\n",
      "Performance on test set: Batch Loss = 6.53369808197, Accuracy = 0.787000060081\n",
      "Training epochs #2330000:   Batch Loss = 6.533373, Accuracy = 0.790000021458\n",
      "Performance on test set: Batch Loss = 6.44680738449, Accuracy = 0.809000015259\n",
      "Training epochs #2332000:   Batch Loss = 6.466218, Accuracy = 0.793999969959\n",
      "Performance on test set: Batch Loss = 6.53267145157, Accuracy = 0.76800006628\n",
      "Training epochs #2334000:   Batch Loss = 6.466079, Accuracy = 0.794000089169\n",
      "Performance on test set: Batch Loss = 6.45728111267, Accuracy = 0.789000034332\n",
      "Training epochs #2336000:   Batch Loss = 6.505189, Accuracy = 0.782000005245\n",
      "Performance on test set: Batch Loss = 6.41987943649, Accuracy = 0.805999994278\n",
      "Training epochs #2338000:   Batch Loss = 6.460128, Accuracy = 0.784000098705\n",
      "Performance on test set: Batch Loss = 6.36508798599, Accuracy = 0.828999996185\n",
      "Training epochs #2340000:   Batch Loss = 6.430172, Accuracy = 0.815999984741\n",
      "Performance on test set: Batch Loss = 6.44270515442, Accuracy = 0.800000071526\n",
      "Training epochs #2342000:   Batch Loss = 6.455360, Accuracy = 0.800000071526\n",
      "Performance on test set: Batch Loss = 6.41090774536, Accuracy = 0.816000044346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #2344000:   Batch Loss = 6.388366, Accuracy = 0.81200003624\n",
      "Performance on test set: Batch Loss = 6.42197799683, Accuracy = 0.81400001049\n",
      "Training epochs #2346000:   Batch Loss = 6.464174, Accuracy = 0.789000034332\n",
      "Performance on test set: Batch Loss = 6.37412071228, Accuracy = 0.834000051022\n",
      "Training epochs #2348000:   Batch Loss = 6.419589, Accuracy = 0.81400001049\n",
      "Performance on test set: Batch Loss = 6.41824197769, Accuracy = 0.804000020027\n",
      "Training epochs #2350000:   Batch Loss = 6.363607, Accuracy = 0.832999944687\n",
      "Performance on test set: Batch Loss = 6.35313224792, Accuracy = 0.832000017166\n",
      "Training epochs #2352000:   Batch Loss = 6.407120, Accuracy = 0.811999976635\n",
      "Performance on test set: Batch Loss = 6.42445278168, Accuracy = 0.798999965191\n",
      "Training epochs #2354000:   Batch Loss = 6.389414, Accuracy = 0.813999950886\n",
      "Performance on test set: Batch Loss = 6.36132287979, Accuracy = 0.81200003624\n",
      "Training epochs #2356000:   Batch Loss = 6.347513, Accuracy = 0.822999954224\n",
      "Performance on test set: Batch Loss = 6.3309469223, Accuracy = 0.828999996185\n",
      "Training epochs #2358000:   Batch Loss = 6.343990, Accuracy = 0.819000005722\n",
      "Performance on test set: Batch Loss = 6.28790807724, Accuracy = 0.842000067234\n",
      "Training epochs #2360000:   Batch Loss = 6.277620, Accuracy = 0.844999969006\n",
      "Performance on test set: Batch Loss = 6.35526514053, Accuracy = 0.811999976635\n",
      "Training epochs #2362000:   Batch Loss = 6.352164, Accuracy = 0.819000005722\n",
      "Performance on test set: Batch Loss = 6.32672834396, Accuracy = 0.828999996185\n",
      "Training epochs #2364000:   Batch Loss = 6.365291, Accuracy = 0.818000078201\n",
      "Performance on test set: Batch Loss = 6.34649085999, Accuracy = 0.828999996185\n",
      "Training epochs #2366000:   Batch Loss = 6.299199, Accuracy = 0.830000042915\n",
      "Performance on test set: Batch Loss = 6.29576683044, Accuracy = 0.835000038147\n",
      "Training epochs #2368000:   Batch Loss = 6.345989, Accuracy = 0.81200003624\n",
      "Performance on test set: Batch Loss = 6.33224725723, Accuracy = 0.802999973297\n",
      "Training epochs #2370000:   Batch Loss = 6.319598, Accuracy = 0.825000047684\n",
      "Performance on test set: Batch Loss = 6.27212047577, Accuracy = 0.837000012398\n",
      "Training epochs #2372000:   Batch Loss = 6.302419, Accuracy = 0.809000015259\n",
      "Performance on test set: Batch Loss = 6.3365688324, Accuracy = 0.805999994278\n",
      "Training epochs #2374000:   Batch Loss = 6.288730, Accuracy = 0.817000031471\n",
      "Performance on test set: Batch Loss = 6.27596473694, Accuracy = 0.818000018597\n",
      "Training epochs #2376000:   Batch Loss = 6.280288, Accuracy = 0.824999988079\n",
      "Performance on test set: Batch Loss = 6.26324939728, Accuracy = 0.831000030041\n",
      "Training epochs #2378000:   Batch Loss = 6.265407, Accuracy = 0.828999996185\n",
      "Performance on test set: Batch Loss = 6.21998310089, Accuracy = 0.829999983311\n",
      "Training epochs #2380000:   Batch Loss = 6.255767, Accuracy = 0.81099998951\n",
      "Performance on test set: Batch Loss = 6.27015638351, Accuracy = 0.828999996185\n",
      "Training epochs #2382000:   Batch Loss = 6.188612, Accuracy = 0.85799998045\n",
      "Performance on test set: Batch Loss = 6.24918699265, Accuracy = 0.838000059128\n",
      "Training epochs #2384000:   Batch Loss = 6.242711, Accuracy = 0.805999994278\n",
      "Performance on test set: Batch Loss = 6.28140449524, Accuracy = 0.821000039577\n",
      "Training epochs #2386000:   Batch Loss = 6.239037, Accuracy = 0.814999997616\n",
      "Performance on test set: Batch Loss = 6.2342414856, Accuracy = 0.815999984741\n",
      "Training epochs #2388000:   Batch Loss = 6.202384, Accuracy = 0.817999958992\n",
      "Performance on test set: Batch Loss = 6.25343036652, Accuracy = 0.819000005722\n",
      "Training epochs #2390000:   Batch Loss = 6.293101, Accuracy = 0.847000002861\n",
      "Performance on test set: Batch Loss = 6.20151996613, Accuracy = 0.868999958038\n",
      "Training epochs #2392000:   Batch Loss = 6.201750, Accuracy = 0.874000072479\n",
      "Performance on test set: Batch Loss = 6.27264451981, Accuracy = 0.841000020504\n",
      "Training epochs #2394000:   Batch Loss = 6.196738, Accuracy = 0.869000017643\n",
      "Performance on test set: Batch Loss = 6.21797132492, Accuracy = 0.866000056267\n",
      "Training epochs #2396000:   Batch Loss = 6.257082, Accuracy = 0.854000031948\n",
      "Performance on test set: Batch Loss = 6.21130800247, Accuracy = 0.863000094891\n",
      "Training epochs #2398000:   Batch Loss = 6.191985, Accuracy = 0.86000007391\n",
      "Performance on test set: Batch Loss = 6.17004156113, Accuracy = 0.872000038624\n",
      "Training epochs #2400000:   Batch Loss = 6.223150, Accuracy = 0.853999972343\n",
      "Performance on test set: Batch Loss = 6.21481084824, Accuracy = 0.864000022411\n",
      "Training epochs #2402000:   Batch Loss = 6.173264, Accuracy = 0.884000062943\n",
      "Performance on test set: Batch Loss = 6.19854211807, Accuracy = 0.870000004768\n",
      "Training epochs #2404000:   Batch Loss = 6.161262, Accuracy = 0.870000004768\n",
      "Performance on test set: Batch Loss = 6.22727489471, Accuracy = 0.861999988556\n",
      "Training epochs #2406000:   Batch Loss = 6.187172, Accuracy = 0.855000019073\n",
      "Performance on test set: Batch Loss = 6.17611408234, Accuracy = 0.880999982357\n",
      "Training epochs #2408000:   Batch Loss = 6.223919, Accuracy = 0.84500002861\n",
      "Performance on test set: Batch Loss = 6.2047419548, Accuracy = 0.842000007629\n",
      "Training epochs #2410000:   Batch Loss = 6.236382, Accuracy = 0.835000038147\n",
      "Performance on test set: Batch Loss = 6.15352249146, Accuracy = 0.861000061035\n",
      "Training epochs #2412000:   Batch Loss = 6.174444, Accuracy = 0.849000036716\n",
      "Performance on test set: Batch Loss = 6.22486257553, Accuracy = 0.830999970436\n",
      "Training epochs #2414000:   Batch Loss = 6.157942, Accuracy = 0.857000052929\n",
      "Performance on test set: Batch Loss = 6.17572021484, Accuracy = 0.843000054359\n",
      "Training epochs #2416000:   Batch Loss = 6.202545, Accuracy = 0.84500002861\n",
      "Performance on test set: Batch Loss = 6.16199874878, Accuracy = 0.85900002718\n",
      "Training epochs #2418000:   Batch Loss = 6.196160, Accuracy = 0.851000010967\n",
      "Performance on test set: Batch Loss = 6.1274433136, Accuracy = 0.863000035286\n",
      "Training epochs #2420000:   Batch Loss = 6.177664, Accuracy = 0.856000065804\n",
      "Performance on test set: Batch Loss = 6.16025114059, Accuracy = 0.856000065804\n",
      "Training epochs #2422000:   Batch Loss = 6.194641, Accuracy = 0.840000033379\n",
      "Performance on test set: Batch Loss = 6.15828800201, Accuracy = 0.85900002718\n",
      "Training epochs #2424000:   Batch Loss = 6.337729, Accuracy = 0.87700009346\n",
      "Performance on test set: Batch Loss = 8.75576210022, Accuracy = 0.155000001192\n",
      "Training epochs #2426000:   Batch Loss = 9.672665, Accuracy = 0.13299998641\n",
      "Performance on test set: Batch Loss = 9.114610672, Accuracy = 0.140000000596\n",
      "Training epochs #2428000:   Batch Loss = 8.042631, Accuracy = 0.162999987602\n",
      "Performance on test set: Batch Loss = 7.38371086121, Accuracy = 0.747999966145\n",
      "Training epochs #2430000:   Batch Loss = 7.434203, Accuracy = 0.638000011444\n",
      "Performance on test set: Batch Loss = 7.41990804672, Accuracy = 0.661000013351\n",
      "Training epochs #2432000:   Batch Loss = 7.639009, Accuracy = 0.641999959946\n",
      "Performance on test set: Batch Loss = 7.9277381897, Accuracy = 0.621000051498\n",
      "Training epochs #2434000:   Batch Loss = 7.806776, Accuracy = 0.643000006676\n",
      "Performance on test set: Batch Loss = 7.82117700577, Accuracy = 0.635999977589\n",
      "Training epochs #2436000:   Batch Loss = 7.582151, Accuracy = 0.652999997139\n",
      "Performance on test set: Batch Loss = 7.4208240509, Accuracy = 0.65600001812\n",
      "Training epochs #2438000:   Batch Loss = 7.376340, Accuracy = 0.661000072956\n",
      "Performance on test set: Batch Loss = 7.20854187012, Accuracy = 0.657999992371\n",
      "Training epochs #2440000:   Batch Loss = 7.069600, Accuracy = 0.670000076294\n",
      "Performance on test set: Batch Loss = 7.21528768539, Accuracy = 0.634000003338\n",
      "Training epochs #2442000:   Batch Loss = 7.106372, Accuracy = 0.657999992371\n",
      "Performance on test set: Batch Loss = 7.09158802032, Accuracy = 0.758000016212\n",
      "Training epochs #2444000:   Batch Loss = 7.185240, Accuracy = 0.762000024319\n",
      "Performance on test set: Batch Loss = 7.14865255356, Accuracy = 0.741999983788\n",
      "Training epochs #2446000:   Batch Loss = 7.147181, Accuracy = 0.747000038624\n",
      "Performance on test set: Batch Loss = 7.1506061554, Accuracy = 0.740999996662\n",
      "Training epochs #2448000:   Batch Loss = 7.159608, Accuracy = 0.725000023842\n",
      "Performance on test set: Batch Loss = 7.1436252594, Accuracy = 0.726000010967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #2450000:   Batch Loss = 7.070662, Accuracy = 0.75100004673\n",
      "Performance on test set: Batch Loss = 7.05488777161, Accuracy = 0.636000037193\n",
      "Training epochs #2452000:   Batch Loss = 7.052663, Accuracy = 0.634999990463\n",
      "Performance on test set: Batch Loss = 7.07196331024, Accuracy = 0.613000035286\n",
      "Training epochs #2454000:   Batch Loss = 7.065625, Accuracy = 0.621000051498\n",
      "Performance on test set: Batch Loss = 7.09576511383, Accuracy = 0.620999991894\n",
      "Training epochs #2456000:   Batch Loss = 6.953348, Accuracy = 0.666000008583\n",
      "Performance on test set: Batch Loss = 6.92027187347, Accuracy = 0.666999995708\n",
      "Training epochs #2458000:   Batch Loss = 6.880741, Accuracy = 0.666999995708\n",
      "Performance on test set: Batch Loss = 6.91399049759, Accuracy = 0.657999992371\n",
      "Training epochs #2460000:   Batch Loss = 6.924501, Accuracy = 0.759999990463\n",
      "Performance on test set: Batch Loss = 6.95793008804, Accuracy = 0.738000035286\n",
      "Training epochs #2462000:   Batch Loss = 6.759201, Accuracy = 0.787999987602\n",
      "Performance on test set: Batch Loss = 6.82201814651, Accuracy = 0.755999982357\n",
      "Training epochs #2464000:   Batch Loss = 6.820795, Accuracy = 0.745000004768\n",
      "Performance on test set: Batch Loss = 6.77947330475, Accuracy = 0.761000037193\n",
      "Training epochs #2466000:   Batch Loss = 6.815891, Accuracy = 0.754999995232\n",
      "Performance on test set: Batch Loss = 6.75696516037, Accuracy = 0.763999998569\n",
      "Training epochs #2468000:   Batch Loss = 6.743489, Accuracy = 0.768000006676\n",
      "Performance on test set: Batch Loss = 6.7847442627, Accuracy = 0.747000038624\n",
      "Training epochs #2470000:   Batch Loss = 6.810679, Accuracy = 0.738999962807\n",
      "Performance on test set: Batch Loss = 6.67805242538, Accuracy = 0.773000061512\n",
      "Training epochs #2472000:   Batch Loss = 6.687196, Accuracy = 0.759999990463\n",
      "Performance on test set: Batch Loss = 6.7466878891, Accuracy = 0.741000056267\n",
      "Training epochs #2474000:   Batch Loss = 6.661057, Accuracy = 0.768000006676\n",
      "Performance on test set: Batch Loss = 6.74434947968, Accuracy = 0.743000030518\n",
      "Training epochs #2476000:   Batch Loss = 6.705021, Accuracy = 0.75100004673\n",
      "Performance on test set: Batch Loss = 6.58476305008, Accuracy = 0.777999997139\n",
      "Training epochs #2478000:   Batch Loss = 6.646947, Accuracy = 0.762999951839\n",
      "Performance on test set: Batch Loss = 6.58939599991, Accuracy = 0.767999947071\n",
      "Training epochs #2480000:   Batch Loss = 6.633924, Accuracy = 0.750999987125\n",
      "Performance on test set: Batch Loss = 6.68905115128, Accuracy = 0.737000048161\n",
      "Training epochs #2482000:   Batch Loss = 6.635446, Accuracy = 0.764999985695\n",
      "Performance on test set: Batch Loss = 6.60937738419, Accuracy = 0.756999969482\n",
      "Training epochs #2484000:   Batch Loss = 6.570480, Accuracy = 0.777000010014\n",
      "Performance on test set: Batch Loss = 6.60429143906, Accuracy = 0.76700001955\n",
      "Training epochs #2486000:   Batch Loss = 6.649573, Accuracy = 0.735000014305\n",
      "Performance on test set: Batch Loss = 6.57739877701, Accuracy = 0.763999998569\n",
      "Training epochs #2488000:   Batch Loss = 6.627367, Accuracy = 0.740999996662\n",
      "Performance on test set: Batch Loss = 6.61177635193, Accuracy = 0.746999979019\n",
      "Training epochs #2490000:   Batch Loss = 6.596569, Accuracy = 0.756999969482\n",
      "Performance on test set: Batch Loss = 6.54672288895, Accuracy = 0.773999989033\n",
      "Training epochs #2492000:   Batch Loss = 6.622934, Accuracy = 0.755999922752\n",
      "Performance on test set: Batch Loss = 6.65003299713, Accuracy = 0.744000017643\n",
      "Training epochs #2494000:   Batch Loss = 6.614780, Accuracy = 0.743000030518\n",
      "Performance on test set: Batch Loss = 6.62522745132, Accuracy = 0.751999974251\n",
      "Training epochs #2496000:   Batch Loss = 6.606126, Accuracy = 0.741000056267\n",
      "Performance on test set: Batch Loss = 6.50780200958, Accuracy = 0.789000093937\n",
      "Training epochs #2498000:   Batch Loss = 6.610738, Accuracy = 0.744000017643\n",
      "Performance on test set: Batch Loss = 6.50258159637, Accuracy = 0.775999963284\n",
      "Training epochs #2500000:   Batch Loss = 6.540738, Accuracy = 0.769999980927\n",
      "Performance on test set: Batch Loss = 6.60002326965, Accuracy = 0.741000056267\n",
      "Training epochs #2502000:   Batch Loss = 6.576641, Accuracy = 0.754000008106\n",
      "Performance on test set: Batch Loss = 6.53377962112, Accuracy = 0.771000027657\n",
      "Training epochs #2504000:   Batch Loss = 6.511539, Accuracy = 0.774000048637\n",
      "Performance on test set: Batch Loss = 6.53435325623, Accuracy = 0.776000082493\n",
      "Training epochs #2506000:   Batch Loss = 6.601222, Accuracy = 0.755999982357\n",
      "Performance on test set: Batch Loss = 6.51911354065, Accuracy = 0.786000013351\n",
      "Training epochs #2508000:   Batch Loss = 6.541931, Accuracy = 0.759000062943\n",
      "Performance on test set: Batch Loss = 6.55258989334, Accuracy = 0.769999980927\n",
      "Training epochs #2510000:   Batch Loss = 6.554735, Accuracy = 0.789000034332\n",
      "Performance on test set: Batch Loss = 6.48944711685, Accuracy = 0.799000024796\n",
      "Training epochs #2512000:   Batch Loss = 6.520219, Accuracy = 0.789999961853\n",
      "Performance on test set: Batch Loss = 6.59509754181, Accuracy = 0.760000050068\n",
      "Training epochs #2514000:   Batch Loss = 6.527691, Accuracy = 0.771000027657\n",
      "Performance on test set: Batch Loss = 6.55980157852, Accuracy = 0.768000006676\n",
      "Training epochs #2516000:   Batch Loss = 6.504752, Accuracy = 0.794000029564\n",
      "Performance on test set: Batch Loss = 6.45660495758, Accuracy = 0.809000015259\n",
      "Training epochs #2518000:   Batch Loss = 6.515303, Accuracy = 0.795000016689\n",
      "Performance on test set: Batch Loss = 6.45239877701, Accuracy = 0.796000003815\n",
      "Training epochs #2520000:   Batch Loss = 6.440619, Accuracy = 0.796000003815\n",
      "Performance on test set: Batch Loss = 6.54566192627, Accuracy = 0.771999955177\n",
      "Training epochs #2522000:   Batch Loss = 6.505280, Accuracy = 0.792999982834\n",
      "Performance on test set: Batch Loss = 6.48869609833, Accuracy = 0.794000089169\n",
      "Training epochs #2524000:   Batch Loss = 6.517755, Accuracy = 0.787000000477\n",
      "Performance on test set: Batch Loss = 6.49288749695, Accuracy = 0.797999978065\n",
      "Training epochs #2526000:   Batch Loss = 6.470245, Accuracy = 0.794000029564\n",
      "Performance on test set: Batch Loss = 6.47309970856, Accuracy = 0.788000047207\n",
      "Training epochs #2528000:   Batch Loss = 6.516454, Accuracy = 0.774000048637\n",
      "Performance on test set: Batch Loss = 6.50869846344, Accuracy = 0.780999958515\n",
      "Training epochs #2530000:   Batch Loss = 6.493066, Accuracy = 0.788999974728\n",
      "Performance on test set: Batch Loss = 6.44891738892, Accuracy = 0.799000144005\n",
      "Training epochs #2532000:   Batch Loss = 6.489490, Accuracy = 0.775000035763\n",
      "Performance on test set: Batch Loss = 6.55155944824, Accuracy = 0.769000053406\n",
      "Training epochs #2534000:   Batch Loss = 6.500166, Accuracy = 0.787000060081\n",
      "Performance on test set: Batch Loss = 6.51323843002, Accuracy = 0.788000047207\n",
      "Training epochs #2536000:   Batch Loss = 6.431338, Accuracy = 0.811999976635\n",
      "Performance on test set: Batch Loss = 6.4149184227, Accuracy = 0.815999925137\n",
      "Training epochs #2538000:   Batch Loss = 6.408168, Accuracy = 0.819000005722\n",
      "Performance on test set: Batch Loss = 6.40343093872, Accuracy = 0.824999928474\n",
      "Training epochs #2540000:   Batch Loss = 6.466486, Accuracy = 0.807000041008\n",
      "Performance on test set: Batch Loss = 6.49759435654, Accuracy = 0.791000008583\n",
      "Training epochs #2542000:   Batch Loss = 6.362537, Accuracy = 0.836000025272\n",
      "Performance on test set: Batch Loss = 6.44339418411, Accuracy = 0.817999958992\n",
      "Training epochs #2544000:   Batch Loss = 6.437101, Accuracy = 0.802000045776\n",
      "Performance on test set: Batch Loss = 6.44953918457, Accuracy = 0.81400001049\n",
      "Training epochs #2546000:   Batch Loss = 6.447198, Accuracy = 0.806000053883\n",
      "Performance on test set: Batch Loss = 6.43342781067, Accuracy = 0.805000066757\n",
      "Training epochs #2548000:   Batch Loss = 6.422628, Accuracy = 0.813000023365\n",
      "Performance on test set: Batch Loss = 6.45673084259, Accuracy = 0.793000042439\n",
      "Training epochs #2550000:   Batch Loss = 6.499701, Accuracy = 0.790000021458\n",
      "Performance on test set: Batch Loss = 6.40592193604, Accuracy = 0.81400001049\n",
      "Training epochs #2552000:   Batch Loss = 6.406175, Accuracy = 0.810000002384\n",
      "Performance on test set: Batch Loss = 6.51288175583, Accuracy = 0.778000056744\n",
      "Training epochs #2554000:   Batch Loss = 6.408046, Accuracy = 0.817000031471\n",
      "Performance on test set: Batch Loss = 6.46877288818, Accuracy = 0.797000050545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #2556000:   Batch Loss = 6.464702, Accuracy = 0.795000016689\n",
      "Performance on test set: Batch Loss = 6.37155914307, Accuracy = 0.821000099182\n",
      "Training epochs #2558000:   Batch Loss = 6.411820, Accuracy = 0.806999981403\n",
      "Performance on test set: Batch Loss = 6.35552453995, Accuracy = 0.827999949455\n",
      "Training epochs #2560000:   Batch Loss = 6.406480, Accuracy = 0.811000049114\n",
      "Performance on test set: Batch Loss = 6.44848060608, Accuracy = 0.792999982834\n",
      "Training epochs #2562000:   Batch Loss = 6.393599, Accuracy = 0.814999997616\n",
      "Performance on test set: Batch Loss = 6.39914608002, Accuracy = 0.821000099182\n",
      "Training epochs #2564000:   Batch Loss = 6.359425, Accuracy = 0.818999946117\n",
      "Performance on test set: Batch Loss = 6.40719747543, Accuracy = 0.816000044346\n",
      "Training epochs #2566000:   Batch Loss = 6.417656, Accuracy = 0.799000024796\n",
      "Performance on test set: Batch Loss = 6.3864607811, Accuracy = 0.810000002384\n",
      "Training epochs #2568000:   Batch Loss = 6.424177, Accuracy = 0.796000003815\n",
      "Performance on test set: Batch Loss = 6.40446567535, Accuracy = 0.797999978065\n",
      "Training epochs #2570000:   Batch Loss = 6.406283, Accuracy = 0.811000049114\n",
      "Performance on test set: Batch Loss = 6.35956478119, Accuracy = 0.817000031471\n",
      "Training epochs #2572000:   Batch Loss = 6.395884, Accuracy = 0.81099998951\n",
      "Performance on test set: Batch Loss = 6.45990276337, Accuracy = 0.778999984264\n",
      "Training epochs #2574000:   Batch Loss = 6.403736, Accuracy = 0.804000020027\n",
      "Performance on test set: Batch Loss = 6.41061353683, Accuracy = 0.800999999046\n",
      "Training epochs #2576000:   Batch Loss = 6.399634, Accuracy = 0.795000016689\n",
      "Performance on test set: Batch Loss = 6.31563425064, Accuracy = 0.824000000954\n",
      "Training epochs #2578000:   Batch Loss = 6.404673, Accuracy = 0.797000050545\n",
      "Performance on test set: Batch Loss = 6.29497337341, Accuracy = 0.82699996233\n",
      "Training epochs #2580000:   Batch Loss = 6.344718, Accuracy = 0.819000005722\n",
      "Performance on test set: Batch Loss = 6.36985111237, Accuracy = 0.79800003767\n",
      "Training epochs #2582000:   Batch Loss = 6.371418, Accuracy = 0.803000092506\n",
      "Performance on test set: Batch Loss = 6.33157110214, Accuracy = 0.822000086308\n",
      "Training epochs #2584000:   Batch Loss = 6.301805, Accuracy = 0.818999946117\n",
      "Performance on test set: Batch Loss = 6.3388915062, Accuracy = 0.818000018597\n",
      "Training epochs #2586000:   Batch Loss = 6.387507, Accuracy = 0.792000055313\n",
      "Performance on test set: Batch Loss = 6.31210517883, Accuracy = 0.810000002384\n",
      "Training epochs #2588000:   Batch Loss = 6.322328, Accuracy = 0.807000041008\n",
      "Performance on test set: Batch Loss = 6.32451534271, Accuracy = 0.79800003767\n",
      "Training epochs #2590000:   Batch Loss = 6.325441, Accuracy = 0.811000049114\n",
      "Performance on test set: Batch Loss = 6.28480625153, Accuracy = 0.819000065327\n",
      "Training epochs #2592000:   Batch Loss = 6.321397, Accuracy = 0.803000092506\n",
      "Performance on test set: Batch Loss = 6.37744235992, Accuracy = 0.786000013351\n",
      "Training epochs #2594000:   Batch Loss = 6.294907, Accuracy = 0.814000070095\n",
      "Performance on test set: Batch Loss = 6.32722568512, Accuracy = 0.805999994278\n",
      "Training epochs #2596000:   Batch Loss = 6.281170, Accuracy = 0.813000023365\n",
      "Performance on test set: Batch Loss = 6.24636316299, Accuracy = 0.830000042915\n",
      "Training epochs #2598000:   Batch Loss = 6.287863, Accuracy = 0.814999997616\n",
      "Performance on test set: Batch Loss = 6.22849273682, Accuracy = 0.84600007534\n",
      "Training epochs #2600000:   Batch Loss = 6.231118, Accuracy = 0.834000051022\n",
      "Performance on test set: Batch Loss = 6.28758955002, Accuracy = 0.805000066757\n",
      "Training epochs #2602000:   Batch Loss = 6.280620, Accuracy = 0.820000052452\n",
      "Performance on test set: Batch Loss = 6.26501464844, Accuracy = 0.827000021935\n",
      "Training epochs #2604000:   Batch Loss = 6.307217, Accuracy = 0.80999994278\n",
      "Performance on test set: Batch Loss = 6.28377008438, Accuracy = 0.823000013828\n",
      "Training epochs #2606000:   Batch Loss = 6.345616, Accuracy = 0.819999992847\n",
      "Performance on test set: Batch Loss = 6.39679193497, Accuracy = 0.814000070095\n",
      "Training epochs #2608000:   Batch Loss = 6.423937, Accuracy = 0.799000024796\n",
      "Performance on test set: Batch Loss = 6.46576166153, Accuracy = 0.797000050545\n",
      "Training epochs #2610000:   Batch Loss = 6.449772, Accuracy = 0.79800003767\n",
      "Performance on test set: Batch Loss = 6.38301181793, Accuracy = 0.811999976635\n",
      "Training epochs #2612000:   Batch Loss = 6.427337, Accuracy = 0.786000013351\n",
      "Performance on test set: Batch Loss = 6.50023460388, Accuracy = 0.778000056744\n",
      "Training epochs #2614000:   Batch Loss = 6.404818, Accuracy = 0.800999999046\n",
      "Performance on test set: Batch Loss = 6.43512487411, Accuracy = 0.801999986172\n",
      "Training epochs #2616000:   Batch Loss = 6.366617, Accuracy = 0.818000018597\n",
      "Performance on test set: Batch Loss = 6.34670209885, Accuracy = 0.823000013828\n",
      "Training epochs #2618000:   Batch Loss = 6.346912, Accuracy = 0.820000052452\n",
      "Performance on test set: Batch Loss = 6.325756073, Accuracy = 0.82699996233\n",
      "Training epochs #2620000:   Batch Loss = 6.380606, Accuracy = 0.809000015259\n",
      "Performance on test set: Batch Loss = 6.39485311508, Accuracy = 0.797999978065\n",
      "Training epochs #2622000:   Batch Loss = 6.280778, Accuracy = 0.841000139713\n",
      "Performance on test set: Batch Loss = 6.35723209381, Accuracy = 0.820999979973\n",
      "Training epochs #2624000:   Batch Loss = 6.354465, Accuracy = 0.809000015259\n",
      "Performance on test set: Batch Loss = 6.36052465439, Accuracy = 0.816000044346\n",
      "Training epochs #2626000:   Batch Loss = 6.348882, Accuracy = 0.810000061989\n",
      "Performance on test set: Batch Loss = 6.34368276596, Accuracy = 0.811999976635\n",
      "Training epochs #2628000:   Batch Loss = 6.337453, Accuracy = 0.813000023365\n",
      "Performance on test set: Batch Loss = 6.37385749817, Accuracy = 0.79800003767\n",
      "Training epochs #2630000:   Batch Loss = 6.409444, Accuracy = 0.795000016689\n",
      "Performance on test set: Batch Loss = 6.31941986084, Accuracy = 0.819999933243\n",
      "Training epochs #2632000:   Batch Loss = 6.324459, Accuracy = 0.820000052452\n",
      "Performance on test set: Batch Loss = 6.43742465973, Accuracy = 0.778000056744\n",
      "Training epochs #2634000:   Batch Loss = 6.318871, Accuracy = 0.822000086308\n",
      "Performance on test set: Batch Loss = 6.38806343079, Accuracy = 0.800999999046\n",
      "Training epochs #2636000:   Batch Loss = 6.392195, Accuracy = 0.799999952316\n",
      "Performance on test set: Batch Loss = 6.28671216965, Accuracy = 0.824000000954\n",
      "Training epochs #2638000:   Batch Loss = 6.332754, Accuracy = 0.806999921799\n",
      "Performance on test set: Batch Loss = 6.27378606796, Accuracy = 0.827999949455\n",
      "Training epochs #2640000:   Batch Loss = 6.333851, Accuracy = 0.81400001049\n",
      "Performance on test set: Batch Loss = 6.34462928772, Accuracy = 0.79800003767\n",
      "Training epochs #2642000:   Batch Loss = 6.326361, Accuracy = 0.817000031471\n",
      "Performance on test set: Batch Loss = 6.30969524384, Accuracy = 0.823000073433\n",
      "Training epochs #2644000:   Batch Loss = 6.260263, Accuracy = 0.824999928474\n",
      "Performance on test set: Batch Loss = 6.32298231125, Accuracy = 0.816000044346\n",
      "Training epochs #2646000:   Batch Loss = 6.324121, Accuracy = 0.799000024796\n",
      "Performance on test set: Batch Loss = 6.30313014984, Accuracy = 0.828000068665\n",
      "Training epochs #2648000:   Batch Loss = 6.342350, Accuracy = 0.810000061989\n",
      "Performance on test set: Batch Loss = 6.33356571198, Accuracy = 0.79800003767\n",
      "Training epochs #2650000:   Batch Loss = 6.333145, Accuracy = 0.80999994278\n",
      "Performance on test set: Batch Loss = 6.27331256866, Accuracy = 0.818000018597\n",
      "Training epochs #2652000:   Batch Loss = 6.334099, Accuracy = 0.817000031471\n",
      "Performance on test set: Batch Loss = 6.39319324493, Accuracy = 0.779000043869\n",
      "Training epochs #2654000:   Batch Loss = 6.323925, Accuracy = 0.805000007153\n",
      "Performance on test set: Batch Loss = 6.34840202332, Accuracy = 0.810000002384\n",
      "Training epochs #2656000:   Batch Loss = 6.339334, Accuracy = 0.808999955654\n",
      "Performance on test set: Batch Loss = 6.24836683273, Accuracy = 0.823000013828\n",
      "Training epochs #2658000:   Batch Loss = 6.333189, Accuracy = 0.79699999094\n",
      "Performance on test set: Batch Loss = 6.22666454315, Accuracy = 0.838999986649\n",
      "Training epochs #2660000:   Batch Loss = 6.294072, Accuracy = 0.839000046253\n",
      "Performance on test set: Batch Loss = 6.29758930206, Accuracy = 0.79800003767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #2662000:   Batch Loss = 6.309628, Accuracy = 0.801999986172\n",
      "Performance on test set: Batch Loss = 6.26708698273, Accuracy = 0.823000073433\n",
      "Training epochs #2664000:   Batch Loss = 6.242978, Accuracy = 0.851999998093\n",
      "Performance on test set: Batch Loss = 6.28406476974, Accuracy = 0.816000044346\n",
      "Training epochs #2666000:   Batch Loss = 6.360793, Accuracy = 0.792999982834\n",
      "Performance on test set: Batch Loss = 6.25857496262, Accuracy = 0.828000068665\n",
      "Training epochs #2668000:   Batch Loss = 6.339180, Accuracy = 0.829999983311\n",
      "Performance on test set: Batch Loss = 6.34622430801, Accuracy = 0.79800003767\n",
      "Training epochs #2670000:   Batch Loss = 6.347939, Accuracy = 0.811000049114\n",
      "Performance on test set: Batch Loss = 6.31010532379, Accuracy = 0.821999907494\n",
      "Training epochs #2672000:   Batch Loss = 6.342845, Accuracy = 0.805000066757\n",
      "Performance on test set: Batch Loss = 6.40451478958, Accuracy = 0.804000020027\n",
      "Training epochs #2674000:   Batch Loss = 6.291403, Accuracy = 0.81200003624\n",
      "Performance on test set: Batch Loss = 6.330057621, Accuracy = 0.801000058651\n",
      "Training epochs #2676000:   Batch Loss = 6.264660, Accuracy = 0.807000041008\n",
      "Performance on test set: Batch Loss = 6.2394990921, Accuracy = 0.840000033379\n",
      "Training epochs #2678000:   Batch Loss = 6.281907, Accuracy = 0.841000080109\n",
      "Performance on test set: Batch Loss = 6.19715595245, Accuracy = 0.842000067234\n",
      "Training epochs #2680000:   Batch Loss = 6.195227, Accuracy = 0.830000042915\n",
      "Performance on test set: Batch Loss = 6.27067184448, Accuracy = 0.79800003767\n",
      "Training epochs #2682000:   Batch Loss = 6.271463, Accuracy = 0.818000018597\n",
      "Performance on test set: Batch Loss = 6.24573659897, Accuracy = 0.848999977112\n",
      "Training epochs #2684000:   Batch Loss = 6.266130, Accuracy = 0.817000031471\n",
      "Performance on test set: Batch Loss = 6.25262498856, Accuracy = 0.824000000954\n",
      "Training epochs #2686000:   Batch Loss = 6.213052, Accuracy = 0.82800000906\n",
      "Performance on test set: Batch Loss = 6.23156261444, Accuracy = 0.839999973774\n",
      "Training epochs #2688000:   Batch Loss = 6.264452, Accuracy = 0.808999896049\n",
      "Performance on test set: Batch Loss = 6.25659990311, Accuracy = 0.816000044346\n",
      "Training epochs #2690000:   Batch Loss = 6.246638, Accuracy = 0.82800000906\n",
      "Performance on test set: Batch Loss = 6.19515323639, Accuracy = 0.847999989986\n",
      "Training epochs #2692000:   Batch Loss = 6.241011, Accuracy = 0.82800000906\n",
      "Performance on test set: Batch Loss = 6.3190279007, Accuracy = 0.804000020027\n",
      "Training epochs #2694000:   Batch Loss = 6.233472, Accuracy = 0.811000049114\n",
      "Performance on test set: Batch Loss = 6.27024316788, Accuracy = 0.81200003624\n",
      "Training epochs #2696000:   Batch Loss = 6.195196, Accuracy = 0.846000015736\n",
      "Performance on test set: Batch Loss = 6.18217420578, Accuracy = 0.839000046253\n",
      "Training epochs #2698000:   Batch Loss = 6.195719, Accuracy = 0.840000033379\n",
      "Performance on test set: Batch Loss = 6.15189409256, Accuracy = 0.838999986649\n",
      "Training epochs #2700000:   Batch Loss = 6.236239, Accuracy = 0.832999944687\n",
      "Performance on test set: Batch Loss = 6.21154069901, Accuracy = 0.82800000906\n",
      "Training epochs #2702000:   Batch Loss = 6.119882, Accuracy = 0.851999998093\n",
      "Performance on test set: Batch Loss = 6.19678926468, Accuracy = 0.834000051022\n",
      "Training epochs #2704000:   Batch Loss = 6.195532, Accuracy = 0.835000038147\n",
      "Performance on test set: Batch Loss = 6.21679925919, Accuracy = 0.840000033379\n",
      "Training epochs #2706000:   Batch Loss = 6.197895, Accuracy = 0.833999991417\n",
      "Performance on test set: Batch Loss = 6.19253826141, Accuracy = 0.840000033379\n",
      "Training epochs #2708000:   Batch Loss = 6.180376, Accuracy = 0.84500002861\n",
      "Performance on test set: Batch Loss = 6.21235084534, Accuracy = 0.828999996185\n",
      "Training epochs #2710000:   Batch Loss = 6.254133, Accuracy = 0.816000044346\n",
      "Performance on test set: Batch Loss = 6.15380048752, Accuracy = 0.850000023842\n",
      "Training epochs #2712000:   Batch Loss = 6.178756, Accuracy = 0.841999948025\n",
      "Performance on test set: Batch Loss = 6.28047227859, Accuracy = 0.805000066757\n",
      "Training epochs #2714000:   Batch Loss = 6.157212, Accuracy = 0.847000002861\n",
      "Performance on test set: Batch Loss = 6.22867870331, Accuracy = 0.821000039577\n",
      "Training epochs #2716000:   Batch Loss = 6.230381, Accuracy = 0.819000065327\n",
      "Performance on test set: Batch Loss = 6.14681816101, Accuracy = 0.833000063896\n",
      "Training epochs #2718000:   Batch Loss = 6.169243, Accuracy = 0.840000033379\n",
      "Performance on test set: Batch Loss = 6.11853885651, Accuracy = 0.856999993324\n",
      "Training epochs #2720000:   Batch Loss = 6.198772, Accuracy = 0.828999996185\n",
      "Performance on test set: Batch Loss = 6.17168951035, Accuracy = 0.827000021935\n",
      "Training epochs #2722000:   Batch Loss = 6.188087, Accuracy = 0.837000072002\n",
      "Performance on test set: Batch Loss = 6.2618765831, Accuracy = 0.82900005579\n",
      "Training epochs #2724000:   Batch Loss = 6.249472, Accuracy = 0.826000034809\n",
      "Performance on test set: Batch Loss = 6.25644779205, Accuracy = 0.817000031471\n",
      "Training epochs #2726000:   Batch Loss = 6.155992, Accuracy = 0.833999991417\n",
      "Performance on test set: Batch Loss = 6.34769487381, Accuracy = 0.75100004673\n",
      "Training epochs #2728000:   Batch Loss = 6.278637, Accuracy = 0.799999952316\n",
      "Performance on test set: Batch Loss = 6.39191055298, Accuracy = 0.804000020027\n",
      "Training epochs #2730000:   Batch Loss = 6.388509, Accuracy = 0.809000015259\n",
      "Performance on test set: Batch Loss = 6.32032251358, Accuracy = 0.81400001049\n",
      "Training epochs #2732000:   Batch Loss = 6.240697, Accuracy = 0.807999968529\n",
      "Performance on test set: Batch Loss = 6.27418327332, Accuracy = 0.783000051975\n",
      "Training epochs #2734000:   Batch Loss = 6.254970, Accuracy = 0.839000046253\n",
      "Performance on test set: Batch Loss = 6.28349018097, Accuracy = 0.821000039577\n",
      "Training epochs #2736000:   Batch Loss = 6.240891, Accuracy = 0.803000032902\n",
      "Performance on test set: Batch Loss = 6.16471195221, Accuracy = 0.820000052452\n",
      "Training epochs #2738000:   Batch Loss = 6.253797, Accuracy = 0.788000047207\n",
      "Performance on test set: Batch Loss = 6.14355278015, Accuracy = 0.81200003624\n",
      "Training epochs #2740000:   Batch Loss = 6.198628, Accuracy = 0.814000070095\n",
      "Performance on test set: Batch Loss = 6.22692298889, Accuracy = 0.791999995708\n",
      "Training epochs #2742000:   Batch Loss = 6.211439, Accuracy = 0.811999976635\n",
      "Performance on test set: Batch Loss = 6.16389656067, Accuracy = 0.825999975204\n",
      "Training epochs #2744000:   Batch Loss = 6.147920, Accuracy = 0.822999954224\n",
      "Performance on test set: Batch Loss = 6.20042800903, Accuracy = 0.823000013828\n",
      "Training epochs #2746000:   Batch Loss = 6.252418, Accuracy = 0.794999957085\n",
      "Performance on test set: Batch Loss = 6.16194009781, Accuracy = 0.817000031471\n",
      "Training epochs #2748000:   Batch Loss = 6.230890, Accuracy = 0.807000041008\n",
      "Performance on test set: Batch Loss = 6.17909955978, Accuracy = 0.803000032902\n",
      "Training epochs #2750000:   Batch Loss = 6.163427, Accuracy = 0.81300008297\n",
      "Performance on test set: Batch Loss = 6.10783433914, Accuracy = 0.82800000906\n",
      "Training epochs #2752000:   Batch Loss = 6.208645, Accuracy = 0.809000015259\n",
      "Performance on test set: Batch Loss = 6.2263045311, Accuracy = 0.783000051975\n",
      "Training epochs #2754000:   Batch Loss = 6.150976, Accuracy = 0.81099998951\n",
      "Performance on test set: Batch Loss = 6.16627693176, Accuracy = 0.811000049114\n",
      "Training epochs #2756000:   Batch Loss = 6.120751, Accuracy = 0.819000005722\n",
      "Performance on test set: Batch Loss = 6.07376480103, Accuracy = 0.836000025272\n",
      "Training epochs #2758000:   Batch Loss = 6.116730, Accuracy = 0.84399998188\n",
      "Performance on test set: Batch Loss = 6.05219459534, Accuracy = 0.859000086784\n",
      "Training epochs #2760000:   Batch Loss = 6.043508, Accuracy = 0.851000070572\n",
      "Performance on test set: Batch Loss = 6.10493850708, Accuracy = 0.819000005722\n",
      "Training epochs #2762000:   Batch Loss = 6.128495, Accuracy = 0.820000052452\n",
      "Performance on test set: Batch Loss = 6.09594535828, Accuracy = 0.851000070572\n",
      "Training epochs #2764000:   Batch Loss = 6.127532, Accuracy = 0.842000067234\n",
      "Performance on test set: Batch Loss = 6.11640691757, Accuracy = 0.838999986649\n",
      "Training epochs #2766000:   Batch Loss = 6.053117, Accuracy = 0.836999952793\n",
      "Performance on test set: Batch Loss = 6.09107685089, Accuracy = 0.834000110626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #2768000:   Batch Loss = 6.103250, Accuracy = 0.84399998188\n",
      "Performance on test set: Batch Loss = 6.10310697556, Accuracy = 0.848000049591\n",
      "Training epochs #2770000:   Batch Loss = 6.102327, Accuracy = 0.839000046253\n",
      "Performance on test set: Batch Loss = 6.0400762558, Accuracy = 0.836000025272\n",
      "Training epochs #2772000:   Batch Loss = 6.081260, Accuracy = 0.836000025272\n",
      "Performance on test set: Batch Loss = 6.14704608917, Accuracy = 0.814999997616\n",
      "Training epochs #2774000:   Batch Loss = 6.064552, Accuracy = 0.833999931812\n",
      "Performance on test set: Batch Loss = 6.10326099396, Accuracy = 0.820000052452\n",
      "Training epochs #2776000:   Batch Loss = 6.054209, Accuracy = 0.852999985218\n",
      "Performance on test set: Batch Loss = 6.02054166794, Accuracy = 0.847000062466\n",
      "Training epochs #2778000:   Batch Loss = 6.061110, Accuracy = 0.86000007391\n",
      "Performance on test set: Batch Loss = 5.98977994919, Accuracy = 0.860000014305\n",
      "Training epochs #2780000:   Batch Loss = 6.067239, Accuracy = 0.838999986649\n",
      "Performance on test set: Batch Loss = 6.04729223251, Accuracy = 0.840000033379\n",
      "Training epochs #2782000:   Batch Loss = 5.958620, Accuracy = 0.872000038624\n",
      "Performance on test set: Batch Loss = 6.15189647675, Accuracy = 0.842000067234\n",
      "Training epochs #2784000:   Batch Loss = 6.850920, Accuracy = 0.758000016212\n",
      "Performance on test set: Batch Loss = 7.27094173431, Accuracy = 0.763000011444\n",
      "Training epochs #2786000:   Batch Loss = 7.775445, Accuracy = 0.655999958515\n",
      "Performance on test set: Batch Loss = 7.61853265762, Accuracy = 0.668000042439\n",
      "Training epochs #2788000:   Batch Loss = 7.498592, Accuracy = 0.662000000477\n",
      "Performance on test set: Batch Loss = 7.47426891327, Accuracy = 0.639000058174\n",
      "Training epochs #2790000:   Batch Loss = 7.219971, Accuracy = 0.736999988556\n",
      "Performance on test set: Batch Loss = 7.02803850174, Accuracy = 0.773000001907\n",
      "Training epochs #2792000:   Batch Loss = 6.967240, Accuracy = 0.762000083923\n",
      "Performance on test set: Batch Loss = 7.12710762024, Accuracy = 0.753999948502\n",
      "Training epochs #2794000:   Batch Loss = 7.006862, Accuracy = 0.778999984264\n",
      "Performance on test set: Batch Loss = 6.95590877533, Accuracy = 0.755999982357\n",
      "Training epochs #2796000:   Batch Loss = 6.749459, Accuracy = 0.754000067711\n",
      "Performance on test set: Batch Loss = 6.5215921402, Accuracy = 0.78100001812\n",
      "Training epochs #2798000:   Batch Loss = 6.457072, Accuracy = 0.76700001955\n",
      "Performance on test set: Batch Loss = 6.59562730789, Accuracy = 0.768000006676\n",
      "Training epochs #2800000:   Batch Loss = 6.740902, Accuracy = 0.651000022888\n",
      "Performance on test set: Batch Loss = 6.79396438599, Accuracy = 0.632000029087\n",
      "Training epochs #2802000:   Batch Loss = 6.802926, Accuracy = 0.658999979496\n",
      "Performance on test set: Batch Loss = 6.75114059448, Accuracy = 0.679999947548\n",
      "Training epochs #2804000:   Batch Loss = 6.697454, Accuracy = 0.678000032902\n",
      "Performance on test set: Batch Loss = 6.69460487366, Accuracy = 0.660999953747\n",
      "Training epochs #2806000:   Batch Loss = 6.622862, Accuracy = 0.651999950409\n",
      "Performance on test set: Batch Loss = 6.48234033585, Accuracy = 0.696999967098\n",
      "Training epochs #2808000:   Batch Loss = 6.522817, Accuracy = 0.767000079155\n",
      "Performance on test set: Batch Loss = 6.48966693878, Accuracy = 0.757999956608\n",
      "Training epochs #2810000:   Batch Loss = 6.481920, Accuracy = 0.768000006676\n",
      "Performance on test set: Batch Loss = 6.44008684158, Accuracy = 0.774999976158\n",
      "Training epochs #2812000:   Batch Loss = 6.527313, Accuracy = 0.760000050068\n",
      "Performance on test set: Batch Loss = 6.53864192963, Accuracy = 0.746999979019\n",
      "Training epochs #2814000:   Batch Loss = 6.560382, Accuracy = 0.743000030518\n",
      "Performance on test set: Batch Loss = 6.55821561813, Accuracy = 0.745999932289\n",
      "Training epochs #2816000:   Batch Loss = 6.558805, Accuracy = 0.739000022411\n",
      "Performance on test set: Batch Loss = 6.41044330597, Accuracy = 0.782000005245\n",
      "Training epochs #2818000:   Batch Loss = 6.537027, Accuracy = 0.733999967575\n",
      "Performance on test set: Batch Loss = 6.42178869247, Accuracy = 0.768999993801\n",
      "Training epochs #2820000:   Batch Loss = 6.437786, Accuracy = 0.764999985695\n",
      "Performance on test set: Batch Loss = 6.46717882156, Accuracy = 0.741000056267\n",
      "Training epochs #2822000:   Batch Loss = 6.451078, Accuracy = 0.744000017643\n",
      "Performance on test set: Batch Loss = 6.40652513504, Accuracy = 0.760999977589\n",
      "Training epochs #2824000:   Batch Loss = 6.386848, Accuracy = 0.756000041962\n",
      "Performance on test set: Batch Loss = 6.42293739319, Accuracy = 0.761000037193\n",
      "Training epochs #2826000:   Batch Loss = 6.463559, Accuracy = 0.742000043392\n",
      "Performance on test set: Batch Loss = 6.39433860779, Accuracy = 0.767999947071\n",
      "Training epochs #2828000:   Batch Loss = 6.456157, Accuracy = 0.75\n",
      "Performance on test set: Batch Loss = 6.42523956299, Accuracy = 0.751999974251\n",
      "Training epochs #2830000:   Batch Loss = 6.424474, Accuracy = 0.757000029087\n",
      "Performance on test set: Batch Loss = 6.35471963882, Accuracy = 0.775000035763\n",
      "Training epochs #2832000:   Batch Loss = 6.402183, Accuracy = 0.753000020981\n",
      "Performance on test set: Batch Loss = 6.42945146561, Accuracy = 0.743999958038\n",
      "Training epochs #2834000:   Batch Loss = 6.394375, Accuracy = 0.754999995232\n",
      "Performance on test set: Batch Loss = 6.4276804924, Accuracy = 0.744999945164\n",
      "Training epochs #2836000:   Batch Loss = 6.344846, Accuracy = 0.7650000453\n",
      "Performance on test set: Batch Loss = 6.31042957306, Accuracy = 0.780999958515\n",
      "Training epochs #2838000:   Batch Loss = 6.383234, Accuracy = 0.759000003338\n",
      "Performance on test set: Batch Loss = 6.33859968185, Accuracy = 0.766999959946\n",
      "Training epochs #2840000:   Batch Loss = 6.300141, Accuracy = 0.775999903679\n",
      "Performance on test set: Batch Loss = 6.41654348373, Accuracy = 0.740999996662\n",
      "Training epochs #2842000:   Batch Loss = 6.353712, Accuracy = 0.774000048637\n",
      "Performance on test set: Batch Loss = 6.36608982086, Accuracy = 0.760999977589\n",
      "Training epochs #2844000:   Batch Loss = 6.360829, Accuracy = 0.766000032425\n",
      "Performance on test set: Batch Loss = 6.37621831894, Accuracy = 0.761000037193\n",
      "Training epochs #2846000:   Batch Loss = 6.313527, Accuracy = 0.772000014782\n",
      "Performance on test set: Batch Loss = 6.32947254181, Accuracy = 0.768999993801\n",
      "Training epochs #2848000:   Batch Loss = 6.391098, Accuracy = 0.746999979019\n",
      "Performance on test set: Batch Loss = 6.36401557922, Accuracy = 0.754000067711\n",
      "Training epochs #2850000:   Batch Loss = 6.348659, Accuracy = 0.761000037193\n",
      "Performance on test set: Batch Loss = 6.3151473999, Accuracy = 0.780999958515\n",
      "Training epochs #2852000:   Batch Loss = 6.363429, Accuracy = 0.75\n",
      "Performance on test set: Batch Loss = 6.39403581619, Accuracy = 0.74899995327\n",
      "Training epochs #2854000:   Batch Loss = 6.346806, Accuracy = 0.76499992609\n",
      "Performance on test set: Batch Loss = 6.38825750351, Accuracy = 0.754000008106\n",
      "Training epochs #2856000:   Batch Loss = 6.313232, Accuracy = 0.776000082493\n",
      "Performance on test set: Batch Loss = 6.27154064178, Accuracy = 0.795000016689\n",
      "Training epochs #2858000:   Batch Loss = 6.283840, Accuracy = 0.784000039101\n",
      "Performance on test set: Batch Loss = 6.29369068146, Accuracy = 0.769999980927\n",
      "Training epochs #2860000:   Batch Loss = 6.332385, Accuracy = 0.759000062943\n",
      "Performance on test set: Batch Loss = 6.36616849899, Accuracy = 0.744000017643\n",
      "Training epochs #2862000:   Batch Loss = 6.232100, Accuracy = 0.794000089169\n",
      "Performance on test set: Batch Loss = 6.32745599747, Accuracy = 0.763999998569\n",
      "Training epochs #2864000:   Batch Loss = 6.322522, Accuracy = 0.754000008106\n",
      "Performance on test set: Batch Loss = 6.34275484085, Accuracy = 0.763999938965\n",
      "Training epochs #2866000:   Batch Loss = 6.326905, Accuracy = 0.770999968052\n",
      "Performance on test set: Batch Loss = 6.29258108139, Accuracy = 0.776000082493\n",
      "Training epochs #2868000:   Batch Loss = 6.285367, Accuracy = 0.775999963284\n",
      "Performance on test set: Batch Loss = 6.32473564148, Accuracy = 0.762000024319\n",
      "Training epochs #2870000:   Batch Loss = 6.373274, Accuracy = 0.746000051498\n",
      "Performance on test set: Batch Loss = 6.27615356445, Accuracy = 0.78200006485\n",
      "Training epochs #2872000:   Batch Loss = 6.295620, Accuracy = 0.776000022888\n",
      "Performance on test set: Batch Loss = 6.35624694824, Accuracy = 0.749000012875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #2874000:   Batch Loss = 6.275672, Accuracy = 0.777999937534\n",
      "Performance on test set: Batch Loss = 6.3478307724, Accuracy = 0.754999995232\n",
      "Training epochs #2876000:   Batch Loss = 6.323843, Accuracy = 0.764000058174\n",
      "Performance on test set: Batch Loss = 6.23261260986, Accuracy = 0.795000076294\n",
      "Training epochs #2878000:   Batch Loss = 6.262799, Accuracy = 0.772000014782\n",
      "Performance on test set: Batch Loss = 6.25407266617, Accuracy = 0.778000056744\n",
      "Training epochs #2880000:   Batch Loss = 6.294941, Accuracy = 0.762000024319\n",
      "Performance on test set: Batch Loss = 6.31929588318, Accuracy = 0.750999987125\n",
      "Training epochs #2882000:   Batch Loss = 6.278574, Accuracy = 0.777999997139\n",
      "Performance on test set: Batch Loss = 6.2877035141, Accuracy = 0.770999968052\n",
      "Training epochs #2884000:   Batch Loss = 6.209259, Accuracy = 0.787000000477\n",
      "Performance on test set: Batch Loss = 6.2982673645, Accuracy = 0.764000058174\n",
      "Training epochs #2886000:   Batch Loss = 6.319105, Accuracy = 0.733999967575\n",
      "Performance on test set: Batch Loss = 6.24528312683, Accuracy = 0.779000043869\n",
      "Training epochs #2888000:   Batch Loss = 6.295447, Accuracy = 0.75\n",
      "Performance on test set: Batch Loss = 6.27699708939, Accuracy = 0.758999943733\n",
      "Training epochs #2890000:   Batch Loss = 6.277198, Accuracy = 0.771000087261\n",
      "Performance on test set: Batch Loss = 6.23161458969, Accuracy = 0.782999992371\n",
      "Training epochs #2892000:   Batch Loss = 6.280925, Accuracy = 0.765999913216\n",
      "Performance on test set: Batch Loss = 6.29878997803, Accuracy = 0.750999987125\n",
      "Training epochs #2894000:   Batch Loss = 6.289397, Accuracy = 0.747000038624\n",
      "Performance on test set: Batch Loss = 6.2916135788, Accuracy = 0.754999995232\n",
      "Training epochs #2896000:   Batch Loss = 6.297159, Accuracy = 0.748000085354\n",
      "Performance on test set: Batch Loss = 6.18450117111, Accuracy = 0.795000016689\n",
      "Training epochs #2898000:   Batch Loss = 6.287098, Accuracy = 0.740999996662\n",
      "Performance on test set: Batch Loss = 6.19862890244, Accuracy = 0.777000010014\n",
      "Training epochs #2900000:   Batch Loss = 6.249016, Accuracy = 0.774000048637\n",
      "Performance on test set: Batch Loss = 6.26036548615, Accuracy = 0.75\n",
      "Training epochs #2902000:   Batch Loss = 6.259370, Accuracy = 0.758000016212\n",
      "Performance on test set: Batch Loss = 6.23520898819, Accuracy = 0.771000027657\n",
      "Training epochs #2904000:   Batch Loss = 6.186224, Accuracy = 0.763000011444\n",
      "Performance on test set: Batch Loss = 6.24222707748, Accuracy = 0.766999959946\n",
      "Training epochs #2906000:   Batch Loss = 6.279461, Accuracy = 0.74899995327\n",
      "Performance on test set: Batch Loss = 6.18972206116, Accuracy = 0.778999984264\n",
      "Training epochs #2908000:   Batch Loss = 6.254357, Accuracy = 0.755000054836\n",
      "Performance on test set: Batch Loss = 6.21946811676, Accuracy = 0.759000003338\n",
      "Training epochs #2910000:   Batch Loss = 6.233493, Accuracy = 0.762000024319\n",
      "Performance on test set: Batch Loss = 6.17441225052, Accuracy = 0.785000026226\n",
      "Training epochs #2912000:   Batch Loss = 6.229275, Accuracy = 0.762000024319\n",
      "Performance on test set: Batch Loss = 6.24820709229, Accuracy = 0.754000067711\n",
      "Training epochs #2914000:   Batch Loss = 6.209064, Accuracy = 0.76700001955\n",
      "Performance on test set: Batch Loss = 6.23328399658, Accuracy = 0.764000058174\n",
      "Training epochs #2916000:   Batch Loss = 6.168931, Accuracy = 0.77999997139\n",
      "Performance on test set: Batch Loss = 6.1285943985, Accuracy = 0.802000045776\n",
      "Training epochs #2918000:   Batch Loss = 6.186378, Accuracy = 0.769999980927\n",
      "Performance on test set: Batch Loss = 6.13677358627, Accuracy = 0.807999968529\n",
      "Training epochs #2920000:   Batch Loss = 6.108603, Accuracy = 0.801999986172\n",
      "Performance on test set: Batch Loss = 6.19338417053, Accuracy = 0.757000029087\n",
      "Training epochs #2922000:   Batch Loss = 6.169963, Accuracy = 0.787999987602\n",
      "Performance on test set: Batch Loss = 6.17885541916, Accuracy = 0.776000082493\n",
      "Training epochs #2924000:   Batch Loss = 6.168192, Accuracy = 0.778999984264\n",
      "Performance on test set: Batch Loss = 6.18246936798, Accuracy = 0.777999997139\n",
      "Training epochs #2926000:   Batch Loss = 6.123966, Accuracy = 0.800000071526\n",
      "Performance on test set: Batch Loss = 6.1276679039, Accuracy = 0.796999931335\n",
      "Training epochs #2928000:   Batch Loss = 6.176769, Accuracy = 0.768999993801\n",
      "Performance on test set: Batch Loss = 6.1609210968, Accuracy = 0.773000001907\n",
      "Training epochs #2930000:   Batch Loss = 6.147063, Accuracy = 0.785000026226\n",
      "Performance on test set: Batch Loss = 6.11635398865, Accuracy = 0.796000003815\n",
      "Training epochs #2932000:   Batch Loss = 6.146856, Accuracy = 0.771000027657\n",
      "Performance on test set: Batch Loss = 6.18846988678, Accuracy = 0.780999958515\n",
      "Training epochs #2934000:   Batch Loss = 6.130803, Accuracy = 0.792000055313\n",
      "Performance on test set: Batch Loss = 6.16932582855, Accuracy = 0.797999978065\n",
      "Training epochs #2936000:   Batch Loss = 6.111569, Accuracy = 0.826000034809\n",
      "Performance on test set: Batch Loss = 6.07482528687, Accuracy = 0.837000012398\n",
      "Training epochs #2938000:   Batch Loss = 6.091982, Accuracy = 0.835999965668\n",
      "Performance on test set: Batch Loss = 6.06316947937, Accuracy = 0.836000025272\n",
      "Training epochs #2940000:   Batch Loss = 6.103889, Accuracy = 0.79800003767\n",
      "Performance on test set: Batch Loss = 6.12635612488, Accuracy = 0.791999995708\n",
      "Training epochs #2942000:   Batch Loss = 6.018343, Accuracy = 0.847000002861\n",
      "Performance on test set: Batch Loss = 6.10831069946, Accuracy = 0.819000005722\n",
      "Training epochs #2944000:   Batch Loss = 6.088112, Accuracy = 0.831999957561\n",
      "Performance on test set: Batch Loss = 6.11507368088, Accuracy = 0.824000000954\n",
      "Training epochs #2946000:   Batch Loss = 6.092750, Accuracy = 0.822000026703\n",
      "Performance on test set: Batch Loss = 6.06514644623, Accuracy = 0.832000017166\n",
      "Training epochs #2948000:   Batch Loss = 6.055140, Accuracy = 0.833999991417\n",
      "Performance on test set: Batch Loss = 6.08543872833, Accuracy = 0.823000073433\n",
      "Training epochs #2950000:   Batch Loss = 6.139721, Accuracy = 0.81299996376\n",
      "Performance on test set: Batch Loss = 6.04764223099, Accuracy = 0.832000017166\n",
      "Training epochs #2952000:   Batch Loss = 6.048654, Accuracy = 0.842000007629\n",
      "Performance on test set: Batch Loss = 6.13202571869, Accuracy = 0.801999986172\n",
      "Training epochs #2954000:   Batch Loss = 6.038319, Accuracy = 0.829999983311\n",
      "Performance on test set: Batch Loss = 6.09210109711, Accuracy = 0.831000030041\n",
      "Training epochs #2956000:   Batch Loss = 6.083344, Accuracy = 0.823000013828\n",
      "Performance on test set: Batch Loss = 6.01319932938, Accuracy = 0.835000038147\n",
      "Training epochs #2958000:   Batch Loss = 6.015614, Accuracy = 0.839000046253\n",
      "Performance on test set: Batch Loss = 5.99273204803, Accuracy = 0.865999937057\n",
      "Training epochs #2960000:   Batch Loss = 6.055537, Accuracy = 0.837000012398\n",
      "Performance on test set: Batch Loss = 6.04099464417, Accuracy = 0.826000034809\n",
      "Training epochs #2962000:   Batch Loss = 6.016818, Accuracy = 0.840999960899\n",
      "Performance on test set: Batch Loss = 6.03703069687, Accuracy = 0.849000036716\n",
      "Training epochs #2964000:   Batch Loss = 5.969570, Accuracy = 0.847999930382\n",
      "Performance on test set: Batch Loss = 6.04400682449, Accuracy = 0.828999996185\n",
      "Training epochs #2966000:   Batch Loss = 6.027473, Accuracy = 0.821000039577\n",
      "Performance on test set: Batch Loss = 5.99399852753, Accuracy = 0.832999944687\n",
      "Training epochs #2968000:   Batch Loss = 6.026547, Accuracy = 0.832000017166\n",
      "Performance on test set: Batch Loss = 6.01261043549, Accuracy = 0.84500002861\n",
      "Training epochs #2970000:   Batch Loss = 6.047908, Accuracy = 0.837999999523\n",
      "Performance on test set: Batch Loss = 5.98320817947, Accuracy = 0.856000065804\n",
      "Training epochs #2972000:   Batch Loss = 6.009257, Accuracy = 0.828999996185\n",
      "Performance on test set: Batch Loss = 6.05625915527, Accuracy = 0.805999994278\n",
      "Training epochs #2974000:   Batch Loss = 5.985402, Accuracy = 0.853999972343\n",
      "Performance on test set: Batch Loss = 6.02081775665, Accuracy = 0.826000034809\n",
      "Training epochs #2976000:   Batch Loss = 6.019700, Accuracy = 0.828999996185\n",
      "Performance on test set: Batch Loss = 5.94958877563, Accuracy = 0.844000041485\n",
      "Training epochs #2978000:   Batch Loss = 6.015968, Accuracy = 0.839000046253\n",
      "Performance on test set: Batch Loss = 5.91984081268, Accuracy = 0.876000106335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #2980000:   Batch Loss = 5.986712, Accuracy = 0.850000023842\n",
      "Performance on test set: Batch Loss = 5.95464897156, Accuracy = 0.834000051022\n",
      "Training epochs #2982000:   Batch Loss = 5.991099, Accuracy = 0.830999970436\n",
      "Performance on test set: Batch Loss = 5.96046352386, Accuracy = 0.846000015736\n",
      "Training epochs #2984000:   Batch Loss = 5.894754, Accuracy = 0.850000023842\n",
      "Performance on test set: Batch Loss = 5.97354078293, Accuracy = 0.836000025272\n",
      "Training epochs #2986000:   Batch Loss = 6.008349, Accuracy = 0.809000015259\n",
      "Performance on test set: Batch Loss = 5.92483234406, Accuracy = 0.842999994755\n",
      "Training epochs #2988000:   Batch Loss = 5.975091, Accuracy = 0.843000054359\n",
      "Performance on test set: Batch Loss = 5.94169950485, Accuracy = 0.846000015736\n",
      "Training epochs #2990000:   Batch Loss = 5.930661, Accuracy = 0.839000046253\n",
      "Performance on test set: Batch Loss = 5.91041707993, Accuracy = 0.861999988556\n",
      "Training epochs #2992000:   Batch Loss = 5.983156, Accuracy = 0.821999967098\n",
      "Performance on test set: Batch Loss = 5.98295927048, Accuracy = 0.821000099182\n",
      "Training epochs #2994000:   Batch Loss = 5.916838, Accuracy = 0.840000033379\n",
      "Performance on test set: Batch Loss = 5.94413948059, Accuracy = 0.834999978542\n",
      "Training epochs #2996000:   Batch Loss = 5.900393, Accuracy = 0.853999972343\n",
      "Performance on test set: Batch Loss = 5.8931350708, Accuracy = 0.857000052929\n",
      "Training epochs #2998000:   Batch Loss = 5.915503, Accuracy = 0.847000002861\n",
      "Performance on test set: Batch Loss = 5.87119483948, Accuracy = 0.883000016212\n",
      "Training epochs #3000000:   Batch Loss = 5.852814, Accuracy = 0.852999985218\n",
      "Performance on test set: Batch Loss = 5.89275217056, Accuracy = 0.837999939919\n",
      "Training epochs #3002000:   Batch Loss = 5.923594, Accuracy = 0.848999977112\n",
      "Performance on test set: Batch Loss = 5.92636489868, Accuracy = 0.881000041962\n",
      "Training epochs #3004000:   Batch Loss = 5.947443, Accuracy = 0.833000063896\n",
      "Performance on test set: Batch Loss = 5.95406723022, Accuracy = 0.829999983311\n",
      "Training epochs #3006000:   Batch Loss = 5.874604, Accuracy = 0.847000002861\n",
      "Performance on test set: Batch Loss = 5.88104391098, Accuracy = 0.881999969482\n",
      "Training epochs #3008000:   Batch Loss = 5.926629, Accuracy = 0.882000029087\n",
      "Performance on test set: Batch Loss = 5.89661598206, Accuracy = 0.87800014019\n",
      "Training epochs #3010000:   Batch Loss = 5.957947, Accuracy = 0.876999974251\n",
      "Performance on test set: Batch Loss = 9.01299190521, Accuracy = 0.163000002503\n",
      "Training epochs #3012000:   Batch Loss = 8.548432, Accuracy = 0.136000007391\n",
      "Performance on test set: Batch Loss = 8.06697750092, Accuracy = 0.156000003219\n",
      "Training epochs #3014000:   Batch Loss = 7.405934, Accuracy = 0.768000006676\n",
      "Performance on test set: Batch Loss = 7.31418657303, Accuracy = 0.756000041962\n",
      "Training epochs #3016000:   Batch Loss = 7.205581, Accuracy = 0.768999993801\n",
      "Performance on test set: Batch Loss = 7.05321645737, Accuracy = 0.783000051975\n",
      "Training epochs #3018000:   Batch Loss = 7.097081, Accuracy = 0.774999976158\n",
      "Performance on test set: Batch Loss = 7.21928024292, Accuracy = 0.769999980927\n",
      "Training epochs #3020000:   Batch Loss = 7.159923, Accuracy = 0.763999938965\n",
      "Performance on test set: Batch Loss = 7.57177829742, Accuracy = 0.634999990463\n",
      "Training epochs #3022000:   Batch Loss = 6.999047, Accuracy = 0.790000021458\n",
      "Performance on test set: Batch Loss = 7.18652772903, Accuracy = 0.759000003338\n",
      "Training epochs #3024000:   Batch Loss = 7.198097, Accuracy = 0.745999991894\n",
      "Performance on test set: Batch Loss = 7.07982349396, Accuracy = 0.761999964714\n",
      "Training epochs #3026000:   Batch Loss = 6.955391, Accuracy = 0.758000016212\n",
      "Performance on test set: Batch Loss = 6.77739191055, Accuracy = 0.76499992609\n",
      "Training epochs #3028000:   Batch Loss = 6.707306, Accuracy = 0.76700001955\n",
      "Performance on test set: Batch Loss = 6.80877733231, Accuracy = 0.754000008106\n",
      "Training epochs #3030000:   Batch Loss = 6.798779, Accuracy = 0.75\n",
      "Performance on test set: Batch Loss = 6.6581363678, Accuracy = 0.778999984264\n",
      "Training epochs #3032000:   Batch Loss = 6.599174, Accuracy = 0.775999963284\n",
      "Performance on test set: Batch Loss = 6.62677145004, Accuracy = 0.746999979019\n",
      "Training epochs #3034000:   Batch Loss = 6.555255, Accuracy = 0.774999976158\n",
      "Performance on test set: Batch Loss = 6.63308906555, Accuracy = 0.745999932289\n",
      "Training epochs #3036000:   Batch Loss = 6.586154, Accuracy = 0.756000041962\n",
      "Performance on test set: Batch Loss = 6.44226741791, Accuracy = 0.786000013351\n",
      "Training epochs #3038000:   Batch Loss = 6.482627, Accuracy = 0.766000032425\n",
      "Performance on test set: Batch Loss = 6.52658367157, Accuracy = 0.769999980927\n",
      "Training epochs #3040000:   Batch Loss = 6.535892, Accuracy = 0.756000041962\n",
      "Performance on test set: Batch Loss = 6.5238904953, Accuracy = 0.740999996662\n",
      "Training epochs #3042000:   Batch Loss = 6.417226, Accuracy = 0.776000022888\n",
      "Performance on test set: Batch Loss = 6.46522331238, Accuracy = 0.771000027657\n",
      "Training epochs #3044000:   Batch Loss = 6.479267, Accuracy = 0.780000030994\n",
      "Performance on test set: Batch Loss = 6.43224668503, Accuracy = 0.763000071049\n",
      "Training epochs #3046000:   Batch Loss = 6.511853, Accuracy = 0.727999985218\n",
      "Performance on test set: Batch Loss = 6.39711666107, Accuracy = 0.766999959946\n",
      "Training epochs #3048000:   Batch Loss = 6.482476, Accuracy = 0.742999970913\n",
      "Performance on test set: Batch Loss = 6.5370631218, Accuracy = 0.639999985695\n",
      "Training epochs #3050000:   Batch Loss = 6.422729, Accuracy = 0.760000050068\n",
      "Performance on test set: Batch Loss = 6.34393167496, Accuracy = 0.774000048637\n",
      "Training epochs #3052000:   Batch Loss = 6.447757, Accuracy = 0.763999998569\n",
      "Performance on test set: Batch Loss = 6.81702041626, Accuracy = 0.754999995232\n",
      "Training epochs #3054000:   Batch Loss = 6.453671, Accuracy = 0.743000030518\n",
      "Performance on test set: Batch Loss = 6.85790109634, Accuracy = 0.64099997282\n",
      "Training epochs #3056000:   Batch Loss = 6.927474, Accuracy = 0.634000003338\n",
      "Performance on test set: Batch Loss = 6.91564655304, Accuracy = 0.662999987602\n",
      "Training epochs #3058000:   Batch Loss = 6.963837, Accuracy = 0.631000041962\n",
      "Performance on test set: Batch Loss = 6.94008970261, Accuracy = 0.657999992371\n",
      "Training epochs #3060000:   Batch Loss = 6.974519, Accuracy = 0.649999976158\n",
      "Performance on test set: Batch Loss = 6.97555971146, Accuracy = 0.629999995232\n",
      "Training epochs #3062000:   Batch Loss = 6.952901, Accuracy = 0.637000024319\n",
      "Performance on test set: Batch Loss = 6.8786611557, Accuracy = 0.663999915123\n",
      "Training epochs #3064000:   Batch Loss = 6.879271, Accuracy = 0.644999980927\n",
      "Performance on test set: Batch Loss = 6.89497184753, Accuracy = 0.639000058174\n",
      "Training epochs #3066000:   Batch Loss = 6.851547, Accuracy = 0.641000032425\n",
      "Performance on test set: Batch Loss = 6.7307767868, Accuracy = 0.669000029564\n",
      "Training epochs #3068000:   Batch Loss = 6.781414, Accuracy = 0.6400000453\n",
      "Performance on test set: Batch Loss = 6.76901817322, Accuracy = 0.639000058174\n",
      "Training epochs #3070000:   Batch Loss = 6.683271, Accuracy = 0.64200001955\n",
      "Performance on test set: Batch Loss = 6.62316608429, Accuracy = 0.661000013351\n",
      "Training epochs #3072000:   Batch Loss = 6.605408, Accuracy = 0.648000061512\n",
      "Performance on test set: Batch Loss = 6.64522981644, Accuracy = 0.624000012875\n",
      "Training epochs #3074000:   Batch Loss = 6.537680, Accuracy = 0.649999976158\n",
      "Performance on test set: Batch Loss = 6.56900882721, Accuracy = 0.639999985695\n",
      "Training epochs #3076000:   Batch Loss = 6.513664, Accuracy = 0.652000010014\n",
      "Performance on test set: Batch Loss = 6.46456766129, Accuracy = 0.666000008583\n",
      "Training epochs #3078000:   Batch Loss = 6.537879, Accuracy = 0.670000076294\n",
      "Performance on test set: Batch Loss = 6.51884222031, Accuracy = 0.769999980927\n",
      "Training epochs #3080000:   Batch Loss = 6.488752, Accuracy = 0.780000030994\n",
      "Performance on test set: Batch Loss = 6.57347393036, Accuracy = 0.744000077248\n",
      "Training epochs #3082000:   Batch Loss = 6.529227, Accuracy = 0.774000048637\n",
      "Performance on test set: Batch Loss = 6.53065061569, Accuracy = 0.763000011444\n",
      "Training epochs #3084000:   Batch Loss = 6.536611, Accuracy = 0.767000079155\n",
      "Performance on test set: Batch Loss = 6.54403829575, Accuracy = 0.761999964714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #3086000:   Batch Loss = 6.486645, Accuracy = 0.769999980927\n",
      "Performance on test set: Batch Loss = 6.47201919556, Accuracy = 0.677000045776\n",
      "Training epochs #3088000:   Batch Loss = 6.532764, Accuracy = 0.64099997282\n",
      "Performance on test set: Batch Loss = 6.53445291519, Accuracy = 0.638999998569\n",
      "Training epochs #3090000:   Batch Loss = 6.497063, Accuracy = 0.64099997282\n",
      "Performance on test set: Batch Loss = 6.45386981964, Accuracy = 0.660000026226\n",
      "Training epochs #3092000:   Batch Loss = 6.497500, Accuracy = 0.662000000477\n",
      "Performance on test set: Batch Loss = 6.51429843903, Accuracy = 0.631000041962\n",
      "Training epochs #3094000:   Batch Loss = 6.475952, Accuracy = 0.639999985695\n",
      "Performance on test set: Batch Loss = 6.47250080109, Accuracy = 0.646999955177\n",
      "Training epochs #3096000:   Batch Loss = 6.410882, Accuracy = 0.673999965191\n",
      "Performance on test set: Batch Loss = 6.37977838516, Accuracy = 0.672999978065\n",
      "Training epochs #3098000:   Batch Loss = 6.402881, Accuracy = 0.670000016689\n",
      "Performance on test set: Batch Loss = 6.39593029022, Accuracy = 0.659999966621\n",
      "Training epochs #3100000:   Batch Loss = 6.403474, Accuracy = 0.641999959946\n",
      "Performance on test set: Batch Loss = 6.44679021835, Accuracy = 0.641000032425\n",
      "Training epochs #3102000:   Batch Loss = 6.338436, Accuracy = 0.689999938011\n",
      "Performance on test set: Batch Loss = 6.40703010559, Accuracy = 0.667000055313\n",
      "Training epochs #3104000:   Batch Loss = 6.404598, Accuracy = 0.651000022888\n",
      "Performance on test set: Batch Loss = 6.40999078751, Accuracy = 0.646000027657\n",
      "Training epochs #3106000:   Batch Loss = 6.365875, Accuracy = 0.655999958515\n",
      "Performance on test set: Batch Loss = 6.33772850037, Accuracy = 0.676999986172\n",
      "Training epochs #3108000:   Batch Loss = 6.307433, Accuracy = 0.666999995708\n",
      "Performance on test set: Batch Loss = 6.37564373016, Accuracy = 0.768999993801\n",
      "Training epochs #3110000:   Batch Loss = 6.419425, Accuracy = 0.75\n",
      "Performance on test set: Batch Loss = 6.32339382172, Accuracy = 0.778000056744\n",
      "Training epochs #3112000:   Batch Loss = 6.270001, Accuracy = 0.775000035763\n",
      "Performance on test set: Batch Loss = 6.37443208694, Accuracy = 0.750999987125\n",
      "Training epochs #3114000:   Batch Loss = 6.247757, Accuracy = 0.777999997139\n",
      "Performance on test set: Batch Loss = 6.36898231506, Accuracy = 0.753000020981\n",
      "Training epochs #3116000:   Batch Loss = 6.357030, Accuracy = 0.764999985695\n",
      "Performance on test set: Batch Loss = 6.24169254303, Accuracy = 0.794000029564\n",
      "Training epochs #3118000:   Batch Loss = 6.233200, Accuracy = 0.777999997139\n",
      "Performance on test set: Batch Loss = 6.31305456161, Accuracy = 0.769000053406\n",
      "Training epochs #3120000:   Batch Loss = 6.356922, Accuracy = 0.763000011444\n",
      "Performance on test set: Batch Loss = 6.39981079102, Accuracy = 0.748000025749\n",
      "Training epochs #3122000:   Batch Loss = 6.344912, Accuracy = 0.775000095367\n",
      "Performance on test set: Batch Loss = 6.40560770035, Accuracy = 0.76700001955\n",
      "Training epochs #3124000:   Batch Loss = 6.372476, Accuracy = 0.777999997139\n",
      "Performance on test set: Batch Loss = 6.44227790833, Accuracy = 0.762000024319\n",
      "Training epochs #3126000:   Batch Loss = 6.485106, Accuracy = 0.728999972343\n",
      "Performance on test set: Batch Loss = 6.38297939301, Accuracy = 0.766999959946\n",
      "Training epochs #3128000:   Batch Loss = 6.452192, Accuracy = 0.743000030518\n",
      "Performance on test set: Batch Loss = 6.44349241257, Accuracy = 0.753000020981\n",
      "Training epochs #3130000:   Batch Loss = 6.388729, Accuracy = 0.76599997282\n",
      "Performance on test set: Batch Loss = 6.3455619812, Accuracy = 0.773999989033\n",
      "Training epochs #3132000:   Batch Loss = 6.389682, Accuracy = 0.760000050068\n",
      "Performance on test set: Batch Loss = 6.41505527496, Accuracy = 0.744000017643\n",
      "Training epochs #3134000:   Batch Loss = 6.399004, Accuracy = 0.744000017643\n",
      "Performance on test set: Batch Loss = 6.37523937225, Accuracy = 0.746999979019\n",
      "Training epochs #3136000:   Batch Loss = 6.378118, Accuracy = 0.751999974251\n",
      "Performance on test set: Batch Loss = 6.2841334343, Accuracy = 0.79800003767\n",
      "Training epochs #3138000:   Batch Loss = 6.362661, Accuracy = 0.746999979019\n",
      "Performance on test set: Batch Loss = 6.28345251083, Accuracy = 0.787999987602\n",
      "Training epochs #3140000:   Batch Loss = 6.325129, Accuracy = 0.786000072956\n",
      "Performance on test set: Batch Loss = 6.33970975876, Accuracy = 0.749000072479\n",
      "Training epochs #3142000:   Batch Loss = 6.342772, Accuracy = 0.754000008106\n",
      "Performance on test set: Batch Loss = 6.3139128685, Accuracy = 0.769999980927\n",
      "Training epochs #3144000:   Batch Loss = 6.275959, Accuracy = 0.771999955177\n",
      "Performance on test set: Batch Loss = 6.31822252274, Accuracy = 0.76800006628\n",
      "Training epochs #3146000:   Batch Loss = 6.324019, Accuracy = 0.754000008106\n",
      "Performance on test set: Batch Loss = 6.24857854843, Accuracy = 0.772000074387\n",
      "Training epochs #3148000:   Batch Loss = 6.305978, Accuracy = 0.751999974251\n",
      "Performance on test set: Batch Loss = 6.28158664703, Accuracy = 0.769999980927\n",
      "Training epochs #3150000:   Batch Loss = 6.256897, Accuracy = 0.765000104904\n",
      "Performance on test set: Batch Loss = 6.23827028275, Accuracy = 0.778000056744\n",
      "Training epochs #3152000:   Batch Loss = 6.255074, Accuracy = 0.762000083923\n",
      "Performance on test set: Batch Loss = 6.29431676865, Accuracy = 0.754999995232\n",
      "Training epochs #3154000:   Batch Loss = 6.241403, Accuracy = 0.771999955177\n",
      "Performance on test set: Batch Loss = 6.26523590088, Accuracy = 0.754999995232\n",
      "Training epochs #3156000:   Batch Loss = 6.220304, Accuracy = 0.774000048637\n",
      "Performance on test set: Batch Loss = 6.17452907562, Accuracy = 0.795000016689\n",
      "Training epochs #3158000:   Batch Loss = 6.228611, Accuracy = 0.767999947071\n",
      "Performance on test set: Batch Loss = 6.17604351044, Accuracy = 0.787999987602\n",
      "Training epochs #3160000:   Batch Loss = 6.152566, Accuracy = 0.78200006485\n",
      "Performance on test set: Batch Loss = 6.24423408508, Accuracy = 0.750999987125\n",
      "Training epochs #3162000:   Batch Loss = 6.183788, Accuracy = 0.783999979496\n",
      "Performance on test set: Batch Loss = 6.21541309357, Accuracy = 0.768999993801\n",
      "Training epochs #3164000:   Batch Loss = 6.203822, Accuracy = 0.773000061512\n",
      "Performance on test set: Batch Loss = 6.22747087479, Accuracy = 0.768999993801\n",
      "Training epochs #3166000:   Batch Loss = 6.162906, Accuracy = 0.78100001812\n",
      "Performance on test set: Batch Loss = 6.16351985931, Accuracy = 0.768999993801\n",
      "Training epochs #3168000:   Batch Loss = 6.201755, Accuracy = 0.763000011444\n",
      "Performance on test set: Batch Loss = 6.1952881813, Accuracy = 0.771000027657\n",
      "Training epochs #3170000:   Batch Loss = 6.191055, Accuracy = 0.769000053406\n",
      "Performance on test set: Batch Loss = 6.15344190598, Accuracy = 0.780000030994\n",
      "Training epochs #3172000:   Batch Loss = 6.191537, Accuracy = 0.749000012875\n",
      "Performance on test set: Batch Loss = 6.20812940598, Accuracy = 0.757000029087\n",
      "Training epochs #3174000:   Batch Loss = 6.151283, Accuracy = 0.76700001955\n",
      "Performance on test set: Batch Loss = 6.16298151016, Accuracy = 0.757000029087\n",
      "Training epochs #3176000:   Batch Loss = 6.117145, Accuracy = 0.780000030994\n",
      "Performance on test set: Batch Loss = 6.04616737366, Accuracy = 0.794999957085\n",
      "Training epochs #3178000:   Batch Loss = 6.063653, Accuracy = 0.787000060081\n",
      "Performance on test set: Batch Loss = 6.06279182434, Accuracy = 0.787999987602\n",
      "Training epochs #3180000:   Batch Loss = 6.030517, Accuracy = 0.770000040531\n",
      "Performance on test set: Batch Loss = 6.05661869049, Accuracy = 0.754999995232\n",
      "Training epochs #3182000:   Batch Loss = 5.980646, Accuracy = 0.804000020027\n",
      "Performance on test set: Batch Loss = 6.05888366699, Accuracy = 0.769999980927\n",
      "Training epochs #3184000:   Batch Loss = 6.054090, Accuracy = 0.759999990463\n",
      "Performance on test set: Batch Loss = 6.09804868698, Accuracy = 0.773000001907\n",
      "Training epochs #3186000:   Batch Loss = 6.074616, Accuracy = 0.762000024319\n",
      "Performance on test set: Batch Loss = 6.00662279129, Accuracy = 0.771000027657\n",
      "Training epochs #3188000:   Batch Loss = 6.011003, Accuracy = 0.782999992371\n",
      "Performance on test set: Batch Loss = 6.04524612427, Accuracy = 0.772000074387\n",
      "Training epochs #3190000:   Batch Loss = 6.092596, Accuracy = 0.754999995232\n",
      "Performance on test set: Batch Loss = 6.01117181778, Accuracy = 0.780000030994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #3192000:   Batch Loss = 5.985789, Accuracy = 0.785999953747\n",
      "Performance on test set: Batch Loss = 6.05202007294, Accuracy = 0.759000003338\n",
      "Training epochs #3194000:   Batch Loss = 5.970864, Accuracy = 0.777999997139\n",
      "Performance on test set: Batch Loss = 6.02026748657, Accuracy = 0.759999990463\n",
      "Training epochs #3196000:   Batch Loss = 6.030286, Accuracy = 0.771000027657\n",
      "Performance on test set: Batch Loss = 5.97071170807, Accuracy = 0.796999931335\n",
      "Training epochs #3198000:   Batch Loss = 5.967959, Accuracy = 0.78100001812\n",
      "Performance on test set: Batch Loss = 5.9640288353, Accuracy = 0.789999961853\n",
      "Training epochs #3200000:   Batch Loss = 6.000477, Accuracy = 0.766999959946\n",
      "Performance on test set: Batch Loss = 5.9871006012, Accuracy = 0.757000029087\n",
      "Training epochs #3202000:   Batch Loss = 5.931887, Accuracy = 0.785000085831\n",
      "Performance on test set: Batch Loss = 5.98433351517, Accuracy = 0.771000027657\n",
      "Training epochs #3204000:   Batch Loss = 5.962634, Accuracy = 0.809000015259\n",
      "Performance on test set: Batch Loss = 6.0175652504, Accuracy = 0.797000050545\n",
      "Training epochs #3206000:   Batch Loss = 5.993360, Accuracy = 0.773000001907\n",
      "Performance on test set: Batch Loss = 5.94157028198, Accuracy = 0.806999981403\n",
      "Training epochs #3208000:   Batch Loss = 5.997335, Accuracy = 0.794000029564\n",
      "Performance on test set: Batch Loss = 5.97903966904, Accuracy = 0.811999976635\n",
      "Training epochs #3210000:   Batch Loss = 6.027102, Accuracy = 0.797999978065\n",
      "Performance on test set: Batch Loss = 5.94977092743, Accuracy = 0.820000052452\n",
      "Training epochs #3212000:   Batch Loss = 5.944110, Accuracy = 0.814999997616\n",
      "Performance on test set: Batch Loss = 5.98667144775, Accuracy = 0.799999952316\n",
      "Training epochs #3214000:   Batch Loss = 5.949930, Accuracy = 0.81099998951\n",
      "Performance on test set: Batch Loss = 5.95904779434, Accuracy = 0.810000061989\n",
      "Training epochs #3216000:   Batch Loss = 5.972921, Accuracy = 0.799000024796\n",
      "Performance on test set: Batch Loss = 5.92926454544, Accuracy = 0.829999983311\n",
      "Training epochs #3218000:   Batch Loss = 5.983337, Accuracy = 0.791000068188\n",
      "Performance on test set: Batch Loss = 5.91044139862, Accuracy = 0.824000000954\n",
      "Training epochs #3220000:   Batch Loss = 5.969998, Accuracy = 0.804000079632\n",
      "Performance on test set: Batch Loss = 5.92818689346, Accuracy = 0.817000031471\n",
      "Training epochs #3222000:   Batch Loss = 5.985161, Accuracy = 0.819000005722\n",
      "Performance on test set: Batch Loss = 5.93091011047, Accuracy = 0.832999944687\n",
      "Training epochs #3224000:   Batch Loss = 5.889166, Accuracy = 0.846000015736\n",
      "Performance on test set: Batch Loss = 5.97626781464, Accuracy = 0.842999994755\n",
      "Training epochs #3226000:   Batch Loss = 5.982367, Accuracy = 0.815999984741\n",
      "Performance on test set: Batch Loss = 5.89732837677, Accuracy = 0.833999991417\n",
      "Training epochs #3228000:   Batch Loss = 5.953053, Accuracy = 0.827000081539\n",
      "Performance on test set: Batch Loss = 5.93370294571, Accuracy = 0.837999939919\n",
      "Training epochs #3230000:   Batch Loss = 5.892198, Accuracy = 0.848999977112\n",
      "Performance on test set: Batch Loss = 5.90851545334, Accuracy = 0.847999989986\n",
      "Training epochs #3232000:   Batch Loss = 5.944663, Accuracy = 0.824999988079\n",
      "Performance on test set: Batch Loss = 5.95186233521, Accuracy = 0.827000021935\n",
      "Training epochs #3234000:   Batch Loss = 5.892763, Accuracy = 0.857000052929\n",
      "Performance on test set: Batch Loss = 5.92490339279, Accuracy = 0.837000012398\n",
      "Training epochs #3236000:   Batch Loss = 5.918615, Accuracy = 0.846000015736\n",
      "Performance on test set: Batch Loss = 5.91103219986, Accuracy = 0.854000031948\n",
      "Training epochs #3238000:   Batch Loss = 5.904682, Accuracy = 0.841999948025\n",
      "Performance on test set: Batch Loss = 5.88221311569, Accuracy = 0.860000014305\n",
      "Training epochs #3240000:   Batch Loss = 5.853788, Accuracy = 0.851000070572\n",
      "Performance on test set: Batch Loss = 5.92155170441, Accuracy = 0.841999948025\n",
      "Training epochs #3242000:   Batch Loss = 5.911426, Accuracy = 0.847000062466\n",
      "Performance on test set: Batch Loss = 5.91063547134, Accuracy = 0.850000023842\n",
      "Training epochs #3244000:   Batch Loss = 5.961764, Accuracy = 0.824000000954\n",
      "Performance on test set: Batch Loss = 5.95425415039, Accuracy = 0.81099998951\n",
      "Training epochs #3246000:   Batch Loss = 5.895008, Accuracy = 0.813000023365\n",
      "Performance on test set: Batch Loss = 5.87126111984, Accuracy = 0.813000023365\n",
      "Training epochs #3248000:   Batch Loss = 5.928768, Accuracy = 0.803000032902\n",
      "Performance on test set: Batch Loss = 5.91468954086, Accuracy = 0.821999967098\n",
      "Training epochs #3250000:   Batch Loss = 5.895879, Accuracy = 0.835000038147\n",
      "Performance on test set: Batch Loss = 5.8805937767, Accuracy = 0.844000041485\n",
      "Training epochs #3252000:   Batch Loss = 5.913132, Accuracy = 0.835000038147\n",
      "Performance on test set: Batch Loss = 5.92895126343, Accuracy = 0.825999975204\n",
      "Training epochs #3254000:   Batch Loss = 5.904649, Accuracy = 0.832000017166\n",
      "Performance on test set: Batch Loss = 5.89605855942, Accuracy = 0.837000012398\n",
      "Training epochs #3256000:   Batch Loss = 5.872779, Accuracy = 0.852000057697\n",
      "Performance on test set: Batch Loss = 5.8744020462, Accuracy = 0.850000023842\n",
      "Training epochs #3258000:   Batch Loss = 5.879685, Accuracy = 0.851000010967\n",
      "Performance on test set: Batch Loss = 5.84822559357, Accuracy = 0.865000069141\n",
      "Training epochs #3260000:   Batch Loss = 5.838363, Accuracy = 0.841000020504\n",
      "Performance on test set: Batch Loss = 5.86707925797, Accuracy = 0.845000088215\n",
      "Training epochs #3262000:   Batch Loss = 5.813005, Accuracy = 0.866000056267\n",
      "Performance on test set: Batch Loss = 5.87389326096, Accuracy = 0.851999998093\n",
      "Training epochs #3264000:   Batch Loss = 5.871426, Accuracy = 0.841000080109\n",
      "Performance on test set: Batch Loss = 5.92722845078, Accuracy = 0.852999985218\n",
      "Training epochs #3266000:   Batch Loss = 5.902478, Accuracy = 0.842000067234\n",
      "Performance on test set: Batch Loss = 5.84905815125, Accuracy = 0.84399998188\n",
      "Training epochs #3268000:   Batch Loss = 5.844326, Accuracy = 0.873000025749\n",
      "Performance on test set: Batch Loss = 5.87872600555, Accuracy = 0.861999988556\n",
      "Training epochs #3270000:   Batch Loss = 5.934888, Accuracy = 0.856999993324\n",
      "Performance on test set: Batch Loss = 5.85654163361, Accuracy = 0.875000059605\n",
      "Training epochs #3272000:   Batch Loss = 5.827098, Accuracy = 0.880000054836\n",
      "Performance on test set: Batch Loss = 5.90455389023, Accuracy = 0.849000096321\n",
      "Training epochs #3274000:   Batch Loss = 5.814260, Accuracy = 0.87399995327\n",
      "Performance on test set: Batch Loss = 5.87089776993, Accuracy = 0.869000017643\n",
      "Training epochs #3276000:   Batch Loss = 5.897290, Accuracy = 0.861000001431\n",
      "Performance on test set: Batch Loss = 5.84545564651, Accuracy = 0.871000051498\n",
      "Training epochs #3278000:   Batch Loss = 5.823921, Accuracy = 0.872999966145\n",
      "Performance on test set: Batch Loss = 5.8206448555, Accuracy = 0.886000037193\n",
      "Training epochs #3280000:   Batch Loss = 5.863066, Accuracy = 0.863000035286\n",
      "Performance on test set: Batch Loss = 5.83764791489, Accuracy = 0.869000077248\n",
      "Training epochs #3282000:   Batch Loss = 5.794063, Accuracy = 0.886000037193\n",
      "Performance on test set: Batch Loss = 5.84661006927, Accuracy = 0.871000051498\n",
      "Training epochs #3284000:   Batch Loss = 5.826869, Accuracy = 0.873000025749\n",
      "Performance on test set: Batch Loss = 5.88810873032, Accuracy = 0.868000090122\n",
      "Training epochs #3286000:   Batch Loss = 5.849521, Accuracy = 0.864000082016\n",
      "Performance on test set: Batch Loss = 5.81348514557, Accuracy = 0.866000056267\n",
      "Training epochs #3288000:   Batch Loss = 5.870757, Accuracy = 0.857000052929\n",
      "Performance on test set: Batch Loss = 5.85671281815, Accuracy = 0.862000048161\n",
      "Training epochs #3290000:   Batch Loss = 5.912668, Accuracy = 0.855000019073\n",
      "Performance on test set: Batch Loss = 5.83207798004, Accuracy = 0.87700009346\n",
      "Training epochs #3292000:   Batch Loss = 5.826329, Accuracy = 0.883000075817\n",
      "Performance on test set: Batch Loss = 5.87837076187, Accuracy = 0.850000023842\n",
      "Training epochs #3294000:   Batch Loss = 5.823733, Accuracy = 0.87600004673\n",
      "Performance on test set: Batch Loss = 5.8437871933, Accuracy = 0.871000051498\n",
      "Training epochs #3296000:   Batch Loss = 5.856179, Accuracy = 0.864000082016\n",
      "Performance on test set: Batch Loss = 5.8238120079, Accuracy = 0.873000085354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #3298000:   Batch Loss = 5.867975, Accuracy = 0.849000036716\n",
      "Performance on test set: Batch Loss = 5.79620313644, Accuracy = 0.885999977589\n",
      "Training epochs #3300000:   Batch Loss = 5.859056, Accuracy = 0.869000017643\n",
      "Performance on test set: Batch Loss = 5.84436845779, Accuracy = 0.868000090122\n",
      "Training epochs #3302000:   Batch Loss = 5.941602, Accuracy = 0.859000086784\n",
      "Performance on test set: Batch Loss = 5.83734226227, Accuracy = 0.873000025749\n",
      "Training epochs #3304000:   Batch Loss = 5.788363, Accuracy = 0.869000017643\n",
      "Performance on test set: Batch Loss = 5.87664937973, Accuracy = 0.84399998188\n",
      "Training epochs #3306000:   Batch Loss = 5.901359, Accuracy = 0.825000047684\n",
      "Performance on test set: Batch Loss = 5.81126928329, Accuracy = 0.850000023842\n",
      "Training epochs #3308000:   Batch Loss = 5.895206, Accuracy = 0.825999975204\n",
      "Performance on test set: Batch Loss = 5.84838676453, Accuracy = 0.843000054359\n",
      "Training epochs #3310000:   Batch Loss = 5.805278, Accuracy = 0.873000025749\n",
      "Performance on test set: Batch Loss = 5.81079244614, Accuracy = 0.867000043392\n",
      "Training epochs #3312000:   Batch Loss = 5.865729, Accuracy = 0.842000067234\n",
      "Performance on test set: Batch Loss = 5.86741924286, Accuracy = 0.848999977112\n",
      "Training epochs #3314000:   Batch Loss = 5.794477, Accuracy = 0.870000064373\n",
      "Performance on test set: Batch Loss = 5.83899068832, Accuracy = 0.850999951363\n",
      "Training epochs #3316000:   Batch Loss = 5.815346, Accuracy = 0.865000069141\n",
      "Performance on test set: Batch Loss = 5.81892776489, Accuracy = 0.861000061035\n",
      "Training epochs #3318000:   Batch Loss = 5.817145, Accuracy = 0.856999993324\n",
      "Performance on test set: Batch Loss = 5.78033256531, Accuracy = 0.878000020981\n",
      "Training epochs #3320000:   Batch Loss = 5.763070, Accuracy = 0.873000025749\n",
      "Performance on test set: Batch Loss = 5.80531024933, Accuracy = 0.856000006199\n",
      "Training epochs #3322000:   Batch Loss = 5.803787, Accuracy = 0.865000009537\n",
      "Performance on test set: Batch Loss = 5.8109664917, Accuracy = 0.865999996662\n",
      "Training epochs #3324000:   Batch Loss = 5.872945, Accuracy = 0.850000023842\n",
      "Performance on test set: Batch Loss = 5.85737514496, Accuracy = 0.868000090122\n",
      "Training epochs #3326000:   Batch Loss = 5.793269, Accuracy = 0.867000043392\n",
      "Performance on test set: Batch Loss = 5.78890275955, Accuracy = 0.867000102997\n",
      "Training epochs #3328000:   Batch Loss = 5.813567, Accuracy = 0.865000009537\n",
      "Performance on test set: Batch Loss = 5.8252530098, Accuracy = 0.851999998093\n",
      "Training epochs #3330000:   Batch Loss = 5.805050, Accuracy = 0.867000043392\n",
      "Performance on test set: Batch Loss = 5.79188680649, Accuracy = 0.859000086784\n",
      "Training epochs #3332000:   Batch Loss = 5.816781, Accuracy = 0.855000019073\n",
      "Performance on test set: Batch Loss = 5.84207677841, Accuracy = 0.850000023842\n",
      "Training epochs #3334000:   Batch Loss = 5.818686, Accuracy = 0.854000091553\n",
      "Performance on test set: Batch Loss = 5.81294631958, Accuracy = 0.872000038624\n",
      "Training epochs #3336000:   Batch Loss = 5.786140, Accuracy = 0.874000012875\n",
      "Performance on test set: Batch Loss = 5.78922367096, Accuracy = 0.871000051498\n",
      "Training epochs #3338000:   Batch Loss = 5.803012, Accuracy = 0.862000107765\n",
      "Performance on test set: Batch Loss = 5.75865840912, Accuracy = 0.886000037193\n",
      "Training epochs #3340000:   Batch Loss = 5.753336, Accuracy = 0.866000056267\n",
      "Performance on test set: Batch Loss = 5.77236747742, Accuracy = 0.870999991894\n",
      "Training epochs #3342000:   Batch Loss = 5.727484, Accuracy = 0.8900000453\n",
      "Performance on test set: Batch Loss = 5.78380727768, Accuracy = 0.874000072479\n",
      "Training epochs #3344000:   Batch Loss = 5.785916, Accuracy = 0.864000082016\n",
      "Performance on test set: Batch Loss = 5.82228183746, Accuracy = 0.870999991894\n",
      "Training epochs #3346000:   Batch Loss = 5.800069, Accuracy = 0.872999966145\n",
      "Performance on test set: Batch Loss = 5.87610912323, Accuracy = 0.867000102997\n",
      "Training epochs #3348000:   Batch Loss = 7.931577, Accuracy = 0.153999999166\n",
      "Performance on test set: Batch Loss = 8.96024894714, Accuracy = 0.157000005245\n",
      "Training epochs #3350000:   Batch Loss = 9.408739, Accuracy = 0.146999999881\n",
      "Performance on test set: Batch Loss = 9.00445652008, Accuracy = 0.153999984264\n",
      "Training epochs #3352000:   Batch Loss = 8.361642, Accuracy = 0.165000006557\n",
      "Performance on test set: Batch Loss = 7.75589942932, Accuracy = 0.147999987006\n",
      "Training epochs #3354000:   Batch Loss = 7.036859, Accuracy = 0.776000022888\n",
      "Performance on test set: Batch Loss = 6.90338563919, Accuracy = 0.638000011444\n",
      "Training epochs #3356000:   Batch Loss = 6.947036, Accuracy = 0.619000017643\n",
      "Performance on test set: Batch Loss = 6.91995811462, Accuracy = 0.657000005245\n",
      "Training epochs #3358000:   Batch Loss = 7.072865, Accuracy = 0.663999974728\n",
      "Performance on test set: Batch Loss = 7.17229509354, Accuracy = 0.657000005245\n",
      "Training epochs #3360000:   Batch Loss = 7.313430, Accuracy = 0.644999980927\n",
      "Performance on test set: Batch Loss = 7.48349094391, Accuracy = 0.625\n",
      "Training epochs #3362000:   Batch Loss = 7.361461, Accuracy = 0.65600001812\n",
      "Performance on test set: Batch Loss = 7.26006507874, Accuracy = 0.657999992371\n",
      "Training epochs #3364000:   Batch Loss = 7.251423, Accuracy = 0.648000001907\n",
      "Performance on test set: Batch Loss = 7.23889255524, Accuracy = 0.637000024319\n",
      "Training epochs #3366000:   Batch Loss = 7.201427, Accuracy = 0.618999958038\n",
      "Performance on test set: Batch Loss = 6.91558074951, Accuracy = 0.666000008583\n",
      "Training epochs #3368000:   Batch Loss = 7.036612, Accuracy = 0.615000009537\n",
      "Performance on test set: Batch Loss = 6.91696453094, Accuracy = 0.631999969482\n",
      "Training epochs #3370000:   Batch Loss = 6.840403, Accuracy = 0.647999942303\n",
      "Performance on test set: Batch Loss = 6.78168916702, Accuracy = 0.660000026226\n",
      "Training epochs #3372000:   Batch Loss = 6.922064, Accuracy = 0.635999977589\n",
      "Performance on test set: Batch Loss = 6.94231271744, Accuracy = 0.620000004768\n",
      "Training epochs #3374000:   Batch Loss = 6.855417, Accuracy = 0.631999969482\n",
      "Performance on test set: Batch Loss = 6.83927822113, Accuracy = 0.635999977589\n",
      "Training epochs #3376000:   Batch Loss = 6.767281, Accuracy = 0.630999982357\n",
      "Performance on test set: Batch Loss = 6.65553236008, Accuracy = 0.65600001812\n",
      "Training epochs #3378000:   Batch Loss = 6.652204, Accuracy = 0.625000059605\n",
      "Performance on test set: Batch Loss = 6.54729938507, Accuracy = 0.656999945641\n",
      "Training epochs #3380000:   Batch Loss = 6.546910, Accuracy = 0.646000027657\n",
      "Performance on test set: Batch Loss = 6.59135055542, Accuracy = 0.625\n",
      "Training epochs #3382000:   Batch Loss = 6.585718, Accuracy = 0.631999969482\n",
      "Performance on test set: Batch Loss = 6.51326465607, Accuracy = 0.656999945641\n",
      "Training epochs #3384000:   Batch Loss = 6.519937, Accuracy = 0.638999998569\n",
      "Performance on test set: Batch Loss = 6.55485963821, Accuracy = 0.637000024319\n",
      "Training epochs #3386000:   Batch Loss = 6.602357, Accuracy = 0.633999943733\n",
      "Performance on test set: Batch Loss = 6.50358200073, Accuracy = 0.666000008583\n",
      "Training epochs #3388000:   Batch Loss = 6.586393, Accuracy = 0.633000016212\n",
      "Performance on test set: Batch Loss = 6.59159088135, Accuracy = 0.631999969482\n",
      "Training epochs #3390000:   Batch Loss = 6.561351, Accuracy = 0.638000011444\n",
      "Performance on test set: Batch Loss = 6.50633239746, Accuracy = 0.660000026226\n",
      "Training epochs #3392000:   Batch Loss = 6.529386, Accuracy = 0.643000006676\n",
      "Performance on test set: Batch Loss = 6.58683681488, Accuracy = 0.620000004768\n",
      "Training epochs #3394000:   Batch Loss = 6.511721, Accuracy = 0.643000006676\n",
      "Performance on test set: Batch Loss = 6.57310009003, Accuracy = 0.636000037193\n",
      "Training epochs #3396000:   Batch Loss = 6.494493, Accuracy = 0.651000022888\n",
      "Performance on test set: Batch Loss = 6.44758033752, Accuracy = 0.655999958515\n",
      "Training epochs #3398000:   Batch Loss = 6.496073, Accuracy = 0.660000026226\n",
      "Performance on test set: Batch Loss = 6.47798109055, Accuracy = 0.656999945641\n",
      "Training epochs #3400000:   Batch Loss = 6.437326, Accuracy = 0.670000016689\n",
      "Performance on test set: Batch Loss = 6.54008626938, Accuracy = 0.625\n",
      "Training epochs #3402000:   Batch Loss = 6.477128, Accuracy = 0.65499997139\n",
      "Performance on test set: Batch Loss = 6.47274780273, Accuracy = 0.656999945641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #3404000:   Batch Loss = 6.497344, Accuracy = 0.648000001907\n",
      "Performance on test set: Batch Loss = 6.49949169159, Accuracy = 0.637000024319\n",
      "Training epochs #3406000:   Batch Loss = 6.465716, Accuracy = 0.644999980927\n",
      "Performance on test set: Batch Loss = 6.4406080246, Accuracy = 0.666000068188\n",
      "Training epochs #3408000:   Batch Loss = 6.510890, Accuracy = 0.629999995232\n",
      "Performance on test set: Batch Loss = 6.52447748184, Accuracy = 0.631999969482\n",
      "Training epochs #3410000:   Batch Loss = 6.480069, Accuracy = 0.637000024319\n",
      "Performance on test set: Batch Loss = 6.43829154968, Accuracy = 0.660000026226\n",
      "Training epochs #3412000:   Batch Loss = 6.475456, Accuracy = 0.651999950409\n",
      "Performance on test set: Batch Loss = 6.53196573257, Accuracy = 0.620000004768\n",
      "Training epochs #3414000:   Batch Loss = 6.507636, Accuracy = 0.632000088692\n",
      "Performance on test set: Batch Loss = 6.50039720535, Accuracy = 0.635999977589\n",
      "Training epochs #3416000:   Batch Loss = 6.420409, Accuracy = 0.659999966621\n",
      "Performance on test set: Batch Loss = 6.4132642746, Accuracy = 0.65600001812\n",
      "Training epochs #3418000:   Batch Loss = 6.415333, Accuracy = 0.661000013351\n",
      "Performance on test set: Batch Loss = 6.43163776398, Accuracy = 0.656999945641\n",
      "Training epochs #3420000:   Batch Loss = 6.445718, Accuracy = 0.636999964714\n",
      "Performance on test set: Batch Loss = 6.49775886536, Accuracy = 0.625\n",
      "Training epochs #3422000:   Batch Loss = 6.367371, Accuracy = 0.679000020027\n",
      "Performance on test set: Batch Loss = 6.43566560745, Accuracy = 0.656999945641\n",
      "Training epochs #3424000:   Batch Loss = 6.465677, Accuracy = 0.634000003338\n",
      "Performance on test set: Batch Loss = 6.46487236023, Accuracy = 0.637000024319\n",
      "Training epochs #3426000:   Batch Loss = 6.452147, Accuracy = 0.647000014782\n",
      "Performance on test set: Batch Loss = 6.40686559677, Accuracy = 0.666000008583\n",
      "Training epochs #3428000:   Batch Loss = 6.412826, Accuracy = 0.660999953747\n",
      "Performance on test set: Batch Loss = 6.48794174194, Accuracy = 0.631999969482\n",
      "Training epochs #3430000:   Batch Loss = 6.501480, Accuracy = 0.629999995232\n",
      "Performance on test set: Batch Loss = 6.39951515198, Accuracy = 0.660000026226\n",
      "Training epochs #3432000:   Batch Loss = 6.425012, Accuracy = 0.634000003338\n",
      "Performance on test set: Batch Loss = 6.50050163269, Accuracy = 0.620000004768\n",
      "Training epochs #3434000:   Batch Loss = 6.393076, Accuracy = 0.668999969959\n",
      "Performance on test set: Batch Loss = 6.47193002701, Accuracy = 0.636000037193\n",
      "Training epochs #3436000:   Batch Loss = 6.475023, Accuracy = 0.619000017643\n",
      "Performance on test set: Batch Loss = 6.38269805908, Accuracy = 0.65600001812\n",
      "Training epochs #3438000:   Batch Loss = 6.401352, Accuracy = 0.663999974728\n",
      "Performance on test set: Batch Loss = 6.40290737152, Accuracy = 0.656999945641\n",
      "Training epochs #3440000:   Batch Loss = 6.426904, Accuracy = 0.644999980927\n",
      "Performance on test set: Batch Loss = 6.46820354462, Accuracy = 0.624999940395\n",
      "Training epochs #3442000:   Batch Loss = 6.412571, Accuracy = 0.65600001812\n",
      "Performance on test set: Batch Loss = 6.41041326523, Accuracy = 0.657000005245\n",
      "Training epochs #3444000:   Batch Loss = 6.395317, Accuracy = 0.648000001907\n",
      "Performance on test set: Batch Loss = 6.44210147858, Accuracy = 0.636999964714\n",
      "Training epochs #3446000:   Batch Loss = 6.477538, Accuracy = 0.618000030518\n",
      "Performance on test set: Batch Loss = 6.38218641281, Accuracy = 0.666000008583\n",
      "Training epochs #3448000:   Batch Loss = 6.461971, Accuracy = 0.615000009537\n",
      "Performance on test set: Batch Loss = 6.4578499794, Accuracy = 0.631999969482\n",
      "Training epochs #3450000:   Batch Loss = 6.419299, Accuracy = 0.648000001907\n",
      "Performance on test set: Batch Loss = 6.37490940094, Accuracy = 0.660000026226\n",
      "Training epochs #3452000:   Batch Loss = 6.426541, Accuracy = 0.636000037193\n",
      "Performance on test set: Batch Loss = 6.47115516663, Accuracy = 0.621000051498\n",
      "Training epochs #3454000:   Batch Loss = 6.456168, Accuracy = 0.632000029087\n",
      "Performance on test set: Batch Loss = 6.44387578964, Accuracy = 0.637000024319\n",
      "Training epochs #3456000:   Batch Loss = 6.442977, Accuracy = 0.630999922752\n",
      "Performance on test set: Batch Loss = 6.3534154892, Accuracy = 0.657000005245\n",
      "Training epochs #3458000:   Batch Loss = 6.456394, Accuracy = 0.625\n",
      "Performance on test set: Batch Loss = 6.37516021729, Accuracy = 0.657000005245\n",
      "Training epochs #3460000:   Batch Loss = 6.391252, Accuracy = 0.646000027657\n",
      "Performance on test set: Batch Loss = 6.43774795532, Accuracy = 0.625\n",
      "Training epochs #3462000:   Batch Loss = 6.429013, Accuracy = 0.633000016212\n",
      "Performance on test set: Batch Loss = 6.3826546669, Accuracy = 0.657000005245\n",
      "Training epochs #3464000:   Batch Loss = 6.393771, Accuracy = 0.638999938965\n",
      "Performance on test set: Batch Loss = 6.41208410263, Accuracy = 0.637000024319\n",
      "Training epochs #3466000:   Batch Loss = 6.444321, Accuracy = 0.633999943733\n",
      "Performance on test set: Batch Loss = 6.35160827637, Accuracy = 0.666999995708\n",
      "Training epochs #3468000:   Batch Loss = 6.418633, Accuracy = 0.633000016212\n",
      "Performance on test set: Batch Loss = 6.42391586304, Accuracy = 0.633000016212\n",
      "Training epochs #3470000:   Batch Loss = 6.395139, Accuracy = 0.638000011444\n",
      "Performance on test set: Batch Loss = 6.34139060974, Accuracy = 0.664000034332\n",
      "Training epochs #3472000:   Batch Loss = 6.363833, Accuracy = 0.648999989033\n",
      "Performance on test set: Batch Loss = 6.4224023819, Accuracy = 0.741000056267\n",
      "Training epochs #3474000:   Batch Loss = 6.353242, Accuracy = 0.752000033855\n",
      "Performance on test set: Batch Loss = 6.39626264572, Accuracy = 0.742999970913\n",
      "Training epochs #3476000:   Batch Loss = 6.335881, Accuracy = 0.65700006485\n",
      "Performance on test set: Batch Loss = 6.29923009872, Accuracy = 0.668000042439\n",
      "Training epochs #3478000:   Batch Loss = 6.356705, Accuracy = 0.666000008583\n",
      "Performance on test set: Batch Loss = 6.32397699356, Accuracy = 0.657999992371\n",
      "Training epochs #3480000:   Batch Loss = 6.277740, Accuracy = 0.776000022888\n",
      "Performance on test set: Batch Loss = 6.37493944168, Accuracy = 0.738000094891\n",
      "Training epochs #3482000:   Batch Loss = 6.322242, Accuracy = 0.769999980927\n",
      "Performance on test set: Batch Loss = 6.32343816757, Accuracy = 0.757000029087\n",
      "Training epochs #3484000:   Batch Loss = 6.344211, Accuracy = 0.654000043869\n",
      "Performance on test set: Batch Loss = 6.33253574371, Accuracy = 0.756999969482\n",
      "Training epochs #3486000:   Batch Loss = 6.284780, Accuracy = 0.767000079155\n",
      "Performance on test set: Batch Loss = 6.27927780151, Accuracy = 0.76599997282\n",
      "Training epochs #3488000:   Batch Loss = 6.330838, Accuracy = 0.741000056267\n",
      "Performance on test set: Batch Loss = 6.31820869446, Accuracy = 0.746999979019\n",
      "Training epochs #3490000:   Batch Loss = 6.271557, Accuracy = 0.760000050068\n",
      "Performance on test set: Batch Loss = 6.23466157913, Accuracy = 0.773999929428\n",
      "Training epochs #3492000:   Batch Loss = 6.279149, Accuracy = 0.739000022411\n",
      "Performance on test set: Batch Loss = 6.28751754761, Accuracy = 0.741999983788\n",
      "Training epochs #3494000:   Batch Loss = 6.241194, Accuracy = 0.752999961376\n",
      "Performance on test set: Batch Loss = 6.26853084564, Accuracy = 0.742999970913\n",
      "Training epochs #3496000:   Batch Loss = 6.184973, Accuracy = 0.764999985695\n",
      "Performance on test set: Batch Loss = 6.14306974411, Accuracy = 0.78200006485\n",
      "Training epochs #3498000:   Batch Loss = 6.154369, Accuracy = 0.773999989033\n",
      "Performance on test set: Batch Loss = 6.16265106201, Accuracy = 0.76700001955\n",
      "Training epochs #3500000:   Batch Loss = 6.178960, Accuracy = 0.759000003338\n",
      "Performance on test set: Batch Loss = 6.22644233704, Accuracy = 0.738000035286\n",
      "Training epochs #3502000:   Batch Loss = 6.092377, Accuracy = 0.787999987602\n",
      "Performance on test set: Batch Loss = 6.18717384338, Accuracy = 0.756000041962\n",
      "Training epochs #3504000:   Batch Loss = 6.195597, Accuracy = 0.745999991894\n",
      "Performance on test set: Batch Loss = 6.17855358124, Accuracy = 0.759999990463\n",
      "Training epochs #3506000:   Batch Loss = 6.167552, Accuracy = 0.754000008106\n",
      "Performance on test set: Batch Loss = 6.14511156082, Accuracy = 0.76599997282\n",
      "Training epochs #3508000:   Batch Loss = 6.128356, Accuracy = 0.76599997282\n",
      "Performance on test set: Batch Loss = 6.17784357071, Accuracy = 0.746999979019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #3510000:   Batch Loss = 6.241834, Accuracy = 0.736000061035\n",
      "Performance on test set: Batch Loss = 6.10076713562, Accuracy = 0.773999989033\n",
      "Training epochs #3512000:   Batch Loss = 6.123258, Accuracy = 0.761000037193\n",
      "Performance on test set: Batch Loss = 6.18947887421, Accuracy = 0.745000004768\n",
      "Training epochs #3514000:   Batch Loss = 6.124400, Accuracy = 0.772000014782\n",
      "Performance on test set: Batch Loss = 6.1753821373, Accuracy = 0.743000030518\n",
      "Training epochs #3516000:   Batch Loss = 6.176326, Accuracy = 0.75100004673\n",
      "Performance on test set: Batch Loss = 6.08600521088, Accuracy = 0.778999984264\n",
      "Training epochs #3518000:   Batch Loss = 6.137682, Accuracy = 0.763000011444\n",
      "Performance on test set: Batch Loss = 6.13006591797, Accuracy = 0.76599997282\n",
      "Training epochs #3520000:   Batch Loss = 6.151087, Accuracy = 0.751999974251\n",
      "Performance on test set: Batch Loss = 6.11308479309, Accuracy = 0.737000048161\n",
      "Training epochs #3522000:   Batch Loss = 6.356792, Accuracy = 0.773000061512\n",
      "Performance on test set: Batch Loss = 6.45952796936, Accuracy = 0.657999932766\n",
      "Training epochs #3524000:   Batch Loss = 6.543259, Accuracy = 0.648999989033\n",
      "Performance on test set: Batch Loss = 6.60412120819, Accuracy = 0.638000011444\n",
      "Training epochs #3526000:   Batch Loss = 6.575085, Accuracy = 0.618999958038\n",
      "Performance on test set: Batch Loss = 6.31539773941, Accuracy = 0.7009999156\n",
      "Training epochs #3528000:   Batch Loss = 6.955329, Accuracy = 0.0569999963045\n",
      "Performance on test set: Batch Loss = 6.65228557587, Accuracy = 0.631999969482\n",
      "Training epochs #3530000:   Batch Loss = 6.641096, Accuracy = 0.648000001907\n",
      "Performance on test set: Batch Loss = 6.61016273499, Accuracy = 0.660000026226\n",
      "Training epochs #3532000:   Batch Loss = 6.678064, Accuracy = 0.636999964714\n",
      "Performance on test set: Batch Loss = 6.75036811829, Accuracy = 0.620999932289\n",
      "Training epochs #3534000:   Batch Loss = 6.675241, Accuracy = 0.633000016212\n",
      "Performance on test set: Batch Loss = 6.63915491104, Accuracy = 0.636999964714\n",
      "Training epochs #3536000:   Batch Loss = 6.623630, Accuracy = 0.631999969482\n",
      "Performance on test set: Batch Loss = 6.54211759567, Accuracy = 0.657999992371\n",
      "Training epochs #3538000:   Batch Loss = 6.590917, Accuracy = 0.627000033855\n",
      "Performance on test set: Batch Loss = 6.49323892593, Accuracy = 0.657999992371\n",
      "Training epochs #3540000:   Batch Loss = 6.503635, Accuracy = 0.646000027657\n",
      "Performance on test set: Batch Loss = 6.51338720322, Accuracy = 0.625\n",
      "Training epochs #3542000:   Batch Loss = 6.496693, Accuracy = 0.631999969482\n",
      "Performance on test set: Batch Loss = 6.43118476868, Accuracy = 0.657999992371\n",
      "Training epochs #3544000:   Batch Loss = 6.431993, Accuracy = 0.638999938965\n",
      "Performance on test set: Batch Loss = 6.44634008408, Accuracy = 0.638999998569\n",
      "Training epochs #3546000:   Batch Loss = 6.459468, Accuracy = 0.635000050068\n",
      "Performance on test set: Batch Loss = 6.37563896179, Accuracy = 0.666000008583\n",
      "Training epochs #3548000:   Batch Loss = 6.422063, Accuracy = 0.634000003338\n",
      "Performance on test set: Batch Loss = 6.42562866211, Accuracy = 0.631999969482\n",
      "Training epochs #3550000:   Batch Loss = 6.403172, Accuracy = 0.638000011444\n",
      "Performance on test set: Batch Loss = 6.35050487518, Accuracy = 0.660000026226\n",
      "Training epochs #3552000:   Batch Loss = 6.370752, Accuracy = 0.643999993801\n",
      "Performance on test set: Batch Loss = 6.43599510193, Accuracy = 0.620999991894\n",
      "Training epochs #3554000:   Batch Loss = 6.381645, Accuracy = 0.642999947071\n",
      "Performance on test set: Batch Loss = 6.42326259613, Accuracy = 0.637000024319\n",
      "Training epochs #3556000:   Batch Loss = 6.362779, Accuracy = 0.650999963284\n",
      "Performance on test set: Batch Loss = 6.32196998596, Accuracy = 0.657999992371\n",
      "Training epochs #3558000:   Batch Loss = 6.374235, Accuracy = 0.659999966621\n",
      "Performance on test set: Batch Loss = 6.35324478149, Accuracy = 0.657999992371\n",
      "Training epochs #3560000:   Batch Loss = 6.308122, Accuracy = 0.671000003815\n",
      "Performance on test set: Batch Loss = 6.42751598358, Accuracy = 0.625\n",
      "Training epochs #3562000:   Batch Loss = 6.361659, Accuracy = 0.653999984264\n",
      "Performance on test set: Batch Loss = 6.36482620239, Accuracy = 0.657000005245\n",
      "Training epochs #3564000:   Batch Loss = 6.381512, Accuracy = 0.648000001907\n",
      "Performance on test set: Batch Loss = 6.38281774521, Accuracy = 0.638000011444\n",
      "Training epochs #3566000:   Batch Loss = 6.347809, Accuracy = 0.644999980927\n",
      "Performance on test set: Batch Loss = 6.32396221161, Accuracy = 0.666999995708\n",
      "Training epochs #3568000:   Batch Loss = 6.395669, Accuracy = 0.631000041962\n",
      "Performance on test set: Batch Loss = 6.39479494095, Accuracy = 0.631999969482\n",
      "Training epochs #3570000:   Batch Loss = 6.359201, Accuracy = 0.637999951839\n",
      "Performance on test set: Batch Loss = 6.31276130676, Accuracy = 0.660000026226\n",
      "Training epochs #3572000:   Batch Loss = 6.365931, Accuracy = 0.652999937534\n",
      "Performance on test set: Batch Loss = 6.40437984467, Accuracy = 0.621999979019\n",
      "Training epochs #3574000:   Batch Loss = 6.380199, Accuracy = 0.633000075817\n",
      "Performance on test set: Batch Loss = 6.38195943832, Accuracy = 0.638000011444\n",
      "Training epochs #3576000:   Batch Loss = 6.302863, Accuracy = 0.659999966621\n",
      "Performance on test set: Batch Loss = 6.29072570801, Accuracy = 0.659000039101\n",
      "Training epochs #3578000:   Batch Loss = 6.296013, Accuracy = 0.662000000477\n",
      "Performance on test set: Batch Loss = 6.31181764603, Accuracy = 0.657999992371\n",
      "Training epochs #3580000:   Batch Loss = 6.327961, Accuracy = 0.636999964714\n",
      "Performance on test set: Batch Loss = 6.36833429337, Accuracy = 0.625\n",
      "Training epochs #3582000:   Batch Loss = 6.249233, Accuracy = 0.680999934673\n",
      "Performance on test set: Batch Loss = 6.31837892532, Accuracy = 0.657999992371\n",
      "Training epochs #3584000:   Batch Loss = 6.342677, Accuracy = 0.634000003338\n",
      "Performance on test set: Batch Loss = 6.3433713913, Accuracy = 0.639000058174\n",
      "Training epochs #3586000:   Batch Loss = 6.326211, Accuracy = 0.647999942303\n",
      "Performance on test set: Batch Loss = 6.28819036484, Accuracy = 0.666999995708\n",
      "Training epochs #3588000:   Batch Loss = 6.291970, Accuracy = 0.661000013351\n",
      "Performance on test set: Batch Loss = 6.35266113281, Accuracy = 0.631999969482\n",
      "Training epochs #3590000:   Batch Loss = 6.381199, Accuracy = 0.630999982357\n",
      "Performance on test set: Batch Loss = 6.27867603302, Accuracy = 0.659999966621\n",
      "Training epochs #3592000:   Batch Loss = 6.292950, Accuracy = 0.634000003338\n",
      "Performance on test set: Batch Loss = 6.37140989304, Accuracy = 0.621999979019\n",
      "Training epochs #3594000:   Batch Loss = 6.266989, Accuracy = 0.671000003815\n",
      "Performance on test set: Batch Loss = 6.34732532501, Accuracy = 0.638000011444\n",
      "Training epochs #3596000:   Batch Loss = 6.349411, Accuracy = 0.621999979019\n",
      "Performance on test set: Batch Loss = 6.25095558167, Accuracy = 0.658999979496\n",
      "Training epochs #3598000:   Batch Loss = 6.275371, Accuracy = 0.668999969959\n",
      "Performance on test set: Batch Loss = 6.27291154861, Accuracy = 0.657999992371\n",
      "Training epochs #3600000:   Batch Loss = 6.296506, Accuracy = 0.646999955177\n",
      "Performance on test set: Batch Loss = 6.33165502548, Accuracy = 0.632000029087\n",
      "Training epochs #3602000:   Batch Loss = 6.282667, Accuracy = 0.661000013351\n",
      "Performance on test set: Batch Loss = 6.28372049332, Accuracy = 0.659000039101\n",
      "Training epochs #3604000:   Batch Loss = 6.254546, Accuracy = 0.649999976158\n",
      "Performance on test set: Batch Loss = 6.30447101593, Accuracy = 0.641999959946\n",
      "Training epochs #3606000:   Batch Loss = 6.342098, Accuracy = 0.626999974251\n",
      "Performance on test set: Batch Loss = 6.25083732605, Accuracy = 0.675999999046\n",
      "Training epochs #3608000:   Batch Loss = 6.322530, Accuracy = 0.622999966145\n",
      "Performance on test set: Batch Loss = 6.31018829346, Accuracy = 0.634999990463\n",
      "Training epochs #3610000:   Batch Loss = 6.281964, Accuracy = 0.650999963284\n",
      "Performance on test set: Batch Loss = 6.23954439163, Accuracy = 0.664000034332\n",
      "Training epochs #3612000:   Batch Loss = 6.284425, Accuracy = 0.643000006676\n",
      "Performance on test set: Batch Loss = 6.32593727112, Accuracy = 0.625\n",
      "Training epochs #3614000:   Batch Loss = 6.313451, Accuracy = 0.635999977589\n",
      "Performance on test set: Batch Loss = 6.3064365387, Accuracy = 0.642000079155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #3616000:   Batch Loss = 6.299277, Accuracy = 0.635000050068\n",
      "Performance on test set: Batch Loss = 6.206823349, Accuracy = 0.669000029564\n",
      "Training epochs #3618000:   Batch Loss = 6.310573, Accuracy = 0.630999982357\n",
      "Performance on test set: Batch Loss = 6.22842884064, Accuracy = 0.657999992371\n",
      "Training epochs #3620000:   Batch Loss = 6.247920, Accuracy = 0.650000035763\n",
      "Performance on test set: Batch Loss = 6.28454971313, Accuracy = 0.632000029087\n",
      "Training epochs #3622000:   Batch Loss = 6.281906, Accuracy = 0.637999951839\n",
      "Performance on test set: Batch Loss = 6.24127197266, Accuracy = 0.658000051975\n",
      "Training epochs #3624000:   Batch Loss = 6.236947, Accuracy = 0.639999985695\n",
      "Performance on test set: Batch Loss = 6.25924110413, Accuracy = 0.64099997282\n",
      "Training epochs #3626000:   Batch Loss = 6.295032, Accuracy = 0.636999964714\n",
      "Performance on test set: Batch Loss = 6.20536756516, Accuracy = 0.675999999046\n",
      "Training epochs #3628000:   Batch Loss = 6.265396, Accuracy = 0.736000001431\n",
      "Performance on test set: Batch Loss = 6.25907993317, Accuracy = 0.749000012875\n",
      "Training epochs #3630000:   Batch Loss = 6.244145, Accuracy = 0.754000067711\n",
      "Performance on test set: Batch Loss = 6.19383478165, Accuracy = 0.774999976158\n",
      "Training epochs #3632000:   Batch Loss = 6.217420, Accuracy = 0.752999961376\n",
      "Performance on test set: Batch Loss = 6.27494335175, Accuracy = 0.741999983788\n",
      "Training epochs #3634000:   Batch Loss = 6.221649, Accuracy = 0.753000020981\n",
      "Performance on test set: Batch Loss = 6.25902795792, Accuracy = 0.743000030518\n",
      "Training epochs #3636000:   Batch Loss = 6.196785, Accuracy = 0.7650000453\n",
      "Performance on test set: Batch Loss = 6.15520715714, Accuracy = 0.782000005245\n",
      "Training epochs #3638000:   Batch Loss = 6.206472, Accuracy = 0.756999969482\n",
      "Performance on test set: Batch Loss = 6.17504024506, Accuracy = 0.76800006628\n",
      "Training epochs #3640000:   Batch Loss = 6.134840, Accuracy = 0.779000043869\n",
      "Performance on test set: Batch Loss = 6.23237657547, Accuracy = 0.740000009537\n",
      "Training epochs #3642000:   Batch Loss = 6.180974, Accuracy = 0.768999993801\n",
      "Performance on test set: Batch Loss = 6.1924700737, Accuracy = 0.758000016212\n",
      "Training epochs #3644000:   Batch Loss = 6.200919, Accuracy = 0.762000083923\n",
      "Performance on test set: Batch Loss = 6.20550823212, Accuracy = 0.762000024319\n",
      "Training epochs #3646000:   Batch Loss = 6.159947, Accuracy = 0.766999959946\n",
      "Performance on test set: Batch Loss = 6.14955759048, Accuracy = 0.766999959946\n",
      "Training epochs #3648000:   Batch Loss = 6.205555, Accuracy = 0.741000056267\n",
      "Performance on test set: Batch Loss = 6.20121622086, Accuracy = 0.749000012875\n",
      "Training epochs #3650000:   Batch Loss = 6.175623, Accuracy = 0.759999990463\n",
      "Performance on test set: Batch Loss = 6.13759946823, Accuracy = 0.774999976158\n",
      "Training epochs #3652000:   Batch Loss = 6.195259, Accuracy = 0.740999996662\n",
      "Performance on test set: Batch Loss = 6.21301269531, Accuracy = 0.743000030518\n",
      "Training epochs #3654000:   Batch Loss = 6.187029, Accuracy = 0.754000008106\n",
      "Performance on test set: Batch Loss = 6.20256567001, Accuracy = 0.743999958038\n",
      "Training epochs #3656000:   Batch Loss = 6.120494, Accuracy = 0.763999998569\n",
      "Performance on test set: Batch Loss = 6.09392595291, Accuracy = 0.78300011158\n",
      "Training epochs #3658000:   Batch Loss = 6.101155, Accuracy = 0.773999989033\n",
      "Performance on test set: Batch Loss = 6.10941982269, Accuracy = 0.768000006676\n",
      "Training epochs #3660000:   Batch Loss = 6.119751, Accuracy = 0.759000062943\n",
      "Performance on test set: Batch Loss = 6.16860771179, Accuracy = 0.740000009537\n",
      "Training epochs #3662000:   Batch Loss = 6.046170, Accuracy = 0.788999974728\n",
      "Performance on test set: Batch Loss = 6.12958526611, Accuracy = 0.757000029087\n",
      "Training epochs #3664000:   Batch Loss = 6.137311, Accuracy = 0.745000004768\n",
      "Performance on test set: Batch Loss = 6.14215326309, Accuracy = 0.763000011444\n",
      "Training epochs #3666000:   Batch Loss = 6.130085, Accuracy = 0.755999982357\n",
      "Performance on test set: Batch Loss = 6.08478832245, Accuracy = 0.766000032425\n",
      "Training epochs #3668000:   Batch Loss = 6.081540, Accuracy = 0.766000032425\n",
      "Performance on test set: Batch Loss = 6.1261548996, Accuracy = 0.74899995327\n",
      "Training epochs #3670000:   Batch Loss = 6.177240, Accuracy = 0.736000061035\n",
      "Performance on test set: Batch Loss = 6.06800365448, Accuracy = 0.774999976158\n",
      "Training epochs #3672000:   Batch Loss = 6.063135, Accuracy = 0.761999964714\n",
      "Performance on test set: Batch Loss = 6.13996219635, Accuracy = 0.743999958038\n",
      "Training epochs #3674000:   Batch Loss = 6.052856, Accuracy = 0.772000014782\n",
      "Performance on test set: Batch Loss = 6.12925243378, Accuracy = 0.743999958038\n",
      "Training epochs #3676000:   Batch Loss = 6.116601, Accuracy = 0.752000033855\n",
      "Performance on test set: Batch Loss = 6.02196741104, Accuracy = 0.78200006485\n",
      "Training epochs #3678000:   Batch Loss = 6.039998, Accuracy = 0.7650000453\n",
      "Performance on test set: Batch Loss = 6.03187799454, Accuracy = 0.768000006676\n",
      "Training epochs #3680000:   Batch Loss = 6.067344, Accuracy = 0.750999987125\n",
      "Performance on test set: Batch Loss = 6.08606290817, Accuracy = 0.740000009537\n",
      "Training epochs #3682000:   Batch Loss = 6.042195, Accuracy = 0.764999985695\n",
      "Performance on test set: Batch Loss = 6.05335474014, Accuracy = 0.757000029087\n",
      "Training epochs #3684000:   Batch Loss = 5.994726, Accuracy = 0.776999950409\n",
      "Performance on test set: Batch Loss = 6.06138515472, Accuracy = 0.761000037193\n",
      "Training epochs #3686000:   Batch Loss = 6.084956, Accuracy = 0.729000031948\n",
      "Performance on test set: Batch Loss = 5.99749803543, Accuracy = 0.76599997282\n",
      "Training epochs #3688000:   Batch Loss = 6.053707, Accuracy = 0.739999949932\n",
      "Performance on test set: Batch Loss = 6.02894210815, Accuracy = 0.750000059605\n",
      "Training epochs #3690000:   Batch Loss = 6.031833, Accuracy = 0.758000016212\n",
      "Performance on test set: Batch Loss = 5.97698497772, Accuracy = 0.774000048637\n",
      "Training epochs #3692000:   Batch Loss = 6.000474, Accuracy = 0.758000016212\n",
      "Performance on test set: Batch Loss = 6.03078079224, Accuracy = 0.745000004768\n",
      "Training epochs #3694000:   Batch Loss = 6.017299, Accuracy = 0.741000056267\n",
      "Performance on test set: Batch Loss = 6.02824640274, Accuracy = 0.743999958038\n",
      "Training epochs #3696000:   Batch Loss = 6.013398, Accuracy = 0.736999988556\n",
      "Performance on test set: Batch Loss = 5.92522621155, Accuracy = 0.780000090599\n",
      "Training epochs #3698000:   Batch Loss = 6.021706, Accuracy = 0.734000086784\n",
      "Performance on test set: Batch Loss = 5.93243551254, Accuracy = 0.768000006676\n",
      "Training epochs #3700000:   Batch Loss = 5.965234, Accuracy = 0.760999977589\n",
      "Performance on test set: Batch Loss = 5.96681547165, Accuracy = 0.739999949932\n",
      "Training epochs #3702000:   Batch Loss = 5.984806, Accuracy = 0.743000030518\n",
      "Performance on test set: Batch Loss = 5.947057724, Accuracy = 0.758000016212\n",
      "Training epochs #3704000:   Batch Loss = 5.919450, Accuracy = 0.752999961376\n",
      "Performance on test set: Batch Loss = 5.96296024323, Accuracy = 0.762000083923\n",
      "Training epochs #3706000:   Batch Loss = 5.996788, Accuracy = 0.739999949932\n",
      "Performance on test set: Batch Loss = 5.89766693115, Accuracy = 0.764999985695\n",
      "Training epochs #3708000:   Batch Loss = 5.961180, Accuracy = 0.739000022411\n",
      "Performance on test set: Batch Loss = 5.9288392067, Accuracy = 0.749000012875\n",
      "Training epochs #3710000:   Batch Loss = 5.923318, Accuracy = 0.755000054836\n",
      "Performance on test set: Batch Loss = 5.88832902908, Accuracy = 0.773000001907\n",
      "Training epochs #3712000:   Batch Loss = 5.920468, Accuracy = 0.752000033855\n",
      "Performance on test set: Batch Loss = 5.94548511505, Accuracy = 0.742999970913\n",
      "Training epochs #3714000:   Batch Loss = 5.904322, Accuracy = 0.755000054836\n",
      "Performance on test set: Batch Loss = 5.94740343094, Accuracy = 0.745000004768\n",
      "Training epochs #3716000:   Batch Loss = 5.886117, Accuracy = 0.764999985695\n",
      "Performance on test set: Batch Loss = 5.86295413971, Accuracy = 0.780000030994\n",
      "Training epochs #3718000:   Batch Loss = 5.895956, Accuracy = 0.759000003338\n",
      "Performance on test set: Batch Loss = 5.8677573204, Accuracy = 0.76599997282\n",
      "Training epochs #3720000:   Batch Loss = 5.829142, Accuracy = 0.776999950409\n",
      "Performance on test set: Batch Loss = 5.90185928345, Accuracy = 0.740000069141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #3722000:   Batch Loss = 5.868683, Accuracy = 0.771000027657\n",
      "Performance on test set: Batch Loss = 5.89266443253, Accuracy = 0.759000062943\n",
      "Training epochs #3724000:   Batch Loss = 5.912385, Accuracy = 0.764000058174\n",
      "Performance on test set: Batch Loss = 5.96633243561, Accuracy = 0.762000083923\n",
      "Training epochs #3726000:   Batch Loss = 5.844086, Accuracy = 0.768000006676\n",
      "Performance on test set: Batch Loss = 5.8432226181, Accuracy = 0.764999985695\n",
      "Training epochs #3728000:   Batch Loss = 6.420796, Accuracy = 0.744000017643\n",
      "Performance on test set: Batch Loss = 6.34822368622, Accuracy = 0.753000020981\n",
      "Training epochs #3730000:   Batch Loss = 6.284591, Accuracy = 0.760999977589\n",
      "Performance on test set: Batch Loss = 6.19724225998, Accuracy = 0.774999976158\n",
      "Training epochs #3732000:   Batch Loss = 6.192537, Accuracy = 0.739000022411\n",
      "Performance on test set: Batch Loss = 6.17509222031, Accuracy = 0.744000017643\n",
      "Training epochs #3734000:   Batch Loss = 6.160192, Accuracy = 0.755000054836\n",
      "Performance on test set: Batch Loss = 6.17587041855, Accuracy = 0.743999958038\n",
      "Training epochs #3736000:   Batch Loss = 6.052415, Accuracy = 0.764000058174\n",
      "Performance on test set: Batch Loss = 5.99212217331, Accuracy = 0.78200006485\n",
      "Training epochs #3738000:   Batch Loss = 5.983954, Accuracy = 0.773999989033\n",
      "Performance on test set: Batch Loss = 5.97738361359, Accuracy = 0.76599997282\n",
      "Training epochs #3740000:   Batch Loss = 5.994631, Accuracy = 0.757000029087\n",
      "Performance on test set: Batch Loss = 6.0833697319, Accuracy = 0.739000022411\n",
      "Training epochs #3742000:   Batch Loss = 5.983363, Accuracy = 0.789999961853\n",
      "Performance on test set: Batch Loss = 6.04057168961, Accuracy = 0.758000075817\n",
      "Training epochs #3744000:   Batch Loss = 6.017983, Accuracy = 0.746000051498\n",
      "Performance on test set: Batch Loss = 6.00691604614, Accuracy = 0.763000011444\n",
      "Training epochs #3746000:   Batch Loss = 6.031624, Accuracy = 0.755999982357\n",
      "Performance on test set: Batch Loss = 5.98862695694, Accuracy = 0.763999998569\n",
      "Training epochs #3748000:   Batch Loss = 5.984458, Accuracy = 0.76800006628\n",
      "Performance on test set: Batch Loss = 6.03535175323, Accuracy = 0.748000025749\n",
      "Training epochs #3750000:   Batch Loss = 6.111428, Accuracy = 0.737999916077\n",
      "Performance on test set: Batch Loss = 5.96205472946, Accuracy = 0.775000035763\n",
      "Training epochs #3752000:   Batch Loss = 5.970363, Accuracy = 0.762999951839\n",
      "Performance on test set: Batch Loss = 6.07425403595, Accuracy = 0.752000033855\n",
      "Training epochs #3754000:   Batch Loss = 5.965914, Accuracy = 0.770999968052\n",
      "Performance on test set: Batch Loss = 6.03792667389, Accuracy = 0.744999945164\n",
      "Training epochs #3756000:   Batch Loss = 6.040028, Accuracy = 0.752000033855\n",
      "Performance on test set: Batch Loss = 5.96512746811, Accuracy = 0.778999984264\n",
      "Training epochs #3758000:   Batch Loss = 5.978406, Accuracy = 0.763000071049\n",
      "Performance on test set: Batch Loss = 5.94521331787, Accuracy = 0.766000032425\n",
      "Training epochs #3760000:   Batch Loss = 5.990961, Accuracy = 0.75100004673\n",
      "Performance on test set: Batch Loss = 5.99719715118, Accuracy = 0.740000069141\n",
      "Training epochs #3762000:   Batch Loss = 5.951523, Accuracy = 0.767000079155\n",
      "Performance on test set: Batch Loss = 5.97014427185, Accuracy = 0.764999985695\n",
      "Training epochs #3764000:   Batch Loss = 5.919927, Accuracy = 0.778999984264\n",
      "Performance on test set: Batch Loss = 5.97051668167, Accuracy = 0.763000011444\n",
      "Training epochs #3766000:   Batch Loss = 5.983930, Accuracy = 0.728000044823\n",
      "Performance on test set: Batch Loss = 5.92086172104, Accuracy = 0.764000058174\n",
      "Training epochs #3768000:   Batch Loss = 5.990443, Accuracy = 0.740000009537\n",
      "Performance on test set: Batch Loss = 5.95684862137, Accuracy = 0.75100004673\n",
      "Training epochs #3770000:   Batch Loss = 5.971557, Accuracy = 0.756999969482\n",
      "Performance on test set: Batch Loss = 5.89664268494, Accuracy = 0.774999976158\n",
      "Training epochs #3772000:   Batch Loss = 5.952396, Accuracy = 0.757999956608\n",
      "Performance on test set: Batch Loss = 5.99466896057, Accuracy = 0.745999991894\n",
      "Training epochs #3774000:   Batch Loss = 5.948520, Accuracy = 0.742000043392\n",
      "Performance on test set: Batch Loss = 5.96261739731, Accuracy = 0.745999991894\n",
      "Training epochs #3776000:   Batch Loss = 5.973863, Accuracy = 0.738999962807\n",
      "Performance on test set: Batch Loss = 5.88097906113, Accuracy = 0.780999958515\n",
      "Training epochs #3778000:   Batch Loss = 5.967150, Accuracy = 0.73299998045\n",
      "Performance on test set: Batch Loss = 5.86034297943, Accuracy = 0.768000006676\n",
      "Training epochs #3780000:   Batch Loss = 5.897936, Accuracy = 0.761000037193\n",
      "Performance on test set: Batch Loss = 5.91237163544, Accuracy = 0.740000069141\n",
      "Training epochs #3782000:   Batch Loss = 5.917131, Accuracy = 0.745999932289\n",
      "Performance on test set: Batch Loss = 5.90614080429, Accuracy = 0.763999938965\n",
      "Training epochs #3784000:   Batch Loss = 5.876514, Accuracy = 0.755999922752\n",
      "Performance on test set: Batch Loss = 5.88296365738, Accuracy = 0.768000006676\n",
      "Training epochs #3786000:   Batch Loss = 5.937487, Accuracy = 0.757000029087\n",
      "Performance on test set: Batch Loss = 5.84020376205, Accuracy = 0.788000047207\n",
      "Training epochs #3788000:   Batch Loss = 5.884409, Accuracy = 0.771000027657\n",
      "Performance on test set: Batch Loss = 5.8837928772, Accuracy = 0.78100001812\n",
      "Training epochs #3790000:   Batch Loss = 5.880320, Accuracy = 0.78200006485\n",
      "Performance on test set: Batch Loss = 5.82542991638, Accuracy = 0.791000068188\n",
      "Training epochs #3792000:   Batch Loss = 5.854554, Accuracy = 0.78100001812\n",
      "Performance on test set: Batch Loss = 5.91032981873, Accuracy = 0.773999989033\n",
      "Training epochs #3794000:   Batch Loss = 5.862150, Accuracy = 0.789000034332\n",
      "Performance on test set: Batch Loss = 5.87050628662, Accuracy = 0.787000060081\n",
      "Training epochs #3796000:   Batch Loss = 5.834588, Accuracy = 0.795000016689\n",
      "Performance on test set: Batch Loss = 5.79283809662, Accuracy = 0.808999955654\n",
      "Training epochs #3798000:   Batch Loss = 5.823907, Accuracy = 0.795000016689\n",
      "Performance on test set: Batch Loss = 5.78821802139, Accuracy = 0.81099998951\n",
      "Training epochs #3800000:   Batch Loss = 5.759417, Accuracy = 0.810000061989\n",
      "Performance on test set: Batch Loss = 5.85425901413, Accuracy = 0.777000069618\n",
      "Training epochs #3802000:   Batch Loss = 5.821496, Accuracy = 0.798999905586\n",
      "Performance on test set: Batch Loss = 5.82802724838, Accuracy = 0.800000011921\n",
      "Training epochs #3804000:   Batch Loss = 5.847832, Accuracy = 0.792999982834\n",
      "Performance on test set: Batch Loss = 5.82722520828, Accuracy = 0.803999960423\n",
      "Training epochs #3806000:   Batch Loss = 5.780958, Accuracy = 0.797000050545\n",
      "Performance on test set: Batch Loss = 5.77253675461, Accuracy = 0.789000034332\n",
      "Training epochs #3808000:   Batch Loss = 5.829634, Accuracy = 0.774999976158\n",
      "Performance on test set: Batch Loss = 5.82312870026, Accuracy = 0.788000106812\n",
      "Training epochs #3810000:   Batch Loss = 5.813714, Accuracy = 0.799999952316\n",
      "Performance on test set: Batch Loss = 5.78325796127, Accuracy = 0.810000002384\n",
      "Training epochs #3812000:   Batch Loss = 5.826593, Accuracy = 0.794000029564\n",
      "Performance on test set: Batch Loss = 5.86740398407, Accuracy = 0.784000039101\n",
      "Training epochs #3814000:   Batch Loss = 5.814180, Accuracy = 0.794000029564\n",
      "Performance on test set: Batch Loss = 5.84079313278, Accuracy = 0.775000035763\n",
      "Training epochs #3816000:   Batch Loss = 5.774930, Accuracy = 0.820000052452\n",
      "Performance on test set: Batch Loss = 5.77364492416, Accuracy = 0.818000018597\n",
      "Training epochs #3818000:   Batch Loss = 5.772769, Accuracy = 0.806999981403\n",
      "Performance on test set: Batch Loss = 5.74141931534, Accuracy = 0.811000049114\n",
      "Training epochs #3820000:   Batch Loss = 5.762986, Accuracy = 0.792000055313\n",
      "Performance on test set: Batch Loss = 5.81235122681, Accuracy = 0.782999992371\n",
      "Training epochs #3822000:   Batch Loss = 5.709411, Accuracy = 0.822000026703\n",
      "Performance on test set: Batch Loss = 5.77239704132, Accuracy = 0.799999952316\n",
      "Training epochs #3824000:   Batch Loss = 5.759366, Accuracy = 0.805000007153\n",
      "Performance on test set: Batch Loss = 5.78986310959, Accuracy = 0.819000065327\n",
      "Training epochs #3826000:   Batch Loss = 5.773543, Accuracy = 0.806000053883\n",
      "Performance on test set: Batch Loss = 5.72391605377, Accuracy = 0.808000028133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #3828000:   Batch Loss = 5.710408, Accuracy = 0.826000034809\n",
      "Performance on test set: Batch Loss = 5.74634075165, Accuracy = 0.814999997616\n",
      "Training epochs #3830000:   Batch Loss = 5.823169, Accuracy = 0.804000020027\n",
      "Performance on test set: Batch Loss = 5.70876216888, Accuracy = 0.822000026703\n",
      "Training epochs #3832000:   Batch Loss = 5.690794, Accuracy = 0.833000004292\n",
      "Performance on test set: Batch Loss = 5.77972698212, Accuracy = 0.787000000477\n",
      "Training epochs #3834000:   Batch Loss = 5.675702, Accuracy = 0.827000021935\n",
      "Performance on test set: Batch Loss = 5.72316551208, Accuracy = 0.808000028133\n",
      "Training epochs #3836000:   Batch Loss = 5.728774, Accuracy = 0.807999968529\n",
      "Performance on test set: Batch Loss = 5.65711021423, Accuracy = 0.831000030041\n",
      "Training epochs #3838000:   Batch Loss = 5.679211, Accuracy = 0.823000073433\n",
      "Performance on test set: Batch Loss = 5.65245389938, Accuracy = 0.84600007534\n",
      "Training epochs #3840000:   Batch Loss = 5.689289, Accuracy = 0.820999979973\n",
      "Performance on test set: Batch Loss = 5.69405603409, Accuracy = 0.808000028133\n",
      "Training epochs #3842000:   Batch Loss = 5.689887, Accuracy = 0.832000017166\n",
      "Performance on test set: Batch Loss = 5.67859745026, Accuracy = 0.827000081539\n",
      "Training epochs #3844000:   Batch Loss = 5.672793, Accuracy = 0.79699999094\n",
      "Performance on test set: Batch Loss = 5.75634527206, Accuracy = 0.801999986172\n",
      "Training epochs #3846000:   Batch Loss = 5.677934, Accuracy = 0.804000020027\n",
      "Performance on test set: Batch Loss = 5.69303607941, Accuracy = 0.805000066757\n",
      "Training epochs #3848000:   Batch Loss = 5.675888, Accuracy = 0.794000089169\n",
      "Performance on test set: Batch Loss = 6.10954427719, Accuracy = 0.842999994755\n",
      "Training epochs #3850000:   Batch Loss = 6.545486, Accuracy = 0.170000001788\n",
      "Performance on test set: Batch Loss = 5.71186447144, Accuracy = 0.789000034332\n",
      "Training epochs #3852000:   Batch Loss = 5.979311, Accuracy = 0.774999976158\n",
      "Performance on test set: Batch Loss = 6.07245063782, Accuracy = 0.753000020981\n",
      "Training epochs #3854000:   Batch Loss = 6.151241, Accuracy = 0.753000020981\n",
      "Performance on test set: Batch Loss = 6.22928905487, Accuracy = 0.753000020981\n",
      "Training epochs #3856000:   Batch Loss = 6.343402, Accuracy = 0.740999996662\n",
      "Performance on test set: Batch Loss = 6.0969953537, Accuracy = 0.788000106812\n",
      "Training epochs #3858000:   Batch Loss = 6.251113, Accuracy = 0.736000001431\n",
      "Performance on test set: Batch Loss = 6.07872343063, Accuracy = 0.769000053406\n",
      "Training epochs #3860000:   Batch Loss = 6.050431, Accuracy = 0.76700001955\n",
      "Performance on test set: Batch Loss = 6.00612640381, Accuracy = 0.745999991894\n",
      "Training epochs #3862000:   Batch Loss = 5.906295, Accuracy = 0.759000062943\n",
      "Performance on test set: Batch Loss = 5.83514547348, Accuracy = 0.817000031471\n",
      "Training epochs #3864000:   Batch Loss = 5.922694, Accuracy = 0.819999992847\n",
      "Performance on test set: Batch Loss = 5.85018253326, Accuracy = 0.776000022888\n",
      "Training epochs #3866000:   Batch Loss = 6.047840, Accuracy = 0.754999995232\n",
      "Performance on test set: Batch Loss = 6.00262594223, Accuracy = 0.772999942303\n",
      "Training epochs #3868000:   Batch Loss = 6.029469, Accuracy = 0.755999982357\n",
      "Performance on test set: Batch Loss = 5.90239620209, Accuracy = 0.770000040531\n",
      "Training epochs #3870000:   Batch Loss = 5.873504, Accuracy = 0.784000039101\n",
      "Performance on test set: Batch Loss = 5.91822767258, Accuracy = 0.77999997139\n",
      "Training epochs #3872000:   Batch Loss = 5.929650, Accuracy = 0.773000061512\n",
      "Performance on test set: Batch Loss = 5.97839641571, Accuracy = 0.754999995232\n",
      "Training epochs #3874000:   Batch Loss = 5.932843, Accuracy = 0.771000027657\n",
      "Performance on test set: Batch Loss = 5.92616462708, Accuracy = 0.755999922752\n",
      "Training epochs #3876000:   Batch Loss = 5.947831, Accuracy = 0.772000014782\n",
      "Performance on test set: Batch Loss = 5.89892864227, Accuracy = 0.790000021458\n",
      "Training epochs #3878000:   Batch Loss = 5.876177, Accuracy = 0.766999959946\n",
      "Performance on test set: Batch Loss = 5.88811540604, Accuracy = 0.788000047207\n",
      "Training epochs #3880000:   Batch Loss = 5.901183, Accuracy = 0.77999997139\n",
      "Performance on test set: Batch Loss = 5.95987129211, Accuracy = 0.75\n",
      "Training epochs #3882000:   Batch Loss = 5.877511, Accuracy = 0.786000013351\n",
      "Performance on test set: Batch Loss = 5.80725479126, Accuracy = 0.811999976635\n",
      "Training epochs #3884000:   Batch Loss = 5.857038, Accuracy = 0.804000020027\n",
      "Performance on test set: Batch Loss = 5.89813518524, Accuracy = 0.814999997616\n",
      "Training epochs #3886000:   Batch Loss = 5.757987, Accuracy = 0.819999992847\n",
      "Performance on test set: Batch Loss = 5.84224653244, Accuracy = 0.779000043869\n",
      "Training epochs #3888000:   Batch Loss = 5.950834, Accuracy = 0.770000040531\n",
      "Performance on test set: Batch Loss = 5.83962106705, Accuracy = 0.777000069618\n",
      "Training epochs #3890000:   Batch Loss = 5.779644, Accuracy = 0.804000079632\n",
      "Performance on test set: Batch Loss = 6.12552928925, Accuracy = 0.789000034332\n",
      "Training epochs #3892000:   Batch Loss = 6.206293, Accuracy = 0.749000072479\n",
      "Performance on test set: Batch Loss = 7.3573384285, Accuracy = 0.632999956608\n",
      "Training epochs #3894000:   Batch Loss = 7.618882, Accuracy = 0.64200001955\n",
      "Performance on test set: Batch Loss = 7.50938177109, Accuracy = 0.648000001907\n",
      "Training epochs #3896000:   Batch Loss = 7.423450, Accuracy = 0.665999948978\n",
      "Performance on test set: Batch Loss = 7.44494962692, Accuracy = 0.662000060081\n",
      "Training epochs #3898000:   Batch Loss = 7.453408, Accuracy = 0.665000021458\n",
      "Performance on test set: Batch Loss = 7.31659126282, Accuracy = 0.657999992371\n",
      "Training epochs #3900000:   Batch Loss = 7.409167, Accuracy = 0.63999992609\n",
      "Performance on test set: Batch Loss = 7.34005355835, Accuracy = 0.628999948502\n",
      "Training epochs #3902000:   Batch Loss = 7.120419, Accuracy = 0.679999947548\n",
      "Performance on test set: Batch Loss = 7.13713216782, Accuracy = 0.658999979496\n",
      "Training epochs #3904000:   Batch Loss = 7.267360, Accuracy = 0.636999964714\n",
      "Performance on test set: Batch Loss = 7.19310474396, Accuracy = 0.6400000453\n",
      "Training epochs #3906000:   Batch Loss = 7.014962, Accuracy = 0.652000010014\n",
      "Performance on test set: Batch Loss = 6.88669967651, Accuracy = 0.669000029564\n",
      "Training epochs #3908000:   Batch Loss = 6.852682, Accuracy = 0.665000021458\n",
      "Performance on test set: Batch Loss = 6.89563560486, Accuracy = 0.634999990463\n",
      "Training epochs #3910000:   Batch Loss = 6.802430, Accuracy = 0.632000029087\n",
      "Performance on test set: Batch Loss = 6.70064973831, Accuracy = 0.664999961853\n",
      "Training epochs #3912000:   Batch Loss = 6.683754, Accuracy = 0.638999998569\n",
      "Performance on test set: Batch Loss = 6.7335395813, Accuracy = 0.626999974251\n",
      "Training epochs #3914000:   Batch Loss = 6.493927, Accuracy = 0.67099994421\n",
      "Performance on test set: Batch Loss = 6.5057220459, Accuracy = 0.641000032425\n",
      "Training epochs #3916000:   Batch Loss = 6.487846, Accuracy = 0.625\n",
      "Performance on test set: Batch Loss = 6.33979892731, Accuracy = 0.65700006485\n",
      "Training epochs #3918000:   Batch Loss = 6.295678, Accuracy = 0.667000055313\n",
      "Performance on test set: Batch Loss = 6.26385307312, Accuracy = 0.657999992371\n",
      "Training epochs #3920000:   Batch Loss = 6.248680, Accuracy = 0.647999942303\n",
      "Performance on test set: Batch Loss = 6.27222061157, Accuracy = 0.629000008106\n",
      "Training epochs #3922000:   Batch Loss = 6.212714, Accuracy = 0.663000047207\n",
      "Performance on test set: Batch Loss = 6.23326587677, Accuracy = 0.660000026226\n",
      "Training epochs #3924000:   Batch Loss = 6.228833, Accuracy = 0.777999997139\n",
      "Performance on test set: Batch Loss = 6.29709815979, Accuracy = 0.762000083923\n",
      "Training epochs #3926000:   Batch Loss = 6.369997, Accuracy = 0.728000044823\n",
      "Performance on test set: Batch Loss = 6.26474761963, Accuracy = 0.76599997282\n",
      "Training epochs #3928000:   Batch Loss = 6.329835, Accuracy = 0.740000009537\n",
      "Performance on test set: Batch Loss = 6.30320215225, Accuracy = 0.750999987125\n",
      "Training epochs #3930000:   Batch Loss = 6.268373, Accuracy = 0.758000016212\n",
      "Performance on test set: Batch Loss = 6.20185995102, Accuracy = 0.774999976158\n",
      "Training epochs #3932000:   Batch Loss = 6.215635, Accuracy = 0.758000075817\n",
      "Performance on test set: Batch Loss = 6.2473783493, Accuracy = 0.741999983788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #3934000:   Batch Loss = 6.240759, Accuracy = 0.635999977589\n",
      "Performance on test set: Batch Loss = 6.21478271484, Accuracy = 0.64099997282\n",
      "Training epochs #3936000:   Batch Loss = 6.204261, Accuracy = 0.634999990463\n",
      "Performance on test set: Batch Loss = 6.09658432007, Accuracy = 0.657000005245\n",
      "Training epochs #3938000:   Batch Loss = 6.211459, Accuracy = 0.625000059605\n",
      "Performance on test set: Batch Loss = 6.12543153763, Accuracy = 0.657000005245\n",
      "Training epochs #3940000:   Batch Loss = 6.144483, Accuracy = 0.646000027657\n",
      "Performance on test set: Batch Loss = 6.16948890686, Accuracy = 0.626999914646\n",
      "Training epochs #3942000:   Batch Loss = 6.167068, Accuracy = 0.631999969482\n",
      "Performance on test set: Batch Loss = 6.13419723511, Accuracy = 0.657999992371\n",
      "Training epochs #3944000:   Batch Loss = 6.137599, Accuracy = 0.638999938965\n",
      "Performance on test set: Batch Loss = 6.16892623901, Accuracy = 0.637999951839\n",
      "Training epochs #3946000:   Batch Loss = 6.196904, Accuracy = 0.635999977589\n",
      "Performance on test set: Batch Loss = 6.10625934601, Accuracy = 0.668000042439\n",
      "Training epochs #3948000:   Batch Loss = 6.166680, Accuracy = 0.638000011444\n",
      "Performance on test set: Batch Loss = 6.15221786499, Accuracy = 0.635999977589\n",
      "Training epochs #3950000:   Batch Loss = 6.146755, Accuracy = 0.638000011444\n",
      "Performance on test set: Batch Loss = 6.09057235718, Accuracy = 0.663999974728\n",
      "Training epochs #3952000:   Batch Loss = 6.119266, Accuracy = 0.644000053406\n",
      "Performance on test set: Batch Loss = 6.16693115234, Accuracy = 0.627000033855\n",
      "Training epochs #3954000:   Batch Loss = 6.113793, Accuracy = 0.652999937534\n",
      "Performance on test set: Batch Loss = 6.15195512772, Accuracy = 0.64200001955\n",
      "Training epochs #3956000:   Batch Loss = 6.090304, Accuracy = 0.65700006485\n",
      "Performance on test set: Batch Loss = 6.05116653442, Accuracy = 0.669000029564\n",
      "Training epochs #3958000:   Batch Loss = 6.100750, Accuracy = 0.667999982834\n",
      "Performance on test set: Batch Loss = 6.07278585434, Accuracy = 0.657999992371\n",
      "Training epochs #3960000:   Batch Loss = 6.026302, Accuracy = 0.676999986172\n",
      "Performance on test set: Batch Loss = 6.12427425385, Accuracy = 0.740000009537\n",
      "Training epochs #3962000:   Batch Loss = 6.071798, Accuracy = 0.772000074387\n",
      "Performance on test set: Batch Loss = 6.08359909058, Accuracy = 0.763000011444\n",
      "Training epochs #3964000:   Batch Loss = 6.088336, Accuracy = 0.76700001955\n",
      "Performance on test set: Batch Loss = 6.10980129242, Accuracy = 0.759999990463\n",
      "Training epochs #3966000:   Batch Loss = 6.058061, Accuracy = 0.770000100136\n",
      "Performance on test set: Batch Loss = 6.0463309288, Accuracy = 0.768000006676\n",
      "Training epochs #3968000:   Batch Loss = 6.106566, Accuracy = 0.744000017643\n",
      "Performance on test set: Batch Loss = 6.09364128113, Accuracy = 0.753999948502\n",
      "Training epochs #3970000:   Batch Loss = 6.080439, Accuracy = 0.761000037193\n",
      "Performance on test set: Batch Loss = 6.0404086113, Accuracy = 0.774999976158\n",
      "Training epochs #3972000:   Batch Loss = 6.097548, Accuracy = 0.740999996662\n",
      "Performance on test set: Batch Loss = 6.11233901978, Accuracy = 0.744000017643\n",
      "Training epochs #3974000:   Batch Loss = 6.085386, Accuracy = 0.754999995232\n",
      "Performance on test set: Batch Loss = 6.10679006577, Accuracy = 0.744999945164\n",
      "Training epochs #3976000:   Batch Loss = 6.028602, Accuracy = 0.768000006676\n",
      "Performance on test set: Batch Loss = 6.00230026245, Accuracy = 0.784000039101\n",
      "Training epochs #3978000:   Batch Loss = 6.012768, Accuracy = 0.778999984264\n",
      "Performance on test set: Batch Loss = 6.02725410461, Accuracy = 0.768000006676\n",
      "Training epochs #3980000:   Batch Loss = 6.026414, Accuracy = 0.761999964714\n",
      "Performance on test set: Batch Loss = 6.07354068756, Accuracy = 0.743000030518\n",
      "Training epochs #3982000:   Batch Loss = 5.958735, Accuracy = 0.789999961853\n",
      "Performance on test set: Batch Loss = 6.03687524796, Accuracy = 0.763000011444\n",
      "Training epochs #3984000:   Batch Loss = 6.047550, Accuracy = 0.752000033855\n",
      "Performance on test set: Batch Loss = 6.06224822998, Accuracy = 0.759999990463\n",
      "Training epochs #3986000:   Batch Loss = 6.047751, Accuracy = 0.754000008106\n",
      "Performance on test set: Batch Loss = 6.00052261353, Accuracy = 0.768000006676\n",
      "Training epochs #3988000:   Batch Loss = 6.009366, Accuracy = 0.7650000453\n",
      "Performance on test set: Batch Loss = 6.04388284683, Accuracy = 0.754000008106\n",
      "Training epochs #3990000:   Batch Loss = 6.110249, Accuracy = 0.737999975681\n",
      "Performance on test set: Batch Loss = 5.99352121353, Accuracy = 0.774999976158\n",
      "Training epochs #3992000:   Batch Loss = 5.990169, Accuracy = 0.766000032425\n",
      "Performance on test set: Batch Loss = 6.06800079346, Accuracy = 0.744000017643\n",
      "Training epochs #3994000:   Batch Loss = 5.979990, Accuracy = 0.771000027657\n",
      "Performance on test set: Batch Loss = 6.0610704422, Accuracy = 0.745000004768\n",
      "Training epochs #3996000:   Batch Loss = 6.043280, Accuracy = 0.757000029087\n",
      "Performance on test set: Batch Loss = 5.95523309708, Accuracy = 0.784000039101\n",
      "Training epochs #3998000:   Batch Loss = 5.977776, Accuracy = 0.766000032425\n",
      "Performance on test set: Batch Loss = 5.97817611694, Accuracy = 0.768000006676\n",
      "Training epochs #4000000:   Batch Loss = 6.008578, Accuracy = 0.754999995232\n",
      "Performance on test set: Batch Loss = 6.02509260178, Accuracy = 0.743000030518\n",
      "Training epochs #4002000:   Batch Loss = 5.979169, Accuracy = 0.766000032425\n",
      "Performance on test set: Batch Loss = 5.99351835251, Accuracy = 0.763000011444\n",
      "Training epochs #4004000:   Batch Loss = 5.944522, Accuracy = 0.777999997139\n",
      "Performance on test set: Batch Loss = 6.01378440857, Accuracy = 0.759999990463\n",
      "Training epochs #4006000:   Batch Loss = 6.044315, Accuracy = 0.728000044823\n",
      "Performance on test set: Batch Loss = 5.9524846077, Accuracy = 0.768000006676\n",
      "Training epochs #4008000:   Batch Loss = 6.014448, Accuracy = 0.743000030518\n",
      "Performance on test set: Batch Loss = 5.99215888977, Accuracy = 0.754000008106\n",
      "Training epochs #4010000:   Batch Loss = 5.996974, Accuracy = 0.766000032425\n",
      "Performance on test set: Batch Loss = 5.94596815109, Accuracy = 0.774999976158\n",
      "Training epochs #4012000:   Batch Loss = 5.973798, Accuracy = 0.760999977589\n",
      "Performance on test set: Batch Loss = 6.01299095154, Accuracy = 0.744000017643\n",
      "Training epochs #4014000:   Batch Loss = 6.003381, Accuracy = 0.743999958038\n",
      "Performance on test set: Batch Loss = 6.00697517395, Accuracy = 0.745999932289\n",
      "Training epochs #4016000:   Batch Loss = 6.000561, Accuracy = 0.741000056267\n",
      "Performance on test set: Batch Loss = 5.90055084229, Accuracy = 0.787000060081\n",
      "Training epochs #4018000:   Batch Loss = 6.008829, Accuracy = 0.736999988556\n",
      "Performance on test set: Batch Loss = 5.91729259491, Accuracy = 0.768000006676\n",
      "Training epochs #4020000:   Batch Loss = 5.952427, Accuracy = 0.766999900341\n",
      "Performance on test set: Batch Loss = 5.96191883087, Accuracy = 0.743999958038\n",
      "Training epochs #4022000:   Batch Loss = 5.966707, Accuracy = 0.748000025749\n",
      "Performance on test set: Batch Loss = 5.93550491333, Accuracy = 0.76499992609\n",
      "Training epochs #4024000:   Batch Loss = 5.911302, Accuracy = 0.759999990463\n",
      "Performance on test set: Batch Loss = 5.94738674164, Accuracy = 0.759999990463\n",
      "Training epochs #4026000:   Batch Loss = 5.979765, Accuracy = 0.744000017643\n",
      "Performance on test set: Batch Loss = 5.87715625763, Accuracy = 0.769000053406\n",
      "Training epochs #4028000:   Batch Loss = 5.946442, Accuracy = 0.743000030518\n",
      "Performance on test set: Batch Loss = 5.90867853165, Accuracy = 0.755999982357\n",
      "Training epochs #4030000:   Batch Loss = 5.899708, Accuracy = 0.758000016212\n",
      "Performance on test set: Batch Loss = 5.86553955078, Accuracy = 0.774999976158\n",
      "Training epochs #4032000:   Batch Loss = 5.890923, Accuracy = 0.756000041962\n",
      "Performance on test set: Batch Loss = 5.91854953766, Accuracy = 0.744000017643\n",
      "Training epochs #4034000:   Batch Loss = 5.873938, Accuracy = 0.758999943733\n",
      "Performance on test set: Batch Loss = 5.90023851395, Accuracy = 0.746000051498\n",
      "Training epochs #4036000:   Batch Loss = 5.848938, Accuracy = 0.767000079155\n",
      "Performance on test set: Batch Loss = 5.80542993546, Accuracy = 0.787000000477\n",
      "Training epochs #4038000:   Batch Loss = 5.850156, Accuracy = 0.760000050068\n",
      "Performance on test set: Batch Loss = 5.82614469528, Accuracy = 0.768000006676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #4040000:   Batch Loss = 5.786499, Accuracy = 0.778999984264\n",
      "Performance on test set: Batch Loss = 5.8578915596, Accuracy = 0.744000017643\n",
      "Training epochs #4042000:   Batch Loss = 5.809311, Accuracy = 0.776000022888\n",
      "Performance on test set: Batch Loss = 5.8451924324, Accuracy = 0.7650000453\n",
      "Training epochs #4044000:   Batch Loss = 5.840447, Accuracy = 0.76800006628\n",
      "Performance on test set: Batch Loss = 5.86285924911, Accuracy = 0.761000037193\n",
      "Training epochs #4046000:   Batch Loss = 5.794455, Accuracy = 0.771000027657\n",
      "Performance on test set: Batch Loss = 5.7909488678, Accuracy = 0.768999993801\n",
      "Training epochs #4048000:   Batch Loss = 5.852409, Accuracy = 0.747000038624\n",
      "Performance on test set: Batch Loss = 5.8259677887, Accuracy = 0.754999995232\n",
      "Training epochs #4050000:   Batch Loss = 5.817685, Accuracy = 0.763000011444\n",
      "Performance on test set: Batch Loss = 5.79018735886, Accuracy = 0.774999976158\n",
      "Training epochs #4052000:   Batch Loss = 5.841642, Accuracy = 0.741999983788\n",
      "Performance on test set: Batch Loss = 5.84446763992, Accuracy = 0.745000064373\n",
      "Training epochs #4054000:   Batch Loss = 5.807859, Accuracy = 0.757000029087\n",
      "Performance on test set: Batch Loss = 5.83367013931, Accuracy = 0.746999979019\n",
      "Training epochs #4056000:   Batch Loss = 5.775752, Accuracy = 0.770999968052\n",
      "Performance on test set: Batch Loss = 5.74896621704, Accuracy = 0.788000106812\n",
      "Training epochs #4058000:   Batch Loss = 5.755067, Accuracy = 0.780999958515\n",
      "Performance on test set: Batch Loss = 5.76083946228, Accuracy = 0.769000053406\n",
      "Training epochs #4060000:   Batch Loss = 5.743086, Accuracy = 0.763000071049\n",
      "Performance on test set: Batch Loss = 5.78954410553, Accuracy = 0.744000077248\n",
      "Training epochs #4062000:   Batch Loss = 5.697134, Accuracy = 0.792999982834\n",
      "Performance on test set: Batch Loss = 5.78096103668, Accuracy = 0.763999998569\n",
      "Training epochs #4064000:   Batch Loss = 5.764767, Accuracy = 0.752999961376\n",
      "Performance on test set: Batch Loss = 5.80579090118, Accuracy = 0.761999964714\n",
      "Training epochs #4066000:   Batch Loss = 5.793029, Accuracy = 0.754999995232\n",
      "Performance on test set: Batch Loss = 5.73136997223, Accuracy = 0.768999934196\n",
      "Training epochs #4068000:   Batch Loss = 5.738260, Accuracy = 0.76800006628\n",
      "Performance on test set: Batch Loss = 5.75491189957, Accuracy = 0.754999995232\n",
      "Training epochs #4070000:   Batch Loss = 5.846526, Accuracy = 0.75\n",
      "Performance on test set: Batch Loss = 5.73243904114, Accuracy = 0.780000030994\n",
      "Training epochs #4072000:   Batch Loss = 5.713255, Accuracy = 0.781000077724\n",
      "Performance on test set: Batch Loss = 5.78521966934, Accuracy = 0.753000020981\n",
      "Training epochs #4074000:   Batch Loss = 5.694857, Accuracy = 0.78100001812\n",
      "Performance on test set: Batch Loss = 5.76831865311, Accuracy = 0.753999948502\n",
      "Training epochs #4076000:   Batch Loss = 5.768975, Accuracy = 0.766999959946\n",
      "Performance on test set: Batch Loss = 5.68792772293, Accuracy = 0.796000063419\n",
      "Training epochs #4078000:   Batch Loss = 5.688066, Accuracy = 0.780999958515\n",
      "Performance on test set: Batch Loss = 5.69850301743, Accuracy = 0.787999987602\n",
      "Training epochs #4080000:   Batch Loss = 5.740975, Accuracy = 0.763999998569\n",
      "Performance on test set: Batch Loss = 5.72209215164, Accuracy = 0.75100004673\n",
      "Training epochs #4082000:   Batch Loss = 5.673026, Accuracy = 0.777000010014\n",
      "Performance on test set: Batch Loss = 5.71864891052, Accuracy = 0.768999993801\n",
      "Training epochs #4084000:   Batch Loss = 5.667176, Accuracy = 0.786000013351\n",
      "Performance on test set: Batch Loss = 5.74528694153, Accuracy = 0.766999959946\n",
      "Training epochs #4086000:   Batch Loss = 5.740074, Accuracy = 0.738999962807\n",
      "Performance on test set: Batch Loss = 5.66906404495, Accuracy = 0.773000001907\n",
      "Training epochs #4088000:   Batch Loss = 5.724499, Accuracy = 0.753000020981\n",
      "Performance on test set: Batch Loss = 5.6927690506, Accuracy = 0.771000027657\n",
      "Training epochs #4090000:   Batch Loss = 5.740551, Accuracy = 0.770000100136\n",
      "Performance on test set: Batch Loss = 5.67653751373, Accuracy = 0.779000043869\n",
      "Training epochs #4092000:   Batch Loss = 5.671794, Accuracy = 0.775000035763\n",
      "Performance on test set: Batch Loss = 5.72680902481, Accuracy = 0.753000020981\n",
      "Training epochs #4094000:   Batch Loss = 5.699310, Accuracy = 0.754999995232\n",
      "Performance on test set: Batch Loss = 5.70463418961, Accuracy = 0.756000041962\n",
      "Training epochs #4096000:   Batch Loss = 5.705692, Accuracy = 0.752999961376\n",
      "Performance on test set: Batch Loss = 5.63455152512, Accuracy = 0.795000076294\n",
      "Training epochs #4098000:   Batch Loss = 5.732139, Accuracy = 0.749000012875\n",
      "Performance on test set: Batch Loss = 5.65142536163, Accuracy = 0.788000047207\n",
      "Training epochs #4100000:   Batch Loss = 5.685093, Accuracy = 0.776000022888\n",
      "Performance on test set: Batch Loss = 5.68776035309, Accuracy = 0.750999987125\n",
      "Training epochs #4102000:   Batch Loss = 5.773452, Accuracy = 0.754000008106\n",
      "Performance on test set: Batch Loss = 5.79584407806, Accuracy = 0.768999993801\n",
      "Training epochs #4104000:   Batch Loss = 5.743382, Accuracy = 0.772000014782\n",
      "Performance on test set: Batch Loss = 5.75007629395, Accuracy = 0.766000032425\n",
      "Training epochs #4106000:   Batch Loss = 5.730886, Accuracy = 0.752999961376\n",
      "Performance on test set: Batch Loss = 5.90182876587, Accuracy = 0.773000001907\n",
      "Training epochs #4108000:   Batch Loss = 5.738856, Accuracy = 0.74899995327\n",
      "Performance on test set: Batch Loss = 5.75650787354, Accuracy = 0.76700001955\n",
      "Training epochs #4110000:   Batch Loss = 5.803005, Accuracy = 0.7650000453\n",
      "Performance on test set: Batch Loss = 5.80677604675, Accuracy = 0.778000056744\n",
      "Training epochs #4112000:   Batch Loss = 5.773630, Accuracy = 0.759000062943\n",
      "Performance on test set: Batch Loss = 5.79480886459, Accuracy = 0.751999974251\n",
      "Training epochs #4114000:   Batch Loss = 5.727555, Accuracy = 0.771000027657\n",
      "Performance on test set: Batch Loss = 5.80766487122, Accuracy = 0.754000008106\n",
      "Training epochs #4116000:   Batch Loss = 5.748446, Accuracy = 0.774000048637\n",
      "Performance on test set: Batch Loss = 5.68546915054, Accuracy = 0.793000102043\n",
      "Training epochs #4118000:   Batch Loss = 5.720494, Accuracy = 0.76700001955\n",
      "Performance on test set: Batch Loss = 5.67608547211, Accuracy = 0.786000013351\n",
      "Training epochs #4120000:   Batch Loss = 5.651365, Accuracy = 0.780000030994\n",
      "Performance on test set: Batch Loss = 5.70371627808, Accuracy = 0.75\n",
      "Training epochs #4122000:   Batch Loss = 5.652648, Accuracy = 0.787000000477\n",
      "Performance on test set: Batch Loss = 5.68895530701, Accuracy = 0.769999980927\n",
      "Training epochs #4124000:   Batch Loss = 5.728984, Accuracy = 0.776000022888\n",
      "Performance on test set: Batch Loss = 5.71712207794, Accuracy = 0.772000074387\n",
      "Training epochs #4126000:   Batch Loss = 5.666343, Accuracy = 0.786000013351\n",
      "Performance on test set: Batch Loss = 5.65768146515, Accuracy = 0.776000022888\n",
      "Training epochs #4128000:   Batch Loss = 5.691015, Accuracy = 0.767000079155\n",
      "Performance on test set: Batch Loss = 5.65194177628, Accuracy = 0.774000048637\n",
      "Training epochs #4130000:   Batch Loss = 5.670341, Accuracy = 0.771000027657\n",
      "Performance on test set: Batch Loss = 5.64721918106, Accuracy = 0.778999984264\n",
      "Training epochs #4132000:   Batch Loss = 5.690108, Accuracy = 0.749000012875\n",
      "Performance on test set: Batch Loss = 5.71785593033, Accuracy = 0.755999982357\n",
      "Training epochs #4134000:   Batch Loss = 5.673767, Accuracy = 0.763999998569\n",
      "Performance on test set: Batch Loss = 5.67398023605, Accuracy = 0.757000029087\n",
      "Training epochs #4136000:   Batch Loss = 5.628264, Accuracy = 0.777000069618\n",
      "Performance on test set: Batch Loss = 5.63653850555, Accuracy = 0.79699999094\n",
      "Training epochs #4138000:   Batch Loss = 5.617347, Accuracy = 0.788999974728\n",
      "Performance on test set: Batch Loss = 5.59928321838, Accuracy = 0.787000000477\n",
      "Training epochs #4140000:   Batch Loss = 5.607632, Accuracy = 0.769000053406\n",
      "Performance on test set: Batch Loss = 5.64534378052, Accuracy = 0.754999995232\n",
      "Training epochs #4142000:   Batch Loss = 5.556681, Accuracy = 0.805000007153\n",
      "Performance on test set: Batch Loss = 5.62100887299, Accuracy = 0.772000014782\n",
      "Training epochs #4144000:   Batch Loss = 5.616736, Accuracy = 0.763999998569\n",
      "Performance on test set: Batch Loss = 5.67131900787, Accuracy = 0.792999982834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #4146000:   Batch Loss = 5.663454, Accuracy = 0.759999990463\n",
      "Performance on test set: Batch Loss = 5.59404611588, Accuracy = 0.770999968052\n",
      "Training epochs #4148000:   Batch Loss = 5.597677, Accuracy = 0.78100001812\n",
      "Performance on test set: Batch Loss = 5.60626363754, Accuracy = 0.804999947548\n",
      "Training epochs #4150000:   Batch Loss = 5.712016, Accuracy = 0.787999987602\n",
      "Performance on test set: Batch Loss = 5.60884618759, Accuracy = 0.814000070095\n",
      "Training epochs #4152000:   Batch Loss = 5.565586, Accuracy = 0.817000031471\n",
      "Performance on test set: Batch Loss = 5.66598987579, Accuracy = 0.796000063419\n",
      "Training epochs #4154000:   Batch Loss = 5.550867, Accuracy = 0.811000108719\n",
      "Performance on test set: Batch Loss = 5.62722873688, Accuracy = 0.794000029564\n",
      "Training epochs #4156000:   Batch Loss = 5.660221, Accuracy = 0.79800003767\n",
      "Performance on test set: Batch Loss = 5.58407974243, Accuracy = 0.820999979973\n",
      "Training epochs #4158000:   Batch Loss = 5.561559, Accuracy = 0.807000041008\n",
      "Performance on test set: Batch Loss = 5.55434322357, Accuracy = 0.822000026703\n",
      "Training epochs #4160000:   Batch Loss = 5.611007, Accuracy = 0.800999939442\n",
      "Performance on test set: Batch Loss = 5.58263397217, Accuracy = 0.800000071526\n",
      "Training epochs #4162000:   Batch Loss = 5.535943, Accuracy = 0.814999938011\n",
      "Performance on test set: Batch Loss = 5.57695055008, Accuracy = 0.81099998951\n",
      "Training epochs #4164000:   Batch Loss = 5.566469, Accuracy = 0.814999997616\n",
      "Performance on test set: Batch Loss = 5.63338661194, Accuracy = 0.802000045776\n",
      "Training epochs #4166000:   Batch Loss = 5.603073, Accuracy = 0.784000039101\n",
      "Performance on test set: Batch Loss = 5.55362224579, Accuracy = 0.808999955654\n",
      "Training epochs #4168000:   Batch Loss = 5.619526, Accuracy = 0.803000032902\n",
      "Performance on test set: Batch Loss = 5.57378959656, Accuracy = 0.833000063896\n",
      "Training epochs #4170000:   Batch Loss = 5.647518, Accuracy = 0.818000018597\n",
      "Performance on test set: Batch Loss = 5.56448173523, Accuracy = 0.838999986649\n",
      "Training epochs #4172000:   Batch Loss = 5.552921, Accuracy = 0.840000033379\n",
      "Performance on test set: Batch Loss = 5.62757825851, Accuracy = 0.824000000954\n",
      "Training epochs #4174000:   Batch Loss = 5.567122, Accuracy = 0.847000002861\n",
      "Performance on test set: Batch Loss = 5.59065437317, Accuracy = 0.834000051022\n",
      "Training epochs #4176000:   Batch Loss = 5.595127, Accuracy = 0.839999973774\n",
      "Performance on test set: Batch Loss = 5.55599594116, Accuracy = 0.855000019073\n",
      "Training epochs #4178000:   Batch Loss = 5.624745, Accuracy = 0.830999970436\n",
      "Performance on test set: Batch Loss = 5.5271821022, Accuracy = 0.866000056267\n",
      "Training epochs #4180000:   Batch Loss = 5.588802, Accuracy = 0.848000049591\n",
      "Performance on test set: Batch Loss = 5.56745910645, Accuracy = 0.842000007629\n",
      "Training epochs #4182000:   Batch Loss = 5.612754, Accuracy = 0.833999991417\n",
      "Performance on test set: Batch Loss = 5.57091999054, Accuracy = 0.857000052929\n",
      "Training epochs #4184000:   Batch Loss = 5.532589, Accuracy = 0.855000019073\n",
      "Performance on test set: Batch Loss = 5.65275859833, Accuracy = 0.825000047684\n",
      "Training epochs #4186000:   Batch Loss = 5.658511, Accuracy = 0.788999974728\n",
      "Performance on test set: Batch Loss = 5.54070568085, Accuracy = 0.809000015259\n",
      "Training epochs #4188000:   Batch Loss = 5.628595, Accuracy = 0.800000011921\n",
      "Performance on test set: Batch Loss = 5.573823452, Accuracy = 0.81099998951\n",
      "Training epochs #4190000:   Batch Loss = 5.551057, Accuracy = 0.808999955654\n",
      "Performance on test set: Batch Loss = 5.55057287216, Accuracy = 0.822000026703\n",
      "Training epochs #4192000:   Batch Loss = 5.575921, Accuracy = 0.826000034809\n",
      "Performance on test set: Batch Loss = 5.60733795166, Accuracy = 0.823000013828\n",
      "Training epochs #4194000:   Batch Loss = 5.544233, Accuracy = 0.861000001431\n",
      "Performance on test set: Batch Loss = 5.58106899261, Accuracy = 0.832999944687\n",
      "Training epochs #4196000:   Batch Loss = 5.531618, Accuracy = 0.848999977112\n",
      "Performance on test set: Batch Loss = 5.61032533646, Accuracy = 0.855000019073\n",
      "Training epochs #4198000:   Batch Loss = 6.204965, Accuracy = 0.811000049114\n",
      "Performance on test set: Batch Loss = 6.00490856171, Accuracy = 0.841000020504\n",
      "Training epochs #4200000:   Batch Loss = 5.642160, Accuracy = 0.77999997139\n",
      "Performance on test set: Batch Loss = 5.89527511597, Accuracy = 0.749000012875\n",
      "Training epochs #4202000:   Batch Loss = 5.953641, Accuracy = 0.786000072956\n",
      "Performance on test set: Batch Loss = 6.0986495018, Accuracy = 0.768999993801\n",
      "Training epochs #4204000:   Batch Loss = 6.048732, Accuracy = 0.76800006628\n",
      "Performance on test set: Batch Loss = 6.08234214783, Accuracy = 0.761000037193\n",
      "Training epochs #4206000:   Batch Loss = 5.948913, Accuracy = 0.770999968052\n",
      "Performance on test set: Batch Loss = 5.89163398743, Accuracy = 0.769999921322\n",
      "Training epochs #4208000:   Batch Loss = 5.923793, Accuracy = 0.747000038624\n",
      "Performance on test set: Batch Loss = 5.84722757339, Accuracy = 0.754000008106\n",
      "Training epochs #4210000:   Batch Loss = 5.741392, Accuracy = 0.771000087261\n",
      "Performance on test set: Batch Loss = 5.69794893265, Accuracy = 0.777999997139\n",
      "Training epochs #4212000:   Batch Loss = 5.751173, Accuracy = 0.754000008106\n",
      "Performance on test set: Batch Loss = 5.74686670303, Accuracy = 0.777999997139\n",
      "Training epochs #4214000:   Batch Loss = 5.731045, Accuracy = 0.805000007153\n",
      "Performance on test set: Batch Loss = 5.82590341568, Accuracy = 0.804000020027\n",
      "Training epochs #4216000:   Batch Loss = 5.686206, Accuracy = 0.824000000954\n",
      "Performance on test set: Batch Loss = 5.6708574295, Accuracy = 0.810000002384\n",
      "Training epochs #4218000:   Batch Loss = 5.703973, Accuracy = 0.798999965191\n",
      "Performance on test set: Batch Loss = 5.71107625961, Accuracy = 0.81200003624\n",
      "Training epochs #4220000:   Batch Loss = 5.669661, Accuracy = 0.77999997139\n",
      "Performance on test set: Batch Loss = 5.69711494446, Accuracy = 0.766999959946\n",
      "Training epochs #4222000:   Batch Loss = 5.606520, Accuracy = 0.818000078201\n",
      "Performance on test set: Batch Loss = 5.67117261887, Accuracy = 0.78200006485\n",
      "Training epochs #4224000:   Batch Loss = 5.657494, Accuracy = 0.769999980927\n",
      "Performance on test set: Batch Loss = 5.70827198029, Accuracy = 0.788999974728\n",
      "Training epochs #4226000:   Batch Loss = 5.703159, Accuracy = 0.769999980927\n",
      "Performance on test set: Batch Loss = 5.65652942657, Accuracy = 0.780000030994\n",
      "Training epochs #4228000:   Batch Loss = 5.636190, Accuracy = 0.785999953747\n",
      "Performance on test set: Batch Loss = 5.66042757034, Accuracy = 0.768000006676\n",
      "Training epochs #4230000:   Batch Loss = 5.727677, Accuracy = 0.750999987125\n",
      "Performance on test set: Batch Loss = 5.60450839996, Accuracy = 0.777999997139\n",
      "Training epochs #4232000:   Batch Loss = 5.626497, Accuracy = 0.779000043869\n",
      "Performance on test set: Batch Loss = 5.63966035843, Accuracy = 0.753000020981\n",
      "Training epochs #4234000:   Batch Loss = 5.516616, Accuracy = 0.779000043869\n",
      "Performance on test set: Batch Loss = 5.57639217377, Accuracy = 0.760999977589\n",
      "Training epochs #4236000:   Batch Loss = 5.591813, Accuracy = 0.773000001907\n",
      "Performance on test set: Batch Loss = 5.6563782692, Accuracy = 0.797000050545\n",
      "Training epochs #4238000:   Batch Loss = 6.352609, Accuracy = 0.804000020027\n",
      "Performance on test set: Batch Loss = 6.43777370453, Accuracy = 0.807000041008\n",
      "Training epochs #4240000:   Batch Loss = 6.406384, Accuracy = 0.766000032425\n",
      "Performance on test set: Batch Loss = 6.20751333237, Accuracy = 0.750000059605\n",
      "Training epochs #4242000:   Batch Loss = 6.078745, Accuracy = 0.766000032425\n",
      "Performance on test set: Batch Loss = 6.06509876251, Accuracy = 0.763999998569\n",
      "Training epochs #4244000:   Batch Loss = 5.878822, Accuracy = 0.777999997139\n",
      "Performance on test set: Batch Loss = 5.93069267273, Accuracy = 0.761000037193\n",
      "Training epochs #4246000:   Batch Loss = 5.912743, Accuracy = 0.728999972343\n",
      "Performance on test set: Batch Loss = 5.86081600189, Accuracy = 0.76599997282\n",
      "Training epochs #4248000:   Batch Loss = 6.131942, Accuracy = 0.752000033855\n",
      "Performance on test set: Batch Loss = 6.20647001266, Accuracy = 0.757999956608\n",
      "Training epochs #4250000:   Batch Loss = 6.160832, Accuracy = 0.771999955177\n",
      "Performance on test set: Batch Loss = 5.91252183914, Accuracy = 0.774999976158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #4252000:   Batch Loss = 5.929235, Accuracy = 0.758000016212\n",
      "Performance on test set: Batch Loss = 5.98109960556, Accuracy = 0.746000051498\n",
      "Training epochs #4254000:   Batch Loss = 6.003633, Accuracy = 0.741000056267\n",
      "Performance on test set: Batch Loss = 6.02931880951, Accuracy = 0.746000051498\n",
      "Training epochs #4256000:   Batch Loss = 6.071777, Accuracy = 0.740999996662\n",
      "Performance on test set: Batch Loss = 5.83802175522, Accuracy = 0.786999940872\n",
      "Training epochs #4258000:   Batch Loss = 5.976651, Accuracy = 0.741000056267\n",
      "Performance on test set: Batch Loss = 5.85217809677, Accuracy = 0.768999993801\n",
      "Training epochs #4260000:   Batch Loss = 5.897315, Accuracy = 0.76599997282\n",
      "Performance on test set: Batch Loss = 5.86160230637, Accuracy = 0.745000064373\n",
      "Training epochs #4262000:   Batch Loss = 5.907578, Accuracy = 0.752000033855\n",
      "Performance on test set: Batch Loss = 5.89047670364, Accuracy = 0.763999998569\n",
      "Training epochs #4264000:   Batch Loss = 5.822876, Accuracy = 0.766000032425\n",
      "Performance on test set: Batch Loss = 5.890832901, Accuracy = 0.767000079155\n",
      "Training epochs #4266000:   Batch Loss = 5.955451, Accuracy = 0.744999945164\n",
      "Performance on test set: Batch Loss = 5.8139834404, Accuracy = 0.772000014782\n",
      "Training epochs #4268000:   Batch Loss = 5.886198, Accuracy = 0.753000020981\n",
      "Performance on test set: Batch Loss = 5.80285787582, Accuracy = 0.761000037193\n",
      "Training epochs #4270000:   Batch Loss = 5.797976, Accuracy = 0.768999993801\n",
      "Performance on test set: Batch Loss = 5.72080373764, Accuracy = 0.789999961853\n",
      "Training epochs #4272000:   Batch Loss = 5.729272, Accuracy = 0.782999992371\n",
      "Performance on test set: Batch Loss = 5.78578710556, Accuracy = 0.775000035763\n",
      "Training epochs #4274000:   Batch Loss = 5.707881, Accuracy = 0.795000016689\n",
      "Performance on test set: Batch Loss = 5.81258153915, Accuracy = 0.780999958515\n",
      "Training epochs #4276000:   Batch Loss = 5.696483, Accuracy = 0.770000040531\n",
      "Performance on test set: Batch Loss = 5.67710494995, Accuracy = 0.792999982834\n",
      "Training epochs #4278000:   Batch Loss = 5.720073, Accuracy = 0.768000006676\n",
      "Performance on test set: Batch Loss = 5.68417263031, Accuracy = 0.786000072956\n",
      "Training epochs #4280000:   Batch Loss = 5.674572, Accuracy = 0.782000005245\n",
      "Performance on test set: Batch Loss = 5.75206375122, Accuracy = 0.75\n",
      "Training epochs #4282000:   Batch Loss = 5.689304, Accuracy = 0.786000013351\n",
      "Performance on test set: Batch Loss = 5.76697778702, Accuracy = 0.768999993801\n",
      "Training epochs #4284000:   Batch Loss = 5.732368, Accuracy = 0.776000022888\n",
      "Performance on test set: Batch Loss = 5.75835561752, Accuracy = 0.770000100136\n",
      "Training epochs #4286000:   Batch Loss = 5.667920, Accuracy = 0.78200006485\n",
      "Performance on test set: Batch Loss = 5.64982128143, Accuracy = 0.777000010014\n",
      "Training epochs #4288000:   Batch Loss = 5.686116, Accuracy = 0.772000014782\n",
      "Performance on test set: Batch Loss = 5.60999631882, Accuracy = 0.800999999046\n",
      "Training epochs #4290000:   Batch Loss = 5.614017, Accuracy = 0.814999997616\n",
      "Performance on test set: Batch Loss = 5.5817899704, Accuracy = 0.827000081539\n",
      "Training epochs #4292000:   Batch Loss = 5.677646, Accuracy = 0.809000074863\n",
      "Performance on test set: Batch Loss = 5.63020038605, Accuracy = 0.791000008583\n",
      "Training epochs #4294000:   Batch Loss = 5.560963, Accuracy = 0.796000003815\n",
      "Performance on test set: Batch Loss = 5.59035015106, Accuracy = 0.773999989033\n",
      "Training epochs #4296000:   Batch Loss = 5.547872, Accuracy = 0.786000013351\n",
      "Performance on test set: Batch Loss = 5.54108047485, Accuracy = 0.797999978065\n",
      "Training epochs #4298000:   Batch Loss = 5.517269, Accuracy = 0.806999981403\n",
      "Performance on test set: Batch Loss = 5.47145652771, Accuracy = 0.842000007629\n",
      "Training epochs #4300000:   Batch Loss = 5.471097, Accuracy = 0.816999912262\n",
      "Performance on test set: Batch Loss = 5.54154014587, Accuracy = 0.805999994278\n",
      "Training epochs #4302000:   Batch Loss = 5.452349, Accuracy = 0.848000049591\n",
      "Performance on test set: Batch Loss = 5.50590419769, Accuracy = 0.825000047684\n",
      "Training epochs #4304000:   Batch Loss = 5.476888, Accuracy = 0.805000066757\n",
      "Performance on test set: Batch Loss = 5.52923202515, Accuracy = 0.7990000844\n",
      "Training epochs #4306000:   Batch Loss = 5.529569, Accuracy = 0.785000085831\n",
      "Performance on test set: Batch Loss = 5.4568529129, Accuracy = 0.803000032902\n",
      "Training epochs #4308000:   Batch Loss = 5.445477, Accuracy = 0.817000031471\n",
      "Performance on test set: Batch Loss = 5.48580265045, Accuracy = 0.810000002384\n",
      "Training epochs #4310000:   Batch Loss = 5.566453, Accuracy = 0.807000041008\n",
      "Performance on test set: Batch Loss = 5.46360015869, Accuracy = 0.826000034809\n",
      "Training epochs #4312000:   Batch Loss = 5.432598, Accuracy = 0.834999978542\n",
      "Performance on test set: Batch Loss = 5.52490139008, Accuracy = 0.791000068188\n",
      "Training epochs #4314000:   Batch Loss = 5.409274, Accuracy = 0.824000000954\n",
      "Performance on test set: Batch Loss = 5.47719860077, Accuracy = 0.813000023365\n",
      "Training epochs #4316000:   Batch Loss = 5.487704, Accuracy = 0.81299996376\n",
      "Performance on test set: Batch Loss = 5.43690347672, Accuracy = 0.831000030041\n",
      "Training epochs #4318000:   Batch Loss = 5.414770, Accuracy = 0.825000047684\n",
      "Performance on test set: Batch Loss = 5.41398048401, Accuracy = 0.84600007534\n",
      "Training epochs #4320000:   Batch Loss = 5.464675, Accuracy = 0.821000099182\n",
      "Performance on test set: Batch Loss = 5.43377923965, Accuracy = 0.841000080109\n",
      "Training epochs #4322000:   Batch Loss = 5.385598, Accuracy = 0.850000023842\n",
      "Performance on test set: Batch Loss = 5.44164514542, Accuracy = 0.853000044823\n",
      "Training epochs #4324000:   Batch Loss = 5.418550, Accuracy = 0.853000044823\n",
      "Performance on test set: Batch Loss = 5.47850418091, Accuracy = 0.848000049591\n",
      "Training epochs #4326000:   Batch Loss = 5.432962, Accuracy = 0.841000080109\n",
      "Performance on test set: Batch Loss = 5.41044712067, Accuracy = 0.847999989986\n",
      "Training epochs #4328000:   Batch Loss = 5.466644, Accuracy = 0.837999999523\n",
      "Performance on test set: Batch Loss = 5.43999195099, Accuracy = 0.84500002861\n",
      "Training epochs #4330000:   Batch Loss = 5.489064, Accuracy = 0.836000025272\n",
      "Performance on test set: Batch Loss = 5.41268014908, Accuracy = 0.855000019073\n",
      "Training epochs #4332000:   Batch Loss = 5.415909, Accuracy = 0.853999972343\n",
      "Performance on test set: Batch Loss = 5.48276662827, Accuracy = 0.82900005579\n",
      "Training epochs #4334000:   Batch Loss = 5.418929, Accuracy = 0.853999972343\n",
      "Performance on test set: Batch Loss = 5.434715271, Accuracy = 0.841000020504\n",
      "Training epochs #4336000:   Batch Loss = 5.452104, Accuracy = 0.841999948025\n",
      "Performance on test set: Batch Loss = 5.40522766113, Accuracy = 0.855000019073\n",
      "Training epochs #4338000:   Batch Loss = 5.459625, Accuracy = 0.836000025272\n",
      "Performance on test set: Batch Loss = 5.38537979126, Accuracy = 0.864999949932\n",
      "Training epochs #4340000:   Batch Loss = 5.440972, Accuracy = 0.852000057697\n",
      "Performance on test set: Batch Loss = 5.40357589722, Accuracy = 0.846000015736\n",
      "Training epochs #4342000:   Batch Loss = 5.461174, Accuracy = 0.838999986649\n",
      "Performance on test set: Batch Loss = 5.41480064392, Accuracy = 0.857000112534\n",
      "Training epochs #4344000:   Batch Loss = 5.366868, Accuracy = 0.860999941826\n",
      "Performance on test set: Batch Loss = 5.44371080399, Accuracy = 0.855000019073\n",
      "Training epochs #4346000:   Batch Loss = 5.473204, Accuracy = 0.830999970436\n",
      "Performance on test set: Batch Loss = 5.38304519653, Accuracy = 0.848999977112\n",
      "Training epochs #4348000:   Batch Loss = 5.434073, Accuracy = 0.850999951363\n",
      "Performance on test set: Batch Loss = 5.42256832123, Accuracy = 0.847999930382\n",
      "Training epochs #4350000:   Batch Loss = 5.386629, Accuracy = 0.853000044823\n",
      "Performance on test set: Batch Loss = 5.38857555389, Accuracy = 0.856000065804\n",
      "Training epochs #4352000:   Batch Loss = 5.431549, Accuracy = 0.830000042915\n",
      "Performance on test set: Batch Loss = 5.45608758926, Accuracy = 0.82900005579\n",
      "Training epochs #4354000:   Batch Loss = 5.397486, Accuracy = 0.861000061035\n",
      "Performance on test set: Batch Loss = 5.44207668304, Accuracy = 0.841999948025\n",
      "Training epochs #4356000:   Batch Loss = 5.418887, Accuracy = 0.850000023842\n",
      "Performance on test set: Batch Loss = 5.39402961731, Accuracy = 0.855000019073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #4358000:   Batch Loss = 5.405961, Accuracy = 0.847000002861\n",
      "Performance on test set: Batch Loss = 5.38583707809, Accuracy = 0.865000009537\n",
      "Training epochs #4360000:   Batch Loss = 5.362125, Accuracy = 0.856999993324\n",
      "Performance on test set: Batch Loss = 5.57891273499, Accuracy = 0.811999976635\n",
      "Training epochs #4362000:   Batch Loss = 5.551980, Accuracy = 0.821000039577\n",
      "Performance on test set: Batch Loss = 5.56153869629, Accuracy = 0.825000047684\n",
      "Training epochs #4364000:   Batch Loss = 5.567270, Accuracy = 0.809000015259\n",
      "Performance on test set: Batch Loss = 5.58495855331, Accuracy = 0.797999978065\n",
      "Training epochs #4366000:   Batch Loss = 5.544959, Accuracy = 0.786000013351\n",
      "Performance on test set: Batch Loss = 5.53338384628, Accuracy = 0.799000024796\n",
      "Training epochs #4368000:   Batch Loss = 5.557058, Accuracy = 0.807000041008\n",
      "Performance on test set: Batch Loss = 5.55686187744, Accuracy = 0.810000002384\n",
      "Training epochs #4370000:   Batch Loss = 5.564130, Accuracy = 0.814999997616\n",
      "Performance on test set: Batch Loss = 5.50497531891, Accuracy = 0.828000068665\n",
      "Training epochs #4372000:   Batch Loss = 5.537621, Accuracy = 0.808000087738\n",
      "Performance on test set: Batch Loss = 5.56493330002, Accuracy = 0.789000034332\n",
      "Training epochs #4374000:   Batch Loss = 5.475646, Accuracy = 0.806999981403\n",
      "Performance on test set: Batch Loss = 5.49719953537, Accuracy = 0.837000072002\n",
      "Training epochs #4376000:   Batch Loss = 5.476417, Accuracy = 0.841000020504\n",
      "Performance on test set: Batch Loss = 5.46822166443, Accuracy = 0.851000010967\n",
      "Training epochs #4378000:   Batch Loss = 5.459023, Accuracy = 0.840999960899\n",
      "Performance on test set: Batch Loss = 5.40616846085, Accuracy = 0.865000009537\n",
      "Training epochs #4380000:   Batch Loss = 5.420871, Accuracy = 0.84500002861\n",
      "Performance on test set: Batch Loss = 5.47660779953, Accuracy = 0.833999991417\n",
      "Training epochs #4382000:   Batch Loss = 5.387607, Accuracy = 0.869999945164\n",
      "Performance on test set: Batch Loss = 5.48413467407, Accuracy = 0.846000015736\n",
      "Training epochs #4384000:   Batch Loss = 5.463627, Accuracy = 0.831999957561\n",
      "Performance on test set: Batch Loss = 5.49715614319, Accuracy = 0.836000025272\n",
      "Training epochs #4386000:   Batch Loss = 5.463884, Accuracy = 0.845000088215\n",
      "Performance on test set: Batch Loss = 5.4426202774, Accuracy = 0.829999983311\n",
      "Training epochs #4388000:   Batch Loss = 5.440301, Accuracy = 0.838999986649\n",
      "Performance on test set: Batch Loss = 5.48987770081, Accuracy = 0.817000031471\n",
      "Training epochs #4390000:   Batch Loss = 5.529885, Accuracy = 0.820999979973\n",
      "Performance on test set: Batch Loss = 5.43637990952, Accuracy = 0.842000007629\n",
      "Training epochs #4392000:   Batch Loss = 5.416421, Accuracy = 0.84500002861\n",
      "Performance on test set: Batch Loss = 5.52439832687, Accuracy = 0.805000007153\n",
      "Training epochs #4394000:   Batch Loss = 5.408689, Accuracy = 0.845999956131\n",
      "Performance on test set: Batch Loss = 5.46002483368, Accuracy = 0.836000025272\n",
      "Training epochs #4396000:   Batch Loss = 5.469044, Accuracy = 0.830000042915\n",
      "Performance on test set: Batch Loss = 5.40390300751, Accuracy = 0.842000007629\n",
      "Training epochs #4398000:   Batch Loss = 5.388191, Accuracy = 0.834999978542\n",
      "Performance on test set: Batch Loss = 5.3792014122, Accuracy = 0.846000015736\n",
      "Training epochs #4400000:   Batch Loss = 5.449045, Accuracy = 0.824999988079\n",
      "Performance on test set: Batch Loss = 5.41598939896, Accuracy = 0.824999988079\n",
      "Training epochs #4402000:   Batch Loss = 5.342006, Accuracy = 0.857000052929\n",
      "Performance on test set: Batch Loss = 5.41733646393, Accuracy = 0.839000046253\n",
      "Training epochs #4404000:   Batch Loss = 5.379555, Accuracy = 0.842000007629\n",
      "Performance on test set: Batch Loss = 5.44335985184, Accuracy = 0.837000012398\n",
      "Training epochs #4406000:   Batch Loss = 5.392480, Accuracy = 0.825999975204\n",
      "Performance on test set: Batch Loss = 5.37316131592, Accuracy = 0.836000084877\n",
      "Training epochs #4408000:   Batch Loss = 5.416353, Accuracy = 0.826000094414\n",
      "Performance on test set: Batch Loss = 5.39241838455, Accuracy = 0.8599999547\n",
      "Training epochs #4410000:   Batch Loss = 5.441055, Accuracy = 0.853000044823\n",
      "Performance on test set: Batch Loss = 5.35062026978, Accuracy = 0.875\n",
      "Training epochs #4412000:   Batch Loss = 5.356771, Accuracy = 0.880000054836\n",
      "Performance on test set: Batch Loss = 5.4360909462, Accuracy = 0.847999930382\n",
      "Training epochs #4414000:   Batch Loss = 5.376151, Accuracy = 0.87600004673\n",
      "Performance on test set: Batch Loss = 5.38386201859, Accuracy = 0.870999991894\n",
      "Training epochs #4416000:   Batch Loss = 5.392308, Accuracy = 0.862000048161\n",
      "Performance on test set: Batch Loss = 5.35930013657, Accuracy = 0.870000004768\n",
      "Training epochs #4418000:   Batch Loss = 5.408124, Accuracy = 0.830999970436\n",
      "Performance on test set: Batch Loss = 5.33004522324, Accuracy = 0.865000009537\n",
      "Training epochs #4420000:   Batch Loss = 5.389850, Accuracy = 0.848999977112\n",
      "Performance on test set: Batch Loss = 5.35808086395, Accuracy = 0.84500002861\n",
      "Training epochs #4422000:   Batch Loss = 5.406067, Accuracy = 0.836000084877\n",
      "Performance on test set: Batch Loss = 5.35775089264, Accuracy = 0.855000019073\n",
      "Training epochs #4424000:   Batch Loss = 5.308093, Accuracy = 0.85799998045\n",
      "Performance on test set: Batch Loss = 5.38687992096, Accuracy = 0.854000031948\n",
      "Training epochs #4426000:   Batch Loss = 5.418210, Accuracy = 0.828999996185\n",
      "Performance on test set: Batch Loss = 5.32450962067, Accuracy = 0.848999977112\n",
      "Training epochs #4428000:   Batch Loss = 5.369706, Accuracy = 0.847999989986\n",
      "Performance on test set: Batch Loss = 5.35617446899, Accuracy = 0.846000015736\n",
      "Training epochs #4430000:   Batch Loss = 5.320782, Accuracy = 0.853000104427\n",
      "Performance on test set: Batch Loss = 5.32466506958, Accuracy = 0.856999993324\n",
      "Training epochs #4432000:   Batch Loss = 5.378671, Accuracy = 0.829999983311\n",
      "Performance on test set: Batch Loss = 5.3913526535, Accuracy = 0.830000042915\n",
      "Training epochs #4434000:   Batch Loss = 5.342330, Accuracy = 0.859000086784\n",
      "Performance on test set: Batch Loss = 5.96432447433, Accuracy = 0.22499999404\n",
      "Training epochs #4436000:   Batch Loss = 5.401295, Accuracy = 0.849000036716\n",
      "Performance on test set: Batch Loss = 5.44467020035, Accuracy = 0.847000002861\n",
      "Training epochs #4438000:   Batch Loss = 5.560068, Accuracy = 0.800999999046\n",
      "Performance on test set: Batch Loss = 5.5194940567, Accuracy = 0.787000000477\n",
      "Training epochs #4440000:   Batch Loss = 5.551808, Accuracy = 0.777999997139\n",
      "Performance on test set: Batch Loss = 5.82355833054, Accuracy = 0.749000072479\n",
      "Training epochs #4442000:   Batch Loss = 5.818423, Accuracy = 0.785000085831\n",
      "Performance on test set: Batch Loss = 5.88853120804, Accuracy = 0.768999993801\n",
      "Training epochs #4444000:   Batch Loss = 5.814310, Accuracy = 0.768000006676\n",
      "Performance on test set: Batch Loss = 5.75178909302, Accuracy = 0.762999951839\n",
      "Training epochs #4446000:   Batch Loss = 5.693783, Accuracy = 0.771000027657\n",
      "Performance on test set: Batch Loss = 5.60058927536, Accuracy = 0.78200006485\n",
      "Training epochs #4448000:   Batch Loss = 5.598135, Accuracy = 0.806000053883\n",
      "Performance on test set: Batch Loss = 5.56389427185, Accuracy = 0.801000118256\n",
      "Training epochs #4450000:   Batch Loss = 5.470623, Accuracy = 0.810999929905\n",
      "Performance on test set: Batch Loss = 5.40558576584, Accuracy = 0.822000026703\n",
      "Training epochs #4452000:   Batch Loss = 5.736658, Accuracy = 0.795000016689\n",
      "Performance on test set: Batch Loss = 5.52473783493, Accuracy = 0.792000055313\n",
      "Training epochs #4454000:   Batch Loss = 5.532598, Accuracy = 0.797999978065\n",
      "Performance on test set: Batch Loss = 6.22382879257, Accuracy = 0.193000003695\n",
      "Training epochs #4456000:   Batch Loss = 6.634721, Accuracy = 0.165999993682\n",
      "Performance on test set: Batch Loss = 7.99185276031, Accuracy = 0.0500000007451\n",
      "Training epochs #4458000:   Batch Loss = 8.485849, Accuracy = 0.0359999984503\n",
      "Performance on test set: Batch Loss = 8.11262893677, Accuracy = 0.0460000038147\n",
      "Training epochs #4460000:   Batch Loss = 7.927525, Accuracy = 0.659999966621\n",
      "Performance on test set: Batch Loss = 7.66750621796, Accuracy = 0.628999948502\n",
      "Training epochs #4462000:   Batch Loss = 7.452591, Accuracy = 0.684000015259\n",
      "Performance on test set: Batch Loss = 7.56615257263, Accuracy = 0.662000000477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #4464000:   Batch Loss = 7.888618, Accuracy = 0.638000011444\n",
      "Performance on test set: Batch Loss = 7.88466453552, Accuracy = 0.641999959946\n",
      "Training epochs #4466000:   Batch Loss = 7.691912, Accuracy = 0.650999963284\n",
      "Performance on test set: Batch Loss = 7.52611923218, Accuracy = 0.669000029564\n",
      "Training epochs #4468000:   Batch Loss = 7.473538, Accuracy = 0.666999995708\n",
      "Performance on test set: Batch Loss = 7.56653881073, Accuracy = 0.649999976158\n",
      "Training epochs #4470000:   Batch Loss = 7.361330, Accuracy = 0.646000027657\n",
      "Performance on test set: Batch Loss = 7.26700878143, Accuracy = 0.669000029564\n",
      "Training epochs #4472000:   Batch Loss = 7.273977, Accuracy = 0.651000022888\n",
      "Performance on test set: Batch Loss = 7.33061218262, Accuracy = 0.634999990463\n",
      "Training epochs #4474000:   Batch Loss = 6.954852, Accuracy = 0.676999926567\n",
      "Performance on test set: Batch Loss = 7.05739784241, Accuracy = 0.651999950409\n",
      "Training epochs #4476000:   Batch Loss = 7.214871, Accuracy = 0.636999964714\n",
      "Performance on test set: Batch Loss = 7.01111650467, Accuracy = 0.669000029564\n",
      "Training epochs #4478000:   Batch Loss = 6.833414, Accuracy = 0.681999981403\n",
      "Performance on test set: Batch Loss = 6.79972267151, Accuracy = 0.676999986172\n",
      "Training epochs #4480000:   Batch Loss = 6.764091, Accuracy = 0.65800011158\n",
      "Performance on test set: Batch Loss = 6.74243164062, Accuracy = 0.637000024319\n",
      "Training epochs #4482000:   Batch Loss = 6.606944, Accuracy = 0.659999966621\n",
      "Performance on test set: Batch Loss = 6.5568652153, Accuracy = 0.663999974728\n",
      "Training epochs #4484000:   Batch Loss = 6.634778, Accuracy = 0.652999997139\n",
      "Performance on test set: Batch Loss = 6.60054302216, Accuracy = 0.641000032425\n",
      "Training epochs #4486000:   Batch Loss = 6.567977, Accuracy = 0.620999991894\n",
      "Performance on test set: Batch Loss = 6.39061546326, Accuracy = 0.667999982834\n",
      "Training epochs #4488000:   Batch Loss = 6.519730, Accuracy = 0.617999970913\n",
      "Performance on test set: Batch Loss = 6.40981483459, Accuracy = 0.634999990463\n",
      "Training epochs #4490000:   Batch Loss = 6.312292, Accuracy = 0.650999903679\n",
      "Performance on test set: Batch Loss = 6.27403402328, Accuracy = 0.659999966621\n",
      "Training epochs #4492000:   Batch Loss = 6.186146, Accuracy = 0.639000058174\n",
      "Performance on test set: Batch Loss = 6.28121376038, Accuracy = 0.623000025749\n",
      "Training epochs #4494000:   Batch Loss = 6.211791, Accuracy = 0.632999956608\n",
      "Performance on test set: Batch Loss = 6.13415288925, Accuracy = 0.639000058174\n",
      "Training epochs #4496000:   Batch Loss = 6.120060, Accuracy = 0.634000003338\n",
      "Performance on test set: Batch Loss = 6.04826784134, Accuracy = 0.659999966621\n",
      "Training epochs #4498000:   Batch Loss = 6.111312, Accuracy = 0.629000008106\n",
      "Performance on test set: Batch Loss = 6.04175853729, Accuracy = 0.659000039101\n",
      "Training epochs #4500000:   Batch Loss = 6.027370, Accuracy = 0.648000061512\n",
      "Performance on test set: Batch Loss = 6.06088018417, Accuracy = 0.629000008106\n",
      "Training epochs #4502000:   Batch Loss = 6.047022, Accuracy = 0.631999969482\n",
      "Performance on test set: Batch Loss = 6.00430822372, Accuracy = 0.660000026226\n",
      "Training epochs #4504000:   Batch Loss = 6.028143, Accuracy = 0.643999934196\n",
      "Performance on test set: Batch Loss = 6.02953338623, Accuracy = 0.639999985695\n",
      "Training epochs #4506000:   Batch Loss = 6.051117, Accuracy = 0.635999977589\n",
      "Performance on test set: Batch Loss = 5.97771644592, Accuracy = 0.668999969959\n",
      "Training epochs #4508000:   Batch Loss = 6.026343, Accuracy = 0.635000050068\n",
      "Performance on test set: Batch Loss = 6.0434217453, Accuracy = 0.634999990463\n",
      "Training epochs #4510000:   Batch Loss = 5.969175, Accuracy = 0.638999998569\n",
      "Performance on test set: Batch Loss = 5.96985149384, Accuracy = 0.663999974728\n",
      "Training epochs #4512000:   Batch Loss = 5.962837, Accuracy = 0.64300006628\n",
      "Performance on test set: Batch Loss = 6.030626297, Accuracy = 0.62600004673\n",
      "Training epochs #4514000:   Batch Loss = 5.947606, Accuracy = 0.646999955177\n",
      "Performance on test set: Batch Loss = 5.97558307648, Accuracy = 0.641000032425\n",
      "Training epochs #4516000:   Batch Loss = 5.953042, Accuracy = 0.651999950409\n",
      "Performance on test set: Batch Loss = 5.90203619003, Accuracy = 0.657000005245\n",
      "Training epochs #4518000:   Batch Loss = 5.945547, Accuracy = 0.662000000477\n",
      "Performance on test set: Batch Loss = 5.9296131134, Accuracy = 0.657999992371\n",
      "Training epochs #4520000:   Batch Loss = 5.886937, Accuracy = 0.675000011921\n",
      "Performance on test set: Batch Loss = 5.95461750031, Accuracy = 0.629000008106\n",
      "Training epochs #4522000:   Batch Loss = 5.897939, Accuracy = 0.659000039101\n",
      "Performance on test set: Batch Loss = 5.90653514862, Accuracy = 0.660000085831\n",
      "Training epochs #4524000:   Batch Loss = 5.918271, Accuracy = 0.655000030994\n",
      "Performance on test set: Batch Loss = 5.93762683868, Accuracy = 0.639999985695\n",
      "Training epochs #4526000:   Batch Loss = 5.888075, Accuracy = 0.648999989033\n",
      "Performance on test set: Batch Loss = 5.86807823181, Accuracy = 0.670000016689\n",
      "Training epochs #4528000:   Batch Loss = 5.892195, Accuracy = 0.659999966621\n",
      "Performance on test set: Batch Loss = 5.9101858139, Accuracy = 0.657999992371\n",
      "Training epochs #4530000:   Batch Loss = 5.883600, Accuracy = 0.657999992371\n",
      "Performance on test set: Batch Loss = 5.86231040955, Accuracy = 0.68900001049\n",
      "Training epochs #4532000:   Batch Loss = 5.881651, Accuracy = 0.665999948978\n",
      "Performance on test set: Batch Loss = 5.91110610962, Accuracy = 0.636999964714\n",
      "Training epochs #4534000:   Batch Loss = 5.896183, Accuracy = 0.666999995708\n",
      "Performance on test set: Batch Loss = 5.86186122894, Accuracy = 0.680000007153\n",
      "Training epochs #4536000:   Batch Loss = 5.827825, Accuracy = 0.700000047684\n",
      "Performance on test set: Batch Loss = 5.80992889404, Accuracy = 0.695000052452\n",
      "Training epochs #4538000:   Batch Loss = 5.835674, Accuracy = 0.699999988079\n",
      "Performance on test set: Batch Loss = 5.83467626572, Accuracy = 0.697000026703\n",
      "Training epochs #4540000:   Batch Loss = 5.839343, Accuracy = 0.669999957085\n",
      "Performance on test set: Batch Loss = 5.86140918732, Accuracy = 0.676999986172\n",
      "Training epochs #4542000:   Batch Loss = 5.758467, Accuracy = 0.715000033379\n",
      "Performance on test set: Batch Loss = 5.83530807495, Accuracy = 0.696000039577\n",
      "Training epochs #4544000:   Batch Loss = 5.850224, Accuracy = 0.676999986172\n",
      "Performance on test set: Batch Loss = 5.88085746765, Accuracy = 0.672999978065\n",
      "Training epochs #4546000:   Batch Loss = 5.849363, Accuracy = 0.678000032902\n",
      "Performance on test set: Batch Loss = 5.79835891724, Accuracy = 0.705999970436\n",
      "Training epochs #4548000:   Batch Loss = 5.786253, Accuracy = 0.70199996233\n",
      "Performance on test set: Batch Loss = 5.83841085434, Accuracy = 0.68700003624\n",
      "Training epochs #4550000:   Batch Loss = 5.863523, Accuracy = 0.673999965191\n",
      "Performance on test set: Batch Loss = 5.8022441864, Accuracy = 0.694999992847\n",
      "Training epochs #4552000:   Batch Loss = 5.780827, Accuracy = 0.685000061989\n",
      "Performance on test set: Batch Loss = 5.84579372406, Accuracy = 0.672999978065\n",
      "Training epochs #4554000:   Batch Loss = 5.757115, Accuracy = 0.703999996185\n",
      "Performance on test set: Batch Loss = 5.81126022339, Accuracy = 0.680000007153\n",
      "Training epochs #4556000:   Batch Loss = 5.828718, Accuracy = 0.661000013351\n",
      "Performance on test set: Batch Loss = 5.75526428223, Accuracy = 0.695999979973\n",
      "Training epochs #4558000:   Batch Loss = 5.749095, Accuracy = 0.706999957561\n",
      "Performance on test set: Batch Loss = 5.77994346619, Accuracy = 0.696999967098\n",
      "Training epochs #4560000:   Batch Loss = 5.797140, Accuracy = 0.690999984741\n",
      "Performance on test set: Batch Loss = 5.80078411102, Accuracy = 0.676999986172\n",
      "Training epochs #4562000:   Batch Loss = 5.748035, Accuracy = 0.699000000954\n",
      "Performance on test set: Batch Loss = 5.78332281113, Accuracy = 0.697000026703\n",
      "Training epochs #4564000:   Batch Loss = 5.771193, Accuracy = 0.683000028133\n",
      "Performance on test set: Batch Loss = 5.83083248138, Accuracy = 0.674999952316\n",
      "Training epochs #4566000:   Batch Loss = 5.840829, Accuracy = 0.664999961853\n",
      "Performance on test set: Batch Loss = 5.75515270233, Accuracy = 0.705999970436\n",
      "Training epochs #4568000:   Batch Loss = 5.817043, Accuracy = 0.662000060081\n",
      "Performance on test set: Batch Loss = 5.79404735565, Accuracy = 0.686999976635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #4570000:   Batch Loss = 5.793993, Accuracy = 0.682000041008\n",
      "Performance on test set: Batch Loss = 5.79594945908, Accuracy = 0.668999969959\n",
      "Training epochs #4572000:   Batch Loss = 5.817024, Accuracy = 0.65600001812\n",
      "Performance on test set: Batch Loss = 5.82908964157, Accuracy = 0.636999964714\n",
      "Training epochs #4574000:   Batch Loss = 5.801332, Accuracy = 0.683000028133\n",
      "Performance on test set: Batch Loss = 5.78729438782, Accuracy = 0.680000007153\n",
      "Training epochs #4576000:   Batch Loss = 5.796617, Accuracy = 0.679999947548\n",
      "Performance on test set: Batch Loss = 5.80683803558, Accuracy = 0.67300003767\n",
      "Training epochs #4578000:   Batch Loss = 5.907329, Accuracy = 0.646000027657\n",
      "Performance on test set: Batch Loss = 5.83133983612, Accuracy = 0.677999973297\n",
      "Training epochs #4580000:   Batch Loss = 5.857876, Accuracy = 0.659000039101\n",
      "Performance on test set: Batch Loss = 5.91482877731, Accuracy = 0.64099997282\n",
      "Training epochs #4582000:   Batch Loss = 5.893194, Accuracy = 0.645999968052\n",
      "Performance on test set: Batch Loss = 5.79642534256, Accuracy = 0.667999982834\n",
      "Training epochs #4584000:   Batch Loss = 5.767529, Accuracy = 0.68499994278\n",
      "Performance on test set: Batch Loss = 6.17136573792, Accuracy = 0.675000011921\n",
      "Training epochs #4586000:   Batch Loss = 5.960374, Accuracy = 0.646000027657\n",
      "Performance on test set: Batch Loss = 6.02971839905, Accuracy = 0.670000016689\n",
      "Training epochs #4588000:   Batch Loss = 6.133196, Accuracy = 0.638999998569\n",
      "Performance on test set: Batch Loss = 6.16609477997, Accuracy = 0.637000024319\n",
      "Training epochs #4590000:   Batch Loss = 6.156565, Accuracy = 0.639999985695\n",
      "Performance on test set: Batch Loss = 6.05006551743, Accuracy = 0.659999966621\n",
      "Training epochs #4592000:   Batch Loss = 6.075631, Accuracy = 0.64200001955\n",
      "Performance on test set: Batch Loss = 6.19380569458, Accuracy = 0.620000004768\n",
      "Training epochs #4594000:   Batch Loss = 6.142223, Accuracy = 0.643999993801\n",
      "Performance on test set: Batch Loss = 6.15499544144, Accuracy = 0.637000024319\n",
      "Training epochs #4596000:   Batch Loss = 6.064122, Accuracy = 0.652000010014\n",
      "Performance on test set: Batch Loss = 6.03435134888, Accuracy = 0.657000005245\n",
      "Training epochs #4598000:   Batch Loss = 6.076062, Accuracy = 0.660999953747\n",
      "Performance on test set: Batch Loss = 6.02694892883, Accuracy = 0.656999945641\n",
      "Training epochs #4600000:   Batch Loss = 5.960581, Accuracy = 0.669999957085\n",
      "Performance on test set: Batch Loss = 6.09901428223, Accuracy = 0.625\n",
      "Training epochs #4602000:   Batch Loss = 6.032854, Accuracy = 0.65499997139\n",
      "Performance on test set: Batch Loss = 6.00854873657, Accuracy = 0.656999945641\n",
      "Training epochs #4604000:   Batch Loss = 6.032029, Accuracy = 0.648000001907\n",
      "Performance on test set: Batch Loss = 6.01499509811, Accuracy = 0.637000024319\n",
      "Training epochs #4606000:   Batch Loss = 5.992812, Accuracy = 0.644999980927\n",
      "Performance on test set: Batch Loss = 5.95416545868, Accuracy = 0.666000008583\n",
      "Training epochs #4608000:   Batch Loss = 6.034855, Accuracy = 0.629999995232\n",
      "Performance on test set: Batch Loss = 6.03727674484, Accuracy = 0.631999969482\n",
      "Training epochs #4610000:   Batch Loss = 5.999887, Accuracy = 0.637000024319\n",
      "Performance on test set: Batch Loss = 5.94793510437, Accuracy = 0.660000026226\n",
      "Training epochs #4612000:   Batch Loss = 5.990759, Accuracy = 0.651999950409\n",
      "Performance on test set: Batch Loss = 6.0471830368, Accuracy = 0.620000004768\n",
      "Training epochs #4614000:   Batch Loss = 6.024980, Accuracy = 0.632000029087\n",
      "Performance on test set: Batch Loss = 6.02136993408, Accuracy = 0.636000037193\n",
      "Training epochs #4616000:   Batch Loss = 5.955748, Accuracy = 0.659999966621\n",
      "Performance on test set: Batch Loss = 5.9373922348, Accuracy = 0.65600001812\n",
      "Training epochs #4618000:   Batch Loss = 5.942249, Accuracy = 0.661000013351\n",
      "Performance on test set: Batch Loss = 5.9522895813, Accuracy = 0.657000005245\n",
      "Training epochs #4620000:   Batch Loss = 5.978758, Accuracy = 0.636999964714\n",
      "Performance on test set: Batch Loss = 6.01604509354, Accuracy = 0.625\n",
      "Training epochs #4622000:   Batch Loss = 5.893607, Accuracy = 0.678999960423\n",
      "Performance on test set: Batch Loss = 5.96205997467, Accuracy = 0.657000005245\n",
      "Training epochs #4624000:   Batch Loss = 5.988811, Accuracy = 0.633999943733\n",
      "Performance on test set: Batch Loss = 5.98684883118, Accuracy = 0.637000024319\n",
      "Training epochs #4626000:   Batch Loss = 5.966011, Accuracy = 0.647000014782\n",
      "Performance on test set: Batch Loss = 5.92756080627, Accuracy = 0.666000008583\n",
      "Training epochs #4628000:   Batch Loss = 5.935400, Accuracy = 0.662000000477\n",
      "Performance on test set: Batch Loss = 6.00236129761, Accuracy = 0.631999969482\n",
      "Training epochs #4630000:   Batch Loss = 6.025330, Accuracy = 0.629999995232\n",
      "Performance on test set: Batch Loss = 5.92249155045, Accuracy = 0.660000026226\n",
      "Training epochs #4632000:   Batch Loss = 5.941816, Accuracy = 0.633999943733\n",
      "Performance on test set: Batch Loss = 6.02198076248, Accuracy = 0.620000004768\n",
      "Training epochs #4634000:   Batch Loss = 5.917553, Accuracy = 0.668999969959\n",
      "Performance on test set: Batch Loss = 5.99471139908, Accuracy = 0.636000037193\n",
      "Training epochs #4636000:   Batch Loss = 6.001637, Accuracy = 0.618999958038\n",
      "Performance on test set: Batch Loss = 5.90493774414, Accuracy = 0.656999945641\n",
      "Training epochs #4638000:   Batch Loss = 5.928298, Accuracy = 0.665000021458\n",
      "Performance on test set: Batch Loss = 5.91972732544, Accuracy = 0.656999945641\n",
      "Training epochs #4640000:   Batch Loss = 5.945793, Accuracy = 0.645000040531\n",
      "Performance on test set: Batch Loss = 5.99010276794, Accuracy = 0.625\n",
      "Training epochs #4642000:   Batch Loss = 5.933363, Accuracy = 0.65700006485\n",
      "Performance on test set: Batch Loss = 5.93315315247, Accuracy = 0.657000005245\n",
      "Training epochs #4644000:   Batch Loss = 5.913448, Accuracy = 0.647999942303\n",
      "Performance on test set: Batch Loss = 5.95382118225, Accuracy = 0.636999964714\n",
      "Training epochs #4646000:   Batch Loss = 6.000056, Accuracy = 0.617999970913\n",
      "Performance on test set: Batch Loss = 5.90103721619, Accuracy = 0.666999995708\n",
      "Training epochs #4648000:   Batch Loss = 5.981895, Accuracy = 0.615999996662\n",
      "Performance on test set: Batch Loss = 5.97569847107, Accuracy = 0.631999969482\n",
      "Training epochs #4650000:   Batch Loss = 5.943468, Accuracy = 0.647999942303\n",
      "Performance on test set: Batch Loss = 5.89572238922, Accuracy = 0.660000026226\n",
      "Training epochs #4652000:   Batch Loss = 5.950786, Accuracy = 0.636000037193\n",
      "Performance on test set: Batch Loss = 5.99340772629, Accuracy = 0.621000051498\n",
      "Training epochs #4654000:   Batch Loss = 5.972929, Accuracy = 0.631999969482\n",
      "Performance on test set: Batch Loss = 5.96759939194, Accuracy = 0.637000024319\n",
      "Training epochs #4656000:   Batch Loss = 5.965768, Accuracy = 0.630999982357\n",
      "Performance on test set: Batch Loss = 5.87654399872, Accuracy = 0.65700006485\n",
      "Training epochs #4658000:   Batch Loss = 5.977093, Accuracy = 0.625\n",
      "Performance on test set: Batch Loss = 5.88893938065, Accuracy = 0.656999945641\n",
      "Training epochs #4660000:   Batch Loss = 5.912339, Accuracy = 0.646000027657\n",
      "Performance on test set: Batch Loss = 5.95783519745, Accuracy = 0.625\n",
      "Training epochs #4662000:   Batch Loss = 5.951446, Accuracy = 0.633000016212\n",
      "Performance on test set: Batch Loss = 5.90219974518, Accuracy = 0.657000005245\n",
      "Training epochs #4664000:   Batch Loss = 5.915616, Accuracy = 0.638999938965\n",
      "Performance on test set: Batch Loss = 5.92280578613, Accuracy = 0.637000024319\n",
      "Training epochs #4666000:   Batch Loss = 5.964108, Accuracy = 0.634000062943\n",
      "Performance on test set: Batch Loss = 5.86868047714, Accuracy = 0.667000055313\n",
      "Training epochs #4668000:   Batch Loss = 5.929463, Accuracy = 0.633000016212\n",
      "Performance on test set: Batch Loss = 5.93686103821, Accuracy = 0.632999897003\n",
      "Training epochs #4670000:   Batch Loss = 5.919860, Accuracy = 0.638000011444\n",
      "Performance on test set: Batch Loss = 5.86105394363, Accuracy = 0.660000026226\n",
      "Training epochs #4672000:   Batch Loss = 5.882339, Accuracy = 0.643000006676\n",
      "Performance on test set: Batch Loss = 5.95551109314, Accuracy = 0.621000051498\n",
      "Training epochs #4674000:   Batch Loss = 5.900554, Accuracy = 0.642999947071\n",
      "Performance on test set: Batch Loss = 5.93158721924, Accuracy = 0.636999964714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #4676000:   Batch Loss = 5.866678, Accuracy = 0.651000022888\n",
      "Performance on test set: Batch Loss = 5.83542346954, Accuracy = 0.657000005245\n",
      "Training epochs #4678000:   Batch Loss = 5.875547, Accuracy = 0.661000013351\n",
      "Performance on test set: Batch Loss = 5.84443569183, Accuracy = 0.656999945641\n",
      "Training epochs #4680000:   Batch Loss = 5.810898, Accuracy = 0.670000076294\n",
      "Performance on test set: Batch Loss = 5.91255283356, Accuracy = 0.624999940395\n",
      "Training epochs #4682000:   Batch Loss = 5.855649, Accuracy = 0.65499997139\n",
      "Performance on test set: Batch Loss = 5.85595178604, Accuracy = 0.657000005245\n",
      "Training epochs #4684000:   Batch Loss = 5.876218, Accuracy = 0.654000043869\n",
      "Performance on test set: Batch Loss = 5.8715467453, Accuracy = 0.639999985695\n",
      "Training epochs #4686000:   Batch Loss = 5.838267, Accuracy = 0.650000035763\n",
      "Performance on test set: Batch Loss = 5.81257247925, Accuracy = 0.675999999046\n",
      "Training epochs #4688000:   Batch Loss = 5.883700, Accuracy = 0.638000011444\n",
      "Performance on test set: Batch Loss = 5.87212848663, Accuracy = 0.634000062943\n",
      "Training epochs #4690000:   Batch Loss = 5.851603, Accuracy = 0.639999985695\n",
      "Performance on test set: Batch Loss = 5.79891633987, Accuracy = 0.664000034332\n",
      "Training epochs #4692000:   Batch Loss = 5.848729, Accuracy = 0.652999997139\n",
      "Performance on test set: Batch Loss = 5.88391876221, Accuracy = 0.738000035286\n",
      "Training epochs #4694000:   Batch Loss = 5.854452, Accuracy = 0.752999961376\n",
      "Performance on test set: Batch Loss = 5.86648082733, Accuracy = 0.743000030518\n",
      "Training epochs #4696000:   Batch Loss = 5.786406, Accuracy = 0.763000011444\n",
      "Performance on test set: Batch Loss = 5.76078176498, Accuracy = 0.78100001812\n",
      "Training epochs #4698000:   Batch Loss = 5.752668, Accuracy = 0.773999989033\n",
      "Performance on test set: Batch Loss = 5.76262950897, Accuracy = 0.767000079155\n",
      "Training epochs #4700000:   Batch Loss = 5.787120, Accuracy = 0.759999990463\n",
      "Performance on test set: Batch Loss = 5.82706689835, Accuracy = 0.738000035286\n",
      "Training epochs #4702000:   Batch Loss = 5.702892, Accuracy = 0.788000047207\n",
      "Performance on test set: Batch Loss = 5.77209615707, Accuracy = 0.756999969482\n",
      "Training epochs #4704000:   Batch Loss = 5.783847, Accuracy = 0.746999979019\n",
      "Performance on test set: Batch Loss = 5.78448343277, Accuracy = 0.759000062943\n",
      "Training epochs #4706000:   Batch Loss = 5.776421, Accuracy = 0.751999974251\n",
      "Performance on test set: Batch Loss = 5.7242398262, Accuracy = 0.76599997282\n",
      "Training epochs #4708000:   Batch Loss = 5.731088, Accuracy = 0.766000032425\n",
      "Performance on test set: Batch Loss = 5.77020931244, Accuracy = 0.746999979019\n",
      "Training epochs #4710000:   Batch Loss = 5.831359, Accuracy = 0.736000061035\n",
      "Performance on test set: Batch Loss = 5.70188856125, Accuracy = 0.773999929428\n",
      "Training epochs #4712000:   Batch Loss = 5.712050, Accuracy = 0.759999990463\n",
      "Performance on test set: Batch Loss = 5.78647994995, Accuracy = 0.742000043392\n",
      "Training epochs #4714000:   Batch Loss = 5.691646, Accuracy = 0.769999980927\n",
      "Performance on test set: Batch Loss = 5.76939249039, Accuracy = 0.742999970913\n",
      "Training epochs #4716000:   Batch Loss = 5.762150, Accuracy = 0.75200009346\n",
      "Performance on test set: Batch Loss = 5.66262388229, Accuracy = 0.780000090599\n",
      "Training epochs #4718000:   Batch Loss = 5.682857, Accuracy = 0.763999998569\n",
      "Performance on test set: Batch Loss = 5.66289758682, Accuracy = 0.769000053406\n",
      "Training epochs #4720000:   Batch Loss = 5.695444, Accuracy = 0.750999987125\n",
      "Performance on test set: Batch Loss = 5.72653865814, Accuracy = 0.737000048161\n",
      "Training epochs #4722000:   Batch Loss = 5.672014, Accuracy = 0.76599997282\n",
      "Performance on test set: Batch Loss = 5.6741476059, Accuracy = 0.755999982357\n",
      "Training epochs #4724000:   Batch Loss = 5.636133, Accuracy = 0.773999989033\n",
      "Performance on test set: Batch Loss = 5.68235874176, Accuracy = 0.761000037193\n",
      "Training epochs #4726000:   Batch Loss = 5.719231, Accuracy = 0.726000010967\n",
      "Performance on test set: Batch Loss = 5.62680387497, Accuracy = 0.763999938965\n",
      "Training epochs #4728000:   Batch Loss = 5.689858, Accuracy = 0.740999996662\n",
      "Performance on test set: Batch Loss = 5.67234945297, Accuracy = 0.745000004768\n",
      "Training epochs #4730000:   Batch Loss = 5.665698, Accuracy = 0.755999982357\n",
      "Performance on test set: Batch Loss = 5.60429763794, Accuracy = 0.772000014782\n",
      "Training epochs #4732000:   Batch Loss = 5.655032, Accuracy = 0.755000054836\n",
      "Performance on test set: Batch Loss = 5.69005632401, Accuracy = 0.740999996662\n",
      "Training epochs #4734000:   Batch Loss = 5.660082, Accuracy = 0.743000030518\n",
      "Performance on test set: Batch Loss = 5.66574144363, Accuracy = 0.743999958038\n",
      "Training epochs #4736000:   Batch Loss = 5.666454, Accuracy = 0.736000061035\n",
      "Performance on test set: Batch Loss = 5.57529067993, Accuracy = 0.780000030994\n",
      "Training epochs #4738000:   Batch Loss = 5.671669, Accuracy = 0.73299998045\n",
      "Performance on test set: Batch Loss = 5.57019805908, Accuracy = 0.768999993801\n",
      "Training epochs #4740000:   Batch Loss = 5.596622, Accuracy = 0.760999977589\n",
      "Performance on test set: Batch Loss = 5.63139104843, Accuracy = 0.737000048161\n",
      "Training epochs #4742000:   Batch Loss = 5.638768, Accuracy = 0.743000030518\n",
      "Performance on test set: Batch Loss = 5.583319664, Accuracy = 0.757000029087\n",
      "Training epochs #4744000:   Batch Loss = 5.587747, Accuracy = 0.753000020981\n",
      "Performance on test set: Batch Loss = 5.59407234192, Accuracy = 0.761999964714\n",
      "Training epochs #4746000:   Batch Loss = 5.642355, Accuracy = 0.739999949932\n",
      "Performance on test set: Batch Loss = 5.55004310608, Accuracy = 0.763999998569\n",
      "Training epochs #4748000:   Batch Loss = 5.591294, Accuracy = 0.740000009537\n",
      "Performance on test set: Batch Loss = 5.59452772141, Accuracy = 0.747000038624\n",
      "Training epochs #4750000:   Batch Loss = 5.577375, Accuracy = 0.755999982357\n",
      "Performance on test set: Batch Loss = 5.52865362167, Accuracy = 0.774000048637\n",
      "Training epochs #4752000:   Batch Loss = 5.550791, Accuracy = 0.758000075817\n",
      "Performance on test set: Batch Loss = 5.62467765808, Accuracy = 0.743999958038\n",
      "Training epochs #4754000:   Batch Loss = 5.592599, Accuracy = 0.754999995232\n",
      "Performance on test set: Batch Loss = 5.63010406494, Accuracy = 0.743999958038\n",
      "Training epochs #4756000:   Batch Loss = 5.556382, Accuracy = 0.764999985695\n",
      "Performance on test set: Batch Loss = 5.72700881958, Accuracy = 0.784000039101\n",
      "Training epochs #4758000:   Batch Loss = 5.696580, Accuracy = 0.756999969482\n",
      "Performance on test set: Batch Loss = 5.87624454498, Accuracy = 0.657999992371\n",
      "Training epochs #4760000:   Batch Loss = 6.019488, Accuracy = 0.670000076294\n",
      "Performance on test set: Batch Loss = 6.15183973312, Accuracy = 0.625\n",
      "Training epochs #4762000:   Batch Loss = 6.089915, Accuracy = 0.654000043869\n",
      "Performance on test set: Batch Loss = 6.06028270721, Accuracy = 0.657000005245\n",
      "Training epochs #4764000:   Batch Loss = 6.057619, Accuracy = 0.648000001907\n",
      "Performance on test set: Batch Loss = 6.04720211029, Accuracy = 0.638000011444\n",
      "Training epochs #4766000:   Batch Loss = 5.937812, Accuracy = 0.644999980927\n",
      "Performance on test set: Batch Loss = 5.8441157341, Accuracy = 0.667000055313\n",
      "Training epochs #4768000:   Batch Loss = 5.856720, Accuracy = 0.630999982357\n",
      "Performance on test set: Batch Loss = 5.83754825592, Accuracy = 0.633999943733\n",
      "Training epochs #4770000:   Batch Loss = 5.811483, Accuracy = 0.755000054836\n",
      "Performance on test set: Batch Loss = 5.78351211548, Accuracy = 0.774000048637\n",
      "Training epochs #4772000:   Batch Loss = 5.861800, Accuracy = 0.740000009537\n",
      "Performance on test set: Batch Loss = 5.89778709412, Accuracy = 0.743000030518\n",
      "Training epochs #4774000:   Batch Loss = 5.886645, Accuracy = 0.754000008106\n",
      "Performance on test set: Batch Loss = 5.89041090012, Accuracy = 0.742999911308\n",
      "Training epochs #4776000:   Batch Loss = 5.790747, Accuracy = 0.76499992609\n",
      "Performance on test set: Batch Loss = 5.76262760162, Accuracy = 0.78200006485\n",
      "Training epochs #4778000:   Batch Loss = 5.748219, Accuracy = 0.772999942303\n",
      "Performance on test set: Batch Loss = 5.76131486893, Accuracy = 0.76800006628\n",
      "Training epochs #4780000:   Batch Loss = 5.781024, Accuracy = 0.759999990463\n",
      "Performance on test set: Batch Loss = 5.84898376465, Accuracy = 0.736999988556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #4782000:   Batch Loss = 5.693836, Accuracy = 0.682999908924\n",
      "Performance on test set: Batch Loss = 5.77858161926, Accuracy = 0.657999992371\n",
      "Training epochs #4784000:   Batch Loss = 5.800077, Accuracy = 0.638999998569\n",
      "Performance on test set: Batch Loss = 5.77627134323, Accuracy = 0.757999956608\n",
      "Training epochs #4786000:   Batch Loss = 5.741315, Accuracy = 0.754000008106\n",
      "Performance on test set: Batch Loss = 5.70122528076, Accuracy = 0.76700001955\n",
      "Training epochs #4788000:   Batch Loss = 5.710914, Accuracy = 0.7650000453\n",
      "Performance on test set: Batch Loss = 5.76774597168, Accuracy = 0.749000072479\n",
      "Training epochs #4790000:   Batch Loss = 5.809780, Accuracy = 0.736000061035\n",
      "Performance on test set: Batch Loss = 5.69347190857, Accuracy = 0.774999976158\n",
      "Training epochs #4792000:   Batch Loss = 5.696549, Accuracy = 0.761999964714\n",
      "Performance on test set: Batch Loss = 5.76666307449, Accuracy = 0.743000030518\n",
      "Training epochs #4794000:   Batch Loss = 5.671751, Accuracy = 0.770000040531\n",
      "Performance on test set: Batch Loss = 5.73980617523, Accuracy = 0.743000030518\n",
      "Training epochs #4796000:   Batch Loss = 5.728789, Accuracy = 0.751999974251\n",
      "Performance on test set: Batch Loss = 5.63889980316, Accuracy = 0.78100001812\n",
      "Training epochs #4798000:   Batch Loss = 5.660167, Accuracy = 0.764999985695\n",
      "Performance on test set: Batch Loss = 5.65261459351, Accuracy = 0.768000006676\n",
      "Training epochs #4800000:   Batch Loss = 5.694995, Accuracy = 0.750000059605\n",
      "Performance on test set: Batch Loss = 5.71957302094, Accuracy = 0.739999949932\n",
      "Training epochs #4802000:   Batch Loss = 5.657341, Accuracy = 0.764999985695\n",
      "Performance on test set: Batch Loss = 5.67003679276, Accuracy = 0.755999982357\n",
      "Training epochs #4804000:   Batch Loss = 5.614986, Accuracy = 0.775999963284\n",
      "Performance on test set: Batch Loss = 5.67230987549, Accuracy = 0.759999990463\n",
      "Training epochs #4806000:   Batch Loss = 5.708054, Accuracy = 0.726999998093\n",
      "Performance on test set: Batch Loss = 5.6104798317, Accuracy = 0.766000032425\n",
      "Training epochs #4808000:   Batch Loss = 5.675320, Accuracy = 0.740000009537\n",
      "Performance on test set: Batch Loss = 5.66118001938, Accuracy = 0.747000038624\n",
      "Training epochs #4810000:   Batch Loss = 5.648562, Accuracy = 0.755999982357\n",
      "Performance on test set: Batch Loss = 5.58723163605, Accuracy = 0.773000001907\n",
      "Training epochs #4812000:   Batch Loss = 5.634338, Accuracy = 0.754999995232\n",
      "Performance on test set: Batch Loss = 5.66189432144, Accuracy = 0.741999983788\n",
      "Training epochs #4814000:   Batch Loss = 5.644843, Accuracy = 0.742000043392\n",
      "Performance on test set: Batch Loss = 5.64615726471, Accuracy = 0.742999970913\n",
      "Training epochs #4816000:   Batch Loss = 5.653805, Accuracy = 0.735000014305\n",
      "Performance on test set: Batch Loss = 5.54778051376, Accuracy = 0.778000056744\n",
      "Training epochs #4818000:   Batch Loss = 5.655535, Accuracy = 0.731999993324\n",
      "Performance on test set: Batch Loss = 5.55185031891, Accuracy = 0.76599997282\n",
      "Training epochs #4820000:   Batch Loss = 5.581957, Accuracy = 0.760999977589\n",
      "Performance on test set: Batch Loss = 5.61212396622, Accuracy = 0.739000022411\n",
      "Training epochs #4822000:   Batch Loss = 5.612016, Accuracy = 0.743000030518\n",
      "Performance on test set: Batch Loss = 5.56932258606, Accuracy = 0.756999969482\n",
      "Training epochs #4824000:   Batch Loss = 5.555529, Accuracy = 0.752999961376\n",
      "Performance on test set: Batch Loss = 5.58039999008, Accuracy = 0.761999964714\n",
      "Training epochs #4826000:   Batch Loss = 5.622556, Accuracy = 0.740999996662\n",
      "Performance on test set: Batch Loss = 5.51874160767, Accuracy = 0.763999998569\n",
      "Training epochs #4828000:   Batch Loss = 5.576554, Accuracy = 0.741000056267\n",
      "Performance on test set: Batch Loss = 5.56265354156, Accuracy = 0.75\n",
      "Training epochs #4830000:   Batch Loss = 5.551020, Accuracy = 0.756000041962\n",
      "Performance on test set: Batch Loss = 5.48878622055, Accuracy = 0.775000035763\n",
      "Training epochs #4832000:   Batch Loss = 5.510327, Accuracy = 0.756000041962\n",
      "Performance on test set: Batch Loss = 5.571829319, Accuracy = 0.745000004768\n",
      "Training epochs #4834000:   Batch Loss = 5.519201, Accuracy = 0.756000041962\n",
      "Performance on test set: Batch Loss = 5.53537654877, Accuracy = 0.745000004768\n",
      "Training epochs #4836000:   Batch Loss = 5.493693, Accuracy = 0.769999980927\n",
      "Performance on test set: Batch Loss = 5.45353984833, Accuracy = 0.784000039101\n",
      "Training epochs #4838000:   Batch Loss = 5.484793, Accuracy = 0.76599997282\n",
      "Performance on test set: Batch Loss = 5.44456577301, Accuracy = 0.773000001907\n",
      "Training epochs #4840000:   Batch Loss = 5.416554, Accuracy = 0.77999997139\n",
      "Performance on test set: Batch Loss = 5.50465965271, Accuracy = 0.742999970913\n",
      "Training epochs #4842000:   Batch Loss = 5.465545, Accuracy = 0.774999976158\n",
      "Performance on test set: Batch Loss = 5.46244287491, Accuracy = 0.764999985695\n",
      "Training epochs #4844000:   Batch Loss = 5.479291, Accuracy = 0.769000053406\n",
      "Performance on test set: Batch Loss = 5.47409725189, Accuracy = 0.768000006676\n",
      "Training epochs #4846000:   Batch Loss = 5.434495, Accuracy = 0.770000100136\n",
      "Performance on test set: Batch Loss = 5.41661500931, Accuracy = 0.772999942303\n",
      "Training epochs #4848000:   Batch Loss = 5.453442, Accuracy = 0.752000033855\n",
      "Performance on test set: Batch Loss = 5.46160984039, Accuracy = 0.755000054836\n",
      "Training epochs #4850000:   Batch Loss = 5.447450, Accuracy = 0.761999964714\n",
      "Performance on test set: Batch Loss = 5.39702987671, Accuracy = 0.77999997139\n",
      "Training epochs #4852000:   Batch Loss = 5.437310, Accuracy = 0.747000038624\n",
      "Performance on test set: Batch Loss = 5.4699382782, Accuracy = 0.751999974251\n",
      "Training epochs #4854000:   Batch Loss = 5.431507, Accuracy = 0.763000011444\n",
      "Performance on test set: Batch Loss = 5.43315982819, Accuracy = 0.754000067711\n",
      "Training epochs #4856000:   Batch Loss = 5.381962, Accuracy = 0.769999980927\n",
      "Performance on test set: Batch Loss = 5.36418390274, Accuracy = 0.783999979496\n",
      "Training epochs #4858000:   Batch Loss = 5.364579, Accuracy = 0.790000021458\n",
      "Performance on test set: Batch Loss = 5.35667705536, Accuracy = 0.773000001907\n",
      "Training epochs #4860000:   Batch Loss = 5.357462, Accuracy = 0.772000074387\n",
      "Performance on test set: Batch Loss = 5.40925598145, Accuracy = 0.743999958038\n",
      "Training epochs #4862000:   Batch Loss = 5.293021, Accuracy = 0.79699999094\n",
      "Performance on test set: Batch Loss = 5.35898113251, Accuracy = 0.770000040531\n",
      "Training epochs #4864000:   Batch Loss = 5.365203, Accuracy = 0.761999964714\n",
      "Performance on test set: Batch Loss = 5.38691759109, Accuracy = 0.768999993801\n",
      "Training epochs #4866000:   Batch Loss = 5.359453, Accuracy = 0.76800006628\n",
      "Performance on test set: Batch Loss = 5.33511447906, Accuracy = 0.773999989033\n",
      "Training epochs #4868000:   Batch Loss = 5.334684, Accuracy = 0.787000060081\n",
      "Performance on test set: Batch Loss = 5.3770070076, Accuracy = 0.762000024319\n",
      "Training epochs #4870000:   Batch Loss = 5.432554, Accuracy = 0.754999995232\n",
      "Performance on test set: Batch Loss = 5.32776451111, Accuracy = 0.788000047207\n",
      "Training epochs #4872000:   Batch Loss = 5.322324, Accuracy = 0.807000041008\n",
      "Performance on test set: Batch Loss = 5.39170503616, Accuracy = 0.770000100136\n",
      "Training epochs #4874000:   Batch Loss = 5.286555, Accuracy = 0.800000071526\n",
      "Performance on test set: Batch Loss = 5.35199928284, Accuracy = 0.79800003767\n",
      "Training epochs #4876000:   Batch Loss = 5.358342, Accuracy = 0.796000063419\n",
      "Performance on test set: Batch Loss = 5.30000543594, Accuracy = 0.814999997616\n",
      "Training epochs #4878000:   Batch Loss = 5.287311, Accuracy = 0.811000049114\n",
      "Performance on test set: Batch Loss = 5.29350280762, Accuracy = 0.816999971867\n",
      "Training epochs #4880000:   Batch Loss = 5.330779, Accuracy = 0.81400001049\n",
      "Performance on test set: Batch Loss = 5.33571100235, Accuracy = 0.786000013351\n",
      "Training epochs #4882000:   Batch Loss = 5.272739, Accuracy = 0.791000008583\n",
      "Performance on test set: Batch Loss = 5.32125234604, Accuracy = 0.792999982834\n",
      "Training epochs #4884000:   Batch Loss = 5.295455, Accuracy = 0.810000002384\n",
      "Performance on test set: Batch Loss = 5.34980964661, Accuracy = 0.81400001049\n",
      "Training epochs #4886000:   Batch Loss = 5.327280, Accuracy = 0.792000055313\n",
      "Performance on test set: Batch Loss = 5.27753448486, Accuracy = 0.810000061989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #4888000:   Batch Loss = 5.331137, Accuracy = 0.791000008583\n",
      "Performance on test set: Batch Loss = 5.30853366852, Accuracy = 0.79699999094\n",
      "Training epochs #4890000:   Batch Loss = 5.323552, Accuracy = 0.79800003767\n",
      "Performance on test set: Batch Loss = 6.19484138489, Accuracy = 0.180999994278\n",
      "Training epochs #4892000:   Batch Loss = 5.977075, Accuracy = 0.759000003338\n",
      "Performance on test set: Batch Loss = 5.98692417145, Accuracy = 0.745000004768\n",
      "Training epochs #4894000:   Batch Loss = 6.697983, Accuracy = 0.636999964714\n",
      "Performance on test set: Batch Loss = 6.6787776947, Accuracy = 0.638999998569\n",
      "Training epochs #4896000:   Batch Loss = 6.728376, Accuracy = 0.634000003338\n",
      "Performance on test set: Batch Loss = 6.65812730789, Accuracy = 0.657999992371\n",
      "Training epochs #4898000:   Batch Loss = 6.702203, Accuracy = 0.627000033855\n",
      "Performance on test set: Batch Loss = 6.57572364807, Accuracy = 0.657999992371\n",
      "Training epochs #4900000:   Batch Loss = 6.562273, Accuracy = 0.650000035763\n",
      "Performance on test set: Batch Loss = 6.54133844376, Accuracy = 0.629999995232\n",
      "Training epochs #4902000:   Batch Loss = 6.465539, Accuracy = 0.637000024319\n",
      "Performance on test set: Batch Loss = 6.3339805603, Accuracy = 0.663999915123\n",
      "Training epochs #4904000:   Batch Loss = 6.346019, Accuracy = 0.644999980927\n",
      "Performance on test set: Batch Loss = 6.3689289093, Accuracy = 0.638000011444\n",
      "Training epochs #4906000:   Batch Loss = 6.337108, Accuracy = 0.6400000453\n",
      "Performance on test set: Batch Loss = 6.21782207489, Accuracy = 0.668000042439\n",
      "Training epochs #4908000:   Batch Loss = 6.209896, Accuracy = 0.637000024319\n",
      "Performance on test set: Batch Loss = 6.17281532288, Accuracy = 0.637000024319\n",
      "Training epochs #4910000:   Batch Loss = 6.050712, Accuracy = 0.639999985695\n",
      "Performance on test set: Batch Loss = 5.99531364441, Accuracy = 0.661000013351\n",
      "Training epochs #4912000:   Batch Loss = 5.960668, Accuracy = 0.644000053406\n",
      "Performance on test set: Batch Loss = 6.00408554077, Accuracy = 0.622000038624\n",
      "Training epochs #4914000:   Batch Loss = 5.868932, Accuracy = 0.645999968052\n",
      "Performance on test set: Batch Loss = 5.92556142807, Accuracy = 0.639999985695\n",
      "Training epochs #4916000:   Batch Loss = 5.887557, Accuracy = 0.657999992371\n",
      "Performance on test set: Batch Loss = 5.82234764099, Accuracy = 0.784000039101\n",
      "Training epochs #4918000:   Batch Loss = 5.925027, Accuracy = 0.758000016212\n",
      "Performance on test set: Batch Loss = 5.91254758835, Accuracy = 0.76700001955\n",
      "Training epochs #4920000:   Batch Loss = 5.864837, Accuracy = 0.776000022888\n",
      "Performance on test set: Batch Loss = 5.97068786621, Accuracy = 0.741999983788\n",
      "Training epochs #4922000:   Batch Loss = 5.877933, Accuracy = 0.771999955177\n",
      "Performance on test set: Batch Loss = 5.87484550476, Accuracy = 0.758999943733\n",
      "Training epochs #4924000:   Batch Loss = 5.885545, Accuracy = 0.763000011444\n",
      "Performance on test set: Batch Loss = 5.86536598206, Accuracy = 0.641000032425\n",
      "Training epochs #4926000:   Batch Loss = 5.842765, Accuracy = 0.649999976158\n",
      "Performance on test set: Batch Loss = 5.81710290909, Accuracy = 0.677999973297\n",
      "Training epochs #4928000:   Batch Loss = 5.893569, Accuracy = 0.639999985695\n",
      "Performance on test set: Batch Loss = 5.88677930832, Accuracy = 0.637000024319\n",
      "Training epochs #4930000:   Batch Loss = 5.822379, Accuracy = 0.643000006676\n",
      "Performance on test set: Batch Loss = 5.77617692947, Accuracy = 0.665000021458\n",
      "Training epochs #4932000:   Batch Loss = 5.826743, Accuracy = 0.653999984264\n",
      "Performance on test set: Batch Loss = 5.84739398956, Accuracy = 0.62600004673\n",
      "Training epochs #4934000:   Batch Loss = 5.826663, Accuracy = 0.638000071049\n",
      "Performance on test set: Batch Loss = 5.81458330154, Accuracy = 0.643000006676\n",
      "Training epochs #4936000:   Batch Loss = 5.761096, Accuracy = 0.671000003815\n",
      "Performance on test set: Batch Loss = 5.73527097702, Accuracy = 0.669000029564\n",
      "Training epochs #4938000:   Batch Loss = 5.746112, Accuracy = 0.67199999094\n",
      "Performance on test set: Batch Loss = 5.75791931152, Accuracy = 0.657999992371\n",
      "Training epochs #4940000:   Batch Loss = 5.753820, Accuracy = 0.643000006676\n",
      "Performance on test set: Batch Loss = 5.80170345306, Accuracy = 0.636000037193\n",
      "Training epochs #4942000:   Batch Loss = 5.679011, Accuracy = 0.684000015259\n",
      "Performance on test set: Batch Loss = 5.73771047592, Accuracy = 0.662999987602\n",
      "Training epochs #4944000:   Batch Loss = 5.763578, Accuracy = 0.643000006676\n",
      "Performance on test set: Batch Loss = 5.76256752014, Accuracy = 0.641000032425\n",
      "Training epochs #4946000:   Batch Loss = 5.750369, Accuracy = 0.656999945641\n",
      "Performance on test set: Batch Loss = 5.70356750488, Accuracy = 0.677999973297\n",
      "Training epochs #4948000:   Batch Loss = 5.707955, Accuracy = 0.667999982834\n",
      "Performance on test set: Batch Loss = 5.74792289734, Accuracy = 0.75\n",
      "Training epochs #4950000:   Batch Loss = 5.783349, Accuracy = 0.741000115871\n",
      "Performance on test set: Batch Loss = 5.68713283539, Accuracy = 0.774999976158\n",
      "Training epochs #4952000:   Batch Loss = 5.677435, Accuracy = 0.770000040531\n",
      "Performance on test set: Batch Loss = 5.75046396255, Accuracy = 0.744000017643\n",
      "Training epochs #4954000:   Batch Loss = 5.667714, Accuracy = 0.774000048637\n",
      "Performance on test set: Batch Loss = 5.73744630814, Accuracy = 0.746000051498\n",
      "Training epochs #4956000:   Batch Loss = 5.721872, Accuracy = 0.758999943733\n",
      "Performance on test set: Batch Loss = 5.62964248657, Accuracy = 0.786000013351\n",
      "Training epochs #4958000:   Batch Loss = 5.660808, Accuracy = 0.76800006628\n",
      "Performance on test set: Batch Loss = 5.65103578568, Accuracy = 0.768000006676\n",
      "Training epochs #4960000:   Batch Loss = 5.678738, Accuracy = 0.758000016212\n",
      "Performance on test set: Batch Loss = 5.70916032791, Accuracy = 0.743000030518\n",
      "Training epochs #4962000:   Batch Loss = 5.655062, Accuracy = 0.76700001955\n",
      "Performance on test set: Batch Loss = 5.65605258942, Accuracy = 0.763999998569\n",
      "Training epochs #4964000:   Batch Loss = 5.622991, Accuracy = 0.779000043869\n",
      "Performance on test set: Batch Loss = 5.67641782761, Accuracy = 0.759999990463\n",
      "Training epochs #4966000:   Batch Loss = 5.715743, Accuracy = 0.730000019073\n",
      "Performance on test set: Batch Loss = 5.62236356735, Accuracy = 0.768999934196\n",
      "Training epochs #4968000:   Batch Loss = 5.681522, Accuracy = 0.743000030518\n",
      "Performance on test set: Batch Loss = 5.658452034, Accuracy = 0.754000008106\n",
      "Training epochs #4970000:   Batch Loss = 5.642889, Accuracy = 0.764999985695\n",
      "Performance on test set: Batch Loss = 5.59964466095, Accuracy = 0.774999976158\n",
      "Training epochs #4972000:   Batch Loss = 5.621855, Accuracy = 0.759000003338\n",
      "Performance on test set: Batch Loss = 5.65807962418, Accuracy = 0.745000004768\n",
      "Training epochs #4974000:   Batch Loss = 5.651640, Accuracy = 0.744000017643\n",
      "Performance on test set: Batch Loss = 5.64050769806, Accuracy = 0.746000051498\n",
      "Training epochs #4976000:   Batch Loss = 5.641045, Accuracy = 0.740000009537\n",
      "Performance on test set: Batch Loss = 5.53793716431, Accuracy = 0.787000060081\n",
      "Training epochs #4978000:   Batch Loss = 5.637611, Accuracy = 0.738000035286\n",
      "Performance on test set: Batch Loss = 5.55188846588, Accuracy = 0.768999934196\n",
      "Training epochs #4980000:   Batch Loss = 5.572840, Accuracy = 0.763999938965\n",
      "Performance on test set: Batch Loss = 5.60538911819, Accuracy = 0.743000090122\n",
      "Training epochs #4982000:   Batch Loss = 5.599648, Accuracy = 0.746999979019\n",
      "Performance on test set: Batch Loss = 5.56094026566, Accuracy = 0.763000011444\n",
      "Training epochs #4984000:   Batch Loss = 5.541730, Accuracy = 0.758999943733\n",
      "Performance on test set: Batch Loss = 5.57186126709, Accuracy = 0.761000037193\n",
      "Training epochs #4986000:   Batch Loss = 5.600417, Accuracy = 0.745000004768\n",
      "Performance on test set: Batch Loss = 5.51256847382, Accuracy = 0.767999947071\n",
      "Training epochs #4988000:   Batch Loss = 5.567610, Accuracy = 0.740999996662\n",
      "Performance on test set: Batch Loss = 5.54100894928, Accuracy = 0.754000008106\n",
      "Training epochs #4990000:   Batch Loss = 5.525152, Accuracy = 0.759000003338\n",
      "Performance on test set: Batch Loss = 5.48855924606, Accuracy = 0.775000035763\n",
      "Training epochs #4992000:   Batch Loss = 5.504682, Accuracy = 0.755999982357\n",
      "Performance on test set: Batch Loss = 5.53167152405, Accuracy = 0.747000038624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #4994000:   Batch Loss = 5.473815, Accuracy = 0.762000024319\n",
      "Performance on test set: Batch Loss = 5.51299858093, Accuracy = 0.745999932289\n",
      "Training epochs #4996000:   Batch Loss = 5.462560, Accuracy = 0.766999959946\n",
      "Performance on test set: Batch Loss = 5.41286659241, Accuracy = 0.791999995708\n",
      "Training epochs #4998000:   Batch Loss = 5.466036, Accuracy = 0.76599997282\n",
      "Performance on test set: Batch Loss = 5.42184686661, Accuracy = 0.785999953747\n",
      "Training epochs #5000000:   Batch Loss = 5.389805, Accuracy = 0.78100001812\n",
      "Performance on test set: Batch Loss = 5.4555683136, Accuracy = 0.747999966145\n",
      "Training epochs #5002000:   Batch Loss = 5.412070, Accuracy = 0.785000085831\n",
      "Performance on test set: Batch Loss = 5.43050909042, Accuracy = 0.768000006676\n",
      "Training epochs #5004000:   Batch Loss = 5.431885, Accuracy = 0.772999942303\n",
      "Performance on test set: Batch Loss = 5.43510961533, Accuracy = 0.76700001955\n",
      "Training epochs #5006000:   Batch Loss = 5.377305, Accuracy = 0.777999937534\n",
      "Performance on test set: Batch Loss = 5.3675775528, Accuracy = 0.770999968052\n",
      "Training epochs #5008000:   Batch Loss = 5.397943, Accuracy = 0.7650000453\n",
      "Performance on test set: Batch Loss = 5.39651107788, Accuracy = 0.768999993801\n",
      "Training epochs #5010000:   Batch Loss = 5.361285, Accuracy = 0.769999980927\n",
      "Performance on test set: Batch Loss = 5.36054325104, Accuracy = 0.780000030994\n",
      "Training epochs #5012000:   Batch Loss = 5.376165, Accuracy = 0.75\n",
      "Performance on test set: Batch Loss = 5.4009885788, Accuracy = 0.757000029087\n",
      "Training epochs #5014000:   Batch Loss = 5.368536, Accuracy = 0.76599997282\n",
      "Performance on test set: Batch Loss = 5.3795375824, Accuracy = 0.757000029087\n",
      "Training epochs #5016000:   Batch Loss = 5.306009, Accuracy = 0.779000043869\n",
      "Performance on test set: Batch Loss = 5.28596305847, Accuracy = 0.794000029564\n",
      "Training epochs #5018000:   Batch Loss = 5.312398, Accuracy = 0.786000013351\n",
      "Performance on test set: Batch Loss = 5.28770971298, Accuracy = 0.787000000477\n",
      "Training epochs #5020000:   Batch Loss = 5.303728, Accuracy = 0.769000053406\n",
      "Performance on test set: Batch Loss = 5.31678771973, Accuracy = 0.753000020981\n",
      "Training epochs #5022000:   Batch Loss = 5.227665, Accuracy = 0.804000020027\n",
      "Performance on test set: Batch Loss = 5.30259418488, Accuracy = 0.770000040531\n",
      "Training epochs #5024000:   Batch Loss = 5.283292, Accuracy = 0.759999990463\n",
      "Performance on test set: Batch Loss = 5.31273174286, Accuracy = 0.773000001907\n",
      "Training epochs #5026000:   Batch Loss = 5.300161, Accuracy = 0.763000011444\n",
      "Performance on test set: Batch Loss = 5.25154256821, Accuracy = 0.771999955177\n",
      "Training epochs #5028000:   Batch Loss = 5.255161, Accuracy = 0.788000047207\n",
      "Performance on test set: Batch Loss = 5.26873254776, Accuracy = 0.775000035763\n",
      "Training epochs #5030000:   Batch Loss = 5.337450, Accuracy = 0.760999977589\n",
      "Performance on test set: Batch Loss = 5.24459552765, Accuracy = 0.792000055313\n",
      "Training epochs #5032000:   Batch Loss = 5.206890, Accuracy = 0.81200003624\n",
      "Performance on test set: Batch Loss = 5.31144523621, Accuracy = 0.795000076294\n",
      "Training epochs #5034000:   Batch Loss = 5.201351, Accuracy = 0.81400001049\n",
      "Performance on test set: Batch Loss = 5.26281690598, Accuracy = 0.778000056744\n",
      "Training epochs #5036000:   Batch Loss = 5.284453, Accuracy = 0.780000090599\n",
      "Performance on test set: Batch Loss = 5.19938802719, Accuracy = 0.831000089645\n",
      "Training epochs #5038000:   Batch Loss = 5.268476, Accuracy = 0.825000047684\n",
      "Performance on test set: Batch Loss = 5.26455783844, Accuracy = 0.847000002861\n",
      "Training epochs #5040000:   Batch Loss = 5.229366, Accuracy = 0.822000026703\n",
      "Performance on test set: Batch Loss = 5.26485967636, Accuracy = 0.769999921322\n",
      "Training epochs #5042000:   Batch Loss = 5.194368, Accuracy = 0.824999988079\n",
      "Performance on test set: Batch Loss = 5.3044257164, Accuracy = 0.827000021935\n",
      "Training epochs #5044000:   Batch Loss = 5.212809, Accuracy = 0.830999970436\n",
      "Performance on test set: Batch Loss = 5.24506998062, Accuracy = 0.853000044823\n",
      "Training epochs #5046000:   Batch Loss = 5.540028, Accuracy = 0.84399998188\n",
      "Performance on test set: Batch Loss = 5.84742021561, Accuracy = 0.774000048637\n",
      "Training epochs #5048000:   Batch Loss = 5.744660, Accuracy = 0.754000008106\n",
      "Performance on test set: Batch Loss = 5.8072977066, Accuracy = 0.754999995232\n",
      "Training epochs #5050000:   Batch Loss = 6.222429, Accuracy = 0.658999979496\n",
      "Performance on test set: Batch Loss = 6.32341623306, Accuracy = 0.666999995708\n",
      "Training epochs #5052000:   Batch Loss = 6.275049, Accuracy = 0.645999908447\n",
      "Performance on test set: Batch Loss = 6.15235328674, Accuracy = 0.629999995232\n",
      "Training epochs #5054000:   Batch Loss = 5.862840, Accuracy = 0.638999998569\n",
      "Performance on test set: Batch Loss = 5.70386123657, Accuracy = 0.748000025749\n",
      "Training epochs #5056000:   Batch Loss = 5.632940, Accuracy = 0.738000035286\n",
      "Performance on test set: Batch Loss = 5.36503362656, Accuracy = 0.784000039101\n",
      "Training epochs #5058000:   Batch Loss = 5.572757, Accuracy = 0.75\n",
      "Performance on test set: Batch Loss = 5.69277715683, Accuracy = 0.774000048637\n",
      "Training epochs #5060000:   Batch Loss = 5.636100, Accuracy = 0.780999958515\n",
      "Performance on test set: Batch Loss = 5.49090576172, Accuracy = 0.753000080585\n",
      "Training epochs #5062000:   Batch Loss = 5.845628, Accuracy = 0.753999948502\n",
      "Performance on test set: Batch Loss = 5.69693231583, Accuracy = 0.766000032425\n",
      "Training epochs #5064000:   Batch Loss = 5.536513, Accuracy = 0.772000014782\n",
      "Performance on test set: Batch Loss = 5.54749107361, Accuracy = 0.764000058174\n",
      "Training epochs #5066000:   Batch Loss = 5.627728, Accuracy = 0.739999949932\n",
      "Performance on test set: Batch Loss = 5.55102157593, Accuracy = 0.768999993801\n",
      "Training epochs #5068000:   Batch Loss = 5.700336, Accuracy = 0.740999996662\n",
      "Performance on test set: Batch Loss = 5.57791090012, Accuracy = 0.751999914646\n",
      "Training epochs #5070000:   Batch Loss = 5.583776, Accuracy = 0.755000114441\n",
      "Performance on test set: Batch Loss = 5.49759292603, Accuracy = 0.777999997139\n",
      "Training epochs #5072000:   Batch Loss = 5.562921, Accuracy = 0.754000067711\n",
      "Performance on test set: Batch Loss = 5.53351259232, Accuracy = 0.749999940395\n",
      "Training epochs #5074000:   Batch Loss = 5.471560, Accuracy = 0.768000006676\n",
      "Performance on test set: Batch Loss = 5.5079202652, Accuracy = 0.754999995232\n",
      "Training epochs #5076000:   Batch Loss = 5.446001, Accuracy = 0.773999989033\n",
      "Performance on test set: Batch Loss = 5.40478897095, Accuracy = 0.790000021458\n",
      "Training epochs #5078000:   Batch Loss = 5.494207, Accuracy = 0.767999947071\n",
      "Performance on test set: Batch Loss = 5.4351940155, Accuracy = 0.788999974728\n",
      "Training epochs #5080000:   Batch Loss = 5.412242, Accuracy = 0.782999992371\n",
      "Performance on test set: Batch Loss = 5.47199869156, Accuracy = 0.752999961376\n",
      "Training epochs #5082000:   Batch Loss = 5.442644, Accuracy = 0.786000072956\n",
      "Performance on test set: Batch Loss = 5.46601915359, Accuracy = 0.763000011444\n",
      "Training epochs #5084000:   Batch Loss = 5.451399, Accuracy = 0.774000048637\n",
      "Performance on test set: Batch Loss = 5.44876909256, Accuracy = 0.769000053406\n",
      "Training epochs #5086000:   Batch Loss = 5.377114, Accuracy = 0.777000010014\n",
      "Performance on test set: Batch Loss = 5.37857532501, Accuracy = 0.771999955177\n",
      "Training epochs #5088000:   Batch Loss = 5.446591, Accuracy = 0.762000024319\n",
      "Performance on test set: Batch Loss = 5.41071844101, Accuracy = 0.76700001955\n",
      "Training epochs #5090000:   Batch Loss = 5.408422, Accuracy = 0.770000040531\n",
      "Performance on test set: Batch Loss = 5.3697810173, Accuracy = 0.784999966621\n",
      "Training epochs #5092000:   Batch Loss = 5.436434, Accuracy = 0.749000012875\n",
      "Performance on test set: Batch Loss = 5.43666410446, Accuracy = 0.758000016212\n",
      "Training epochs #5094000:   Batch Loss = 5.384233, Accuracy = 0.76599997282\n",
      "Performance on test set: Batch Loss = 5.42677593231, Accuracy = 0.758000016212\n",
      "Training epochs #5096000:   Batch Loss = 5.346371, Accuracy = 0.780000030994\n",
      "Performance on test set: Batch Loss = 5.31624555588, Accuracy = 0.792000055313\n",
      "Training epochs #5098000:   Batch Loss = 5.353094, Accuracy = 0.782999932766\n",
      "Performance on test set: Batch Loss = 5.3239812851, Accuracy = 0.788999974728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #5100000:   Batch Loss = 5.345391, Accuracy = 0.770000040531\n",
      "Performance on test set: Batch Loss = 5.36988353729, Accuracy = 0.750999927521\n",
      "Training epochs #5102000:   Batch Loss = 5.270618, Accuracy = 0.800000011921\n",
      "Performance on test set: Batch Loss = 5.37360239029, Accuracy = 0.764999985695\n",
      "Training epochs #5104000:   Batch Loss = 5.343894, Accuracy = 0.759000003338\n",
      "Performance on test set: Batch Loss = 5.369972229, Accuracy = 0.772000014782\n",
      "Training epochs #5106000:   Batch Loss = 5.365442, Accuracy = 0.760999977589\n",
      "Performance on test set: Batch Loss = 5.29736232758, Accuracy = 0.776000082493\n",
      "Training epochs #5108000:   Batch Loss = 5.303808, Accuracy = 0.780000030994\n",
      "Performance on test set: Batch Loss = 5.3194694519, Accuracy = 0.768999993801\n",
      "Training epochs #5110000:   Batch Loss = 5.405249, Accuracy = 0.754000067711\n",
      "Performance on test set: Batch Loss = 5.28326702118, Accuracy = 0.78200006485\n",
      "Training epochs #5112000:   Batch Loss = 5.281950, Accuracy = 0.781999945641\n",
      "Performance on test set: Batch Loss = 5.33710432053, Accuracy = 0.757000029087\n",
      "Training epochs #5114000:   Batch Loss = 5.242253, Accuracy = 0.780000030994\n",
      "Performance on test set: Batch Loss = 5.30756473541, Accuracy = 0.759000062943\n",
      "Training epochs #5116000:   Batch Loss = 5.300000, Accuracy = 0.768000006676\n",
      "Performance on test set: Batch Loss = 5.22099685669, Accuracy = 0.800000011921\n",
      "Training epochs #5118000:   Batch Loss = 5.227624, Accuracy = 0.782999992371\n",
      "Performance on test set: Batch Loss = 5.22524404526, Accuracy = 0.789000034332\n",
      "Training epochs #5120000:   Batch Loss = 5.283236, Accuracy = 0.76700001955\n",
      "Performance on test set: Batch Loss = 5.25049209595, Accuracy = 0.750999987125\n",
      "Training epochs #5122000:   Batch Loss = 5.201908, Accuracy = 0.787000000477\n",
      "Performance on test set: Batch Loss = 5.26043796539, Accuracy = 0.76700001955\n",
      "Training epochs #5124000:   Batch Loss = 5.193860, Accuracy = 0.787999987602\n",
      "Performance on test set: Batch Loss = 5.26517105103, Accuracy = 0.771000027657\n",
      "Training epochs #5126000:   Batch Loss = 5.250944, Accuracy = 0.742000043392\n",
      "Performance on test set: Batch Loss = 5.18529510498, Accuracy = 0.773000061512\n",
      "Training epochs #5128000:   Batch Loss = 5.231005, Accuracy = 0.752000033855\n",
      "Performance on test set: Batch Loss = 5.20953655243, Accuracy = 0.77999997139\n",
      "Training epochs #5130000:   Batch Loss = 5.248523, Accuracy = 0.797000050545\n",
      "Performance on test set: Batch Loss = 5.17960786819, Accuracy = 0.811000108719\n",
      "Training epochs #5132000:   Batch Loss = 5.170147, Accuracy = 0.811000049114\n",
      "Performance on test set: Batch Loss = 5.22232103348, Accuracy = 0.797000050545\n",
      "Training epochs #5134000:   Batch Loss = 5.194870, Accuracy = 0.799000024796\n",
      "Performance on test set: Batch Loss = 5.19684934616, Accuracy = 0.79599994421\n",
      "Training epochs #5136000:   Batch Loss = 5.212301, Accuracy = 0.795000076294\n",
      "Performance on test set: Batch Loss = 5.13245201111, Accuracy = 0.830000042915\n",
      "Training epochs #5138000:   Batch Loss = 5.216466, Accuracy = 0.77999997139\n",
      "Performance on test set: Batch Loss = 5.13728761673, Accuracy = 0.816000044346\n",
      "Training epochs #5140000:   Batch Loss = 5.186420, Accuracy = 0.807000041008\n",
      "Performance on test set: Batch Loss = 5.14579916, Accuracy = 0.797999978065\n",
      "Training epochs #5142000:   Batch Loss = 5.194697, Accuracy = 0.793999969959\n",
      "Performance on test set: Batch Loss = 5.16015720367, Accuracy = 0.804999947548\n",
      "Training epochs #5144000:   Batch Loss = 5.113076, Accuracy = 0.803000032902\n",
      "Performance on test set: Batch Loss = 5.16808414459, Accuracy = 0.809000015259\n",
      "Training epochs #5146000:   Batch Loss = 5.192688, Accuracy = 0.819000005722\n",
      "Performance on test set: Batch Loss = 5.09447956085, Accuracy = 0.847000002861\n",
      "Training epochs #5148000:   Batch Loss = 5.158133, Accuracy = 0.845999956131\n",
      "Performance on test set: Batch Loss = 5.13503074646, Accuracy = 0.848000049591\n",
      "Training epochs #5150000:   Batch Loss = 5.149880, Accuracy = 0.849999964237\n",
      "Performance on test set: Batch Loss = 5.10510063171, Accuracy = 0.852000057697\n",
      "Training epochs #5152000:   Batch Loss = 5.147924, Accuracy = 0.817000031471\n",
      "Performance on test set: Batch Loss = 5.16383600235, Accuracy = 0.796000003815\n",
      "Training epochs #5154000:   Batch Loss = 5.127331, Accuracy = 0.824999988079\n",
      "Performance on test set: Batch Loss = 5.14117050171, Accuracy = 0.800000011921\n",
      "Training epochs #5156000:   Batch Loss = 5.113426, Accuracy = 0.830000042915\n",
      "Performance on test set: Batch Loss = 5.08134365082, Accuracy = 0.856999993324\n",
      "Training epochs #5158000:   Batch Loss = 5.107651, Accuracy = 0.845999956131\n",
      "Performance on test set: Batch Loss = 5.07464265823, Accuracy = 0.865000009537\n",
      "Training epochs #5160000:   Batch Loss = 5.049137, Accuracy = 0.85799998045\n",
      "Performance on test set: Batch Loss = 5.0973815918, Accuracy = 0.84500002861\n",
      "Training epochs #5162000:   Batch Loss = 5.101769, Accuracy = 0.851000010967\n",
      "Performance on test set: Batch Loss = 5.102871418, Accuracy = 0.855000019073\n",
      "Training epochs #5164000:   Batch Loss = 5.133996, Accuracy = 0.834000110626\n",
      "Performance on test set: Batch Loss = 5.43671178818, Accuracy = 0.852000117302\n",
      "Training epochs #5166000:   Batch Loss = 6.348028, Accuracy = 0.196999996901\n",
      "Performance on test set: Batch Loss = 6.09913730621, Accuracy = 0.774999976158\n",
      "Training epochs #5168000:   Batch Loss = 6.004219, Accuracy = 0.741999983788\n",
      "Performance on test set: Batch Loss = 5.89381885529, Accuracy = 0.74899995327\n",
      "Training epochs #5170000:   Batch Loss = 5.864161, Accuracy = 0.759999990463\n",
      "Performance on test set: Batch Loss = 5.82034063339, Accuracy = 0.774000048637\n",
      "Training epochs #5172000:   Batch Loss = 6.006782, Accuracy = 0.740999996662\n",
      "Performance on test set: Batch Loss = 6.08115434647, Accuracy = 0.740999937057\n",
      "Training epochs #5174000:   Batch Loss = 5.986705, Accuracy = 0.752999961376\n",
      "Performance on test set: Batch Loss = 6.05685710907, Accuracy = 0.743000030518\n",
      "Training epochs #5176000:   Batch Loss = 5.859695, Accuracy = 0.762999951839\n",
      "Performance on test set: Batch Loss = 5.73550701141, Accuracy = 0.78100001812\n",
      "Training epochs #5178000:   Batch Loss = 5.655085, Accuracy = 0.773999989033\n",
      "Performance on test set: Batch Loss = 5.67299079895, Accuracy = 0.76700001955\n",
      "Training epochs #5180000:   Batch Loss = 5.689608, Accuracy = 0.760000050068\n",
      "Performance on test set: Batch Loss = 5.76083755493, Accuracy = 0.737000048161\n",
      "Training epochs #5182000:   Batch Loss = 5.636030, Accuracy = 0.789000034332\n",
      "Performance on test set: Batch Loss = 5.71363544464, Accuracy = 0.755999982357\n",
      "Training epochs #5184000:   Batch Loss = 5.684288, Accuracy = 0.745999991894\n",
      "Performance on test set: Batch Loss = 5.63768672943, Accuracy = 0.759999990463\n",
      "Training epochs #5186000:   Batch Loss = 5.634145, Accuracy = 0.751999974251\n",
      "Performance on test set: Batch Loss = 5.57709407806, Accuracy = 0.76700001955\n",
      "Training epochs #5188000:   Batch Loss = 5.600389, Accuracy = 0.764000058174\n",
      "Performance on test set: Batch Loss = 5.64621114731, Accuracy = 0.746999979019\n",
      "Training epochs #5190000:   Batch Loss = 5.716434, Accuracy = 0.736000001431\n",
      "Performance on test set: Batch Loss = 5.54088973999, Accuracy = 0.774000048637\n",
      "Training epochs #5192000:   Batch Loss = 5.594257, Accuracy = 0.761000037193\n",
      "Performance on test set: Batch Loss = 5.65445375443, Accuracy = 0.741999983788\n",
      "Training epochs #5194000:   Batch Loss = 5.554678, Accuracy = 0.768000006676\n",
      "Performance on test set: Batch Loss = 5.63599824905, Accuracy = 0.743000030518\n",
      "Training epochs #5196000:   Batch Loss = 5.615681, Accuracy = 0.753000020981\n",
      "Performance on test set: Batch Loss = 5.51680183411, Accuracy = 0.78100001812\n",
      "Training epochs #5198000:   Batch Loss = 5.564312, Accuracy = 0.764999985695\n",
      "Performance on test set: Batch Loss = 5.54942178726, Accuracy = 0.766999959946\n",
      "Training epochs #5200000:   Batch Loss = 5.610693, Accuracy = 0.752000033855\n",
      "Performance on test set: Batch Loss = 5.60111808777, Accuracy = 0.737999975681\n",
      "Training epochs #5202000:   Batch Loss = 5.566052, Accuracy = 0.763000011444\n",
      "Performance on test set: Batch Loss = 5.57304286957, Accuracy = 0.757000029087\n",
      "Training epochs #5204000:   Batch Loss = 5.509706, Accuracy = 0.774999976158\n",
      "Performance on test set: Batch Loss = 5.56489181519, Accuracy = 0.759000003338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #5206000:   Batch Loss = 5.626278, Accuracy = 0.726999998093\n",
      "Performance on test set: Batch Loss = 5.5163602829, Accuracy = 0.766999959946\n",
      "Training epochs #5208000:   Batch Loss = 5.589202, Accuracy = 0.740999996662\n",
      "Performance on test set: Batch Loss = 5.58110618591, Accuracy = 0.746999979019\n",
      "Training epochs #5210000:   Batch Loss = 5.570580, Accuracy = 0.758000016212\n",
      "Performance on test set: Batch Loss = 5.49250125885, Accuracy = 0.773999989033\n",
      "Training epochs #5212000:   Batch Loss = 5.555405, Accuracy = 0.754999935627\n",
      "Performance on test set: Batch Loss = 5.59640741348, Accuracy = 0.741999983788\n",
      "Training epochs #5214000:   Batch Loss = 5.577014, Accuracy = 0.742999970913\n",
      "Performance on test set: Batch Loss = 5.58381795883, Accuracy = 0.743000030518\n",
      "Training epochs #5216000:   Batch Loss = 5.589999, Accuracy = 0.736999988556\n",
      "Performance on test set: Batch Loss = 5.47726345062, Accuracy = 0.78100001812\n",
      "Training epochs #5218000:   Batch Loss = 5.602327, Accuracy = 0.733000040054\n",
      "Performance on test set: Batch Loss = 5.49631881714, Accuracy = 0.766999959946\n",
      "Training epochs #5220000:   Batch Loss = 5.527308, Accuracy = 0.761000037193\n",
      "Performance on test set: Batch Loss = 5.54947376251, Accuracy = 0.740000069141\n",
      "Training epochs #5222000:   Batch Loss = 5.560030, Accuracy = 0.743000030518\n",
      "Performance on test set: Batch Loss = 5.5160651207, Accuracy = 0.756999969482\n",
      "Training epochs #5224000:   Batch Loss = 5.511146, Accuracy = 0.753000020981\n",
      "Performance on test set: Batch Loss = 5.51703739166, Accuracy = 0.759999990463\n",
      "Training epochs #5226000:   Batch Loss = 5.587977, Accuracy = 0.737999975681\n",
      "Performance on test set: Batch Loss = 5.47670841217, Accuracy = 0.76700001955\n",
      "Training epochs #5228000:   Batch Loss = 5.568050, Accuracy = 0.737000048161\n",
      "Performance on test set: Batch Loss = 5.53866529465, Accuracy = 0.749000012875\n",
      "Training epochs #5230000:   Batch Loss = 5.523282, Accuracy = 0.754000008106\n",
      "Performance on test set: Batch Loss = 5.45571708679, Accuracy = 0.774999976158\n",
      "Training epochs #5232000:   Batch Loss = 5.524673, Accuracy = 0.751999974251\n",
      "Performance on test set: Batch Loss = 5.55675506592, Accuracy = 0.741999983788\n",
      "Training epochs #5234000:   Batch Loss = 5.504145, Accuracy = 0.754000008106\n",
      "Performance on test set: Batch Loss = 5.54723596573, Accuracy = 0.743000030518\n",
      "Training epochs #5236000:   Batch Loss = 5.466581, Accuracy = 0.76599997282\n",
      "Performance on test set: Batch Loss = 5.43345165253, Accuracy = 0.780999958515\n",
      "Training epochs #5238000:   Batch Loss = 5.500534, Accuracy = 0.756999969482\n",
      "Performance on test set: Batch Loss = 5.45587158203, Accuracy = 0.766999959946\n",
      "Training epochs #5240000:   Batch Loss = 5.412951, Accuracy = 0.776999950409\n",
      "Performance on test set: Batch Loss = 5.50944662094, Accuracy = 0.740000069141\n",
      "Training epochs #5242000:   Batch Loss = 5.466287, Accuracy = 0.768999993801\n",
      "Performance on test set: Batch Loss = 5.47899103165, Accuracy = 0.757000029087\n",
      "Training epochs #5244000:   Batch Loss = 5.480785, Accuracy = 0.762000024319\n",
      "Performance on test set: Batch Loss = 5.48239040375, Accuracy = 0.759999990463\n",
      "Training epochs #5246000:   Batch Loss = 5.443007, Accuracy = 0.76700001955\n",
      "Performance on test set: Batch Loss = 5.43590641022, Accuracy = 0.76700001955\n",
      "Training epochs #5248000:   Batch Loss = 5.508308, Accuracy = 0.741000056267\n",
      "Performance on test set: Batch Loss = 5.48859786987, Accuracy = 0.74899995327\n",
      "Training epochs #5250000:   Batch Loss = 5.457877, Accuracy = 0.759999990463\n",
      "Performance on test set: Batch Loss = 5.41195297241, Accuracy = 0.774999976158\n",
      "Training epochs #5252000:   Batch Loss = 5.490503, Accuracy = 0.740000009537\n",
      "Performance on test set: Batch Loss = 5.49935293198, Accuracy = 0.744000017643\n",
      "Training epochs #5254000:   Batch Loss = 5.459831, Accuracy = 0.754000008106\n",
      "Performance on test set: Batch Loss = 5.4833574295, Accuracy = 0.742999970913\n",
      "Training epochs #5256000:   Batch Loss = 5.409883, Accuracy = 0.762999951839\n",
      "Performance on test set: Batch Loss = 5.37002420425, Accuracy = 0.78100001812\n",
      "Training epochs #5258000:   Batch Loss = 5.405030, Accuracy = 0.774999976158\n",
      "Performance on test set: Batch Loss = 5.38726854324, Accuracy = 0.768000006676\n",
      "Training epochs #5260000:   Batch Loss = 5.394646, Accuracy = 0.761000037193\n",
      "Performance on test set: Batch Loss = 5.43343067169, Accuracy = 0.740000009537\n",
      "Training epochs #5262000:   Batch Loss = 5.322232, Accuracy = 0.787999987602\n",
      "Performance on test set: Batch Loss = 5.40598726273, Accuracy = 0.759000003338\n",
      "Training epochs #5264000:   Batch Loss = 5.415323, Accuracy = 0.746999979019\n",
      "Performance on test set: Batch Loss = 5.40898180008, Accuracy = 0.759999990463\n",
      "Training epochs #5266000:   Batch Loss = 5.389925, Accuracy = 0.755000054836\n",
      "Performance on test set: Batch Loss = 5.35295009613, Accuracy = 0.769000053406\n",
      "Training epochs #5268000:   Batch Loss = 5.368141, Accuracy = 0.764999985695\n",
      "Performance on test set: Batch Loss = 5.38588428497, Accuracy = 0.75\n",
      "Training epochs #5270000:   Batch Loss = 5.438926, Accuracy = 0.735999941826\n",
      "Performance on test set: Batch Loss = 5.31875896454, Accuracy = 0.775999963284\n",
      "Training epochs #5272000:   Batch Loss = 5.332310, Accuracy = 0.763000011444\n",
      "Performance on test set: Batch Loss = 5.4018034935, Accuracy = 0.741999983788\n",
      "Training epochs #5274000:   Batch Loss = 5.319152, Accuracy = 0.768999993801\n",
      "Performance on test set: Batch Loss = 5.39766693115, Accuracy = 0.744000077248\n",
      "Training epochs #5276000:   Batch Loss = 5.372912, Accuracy = 0.754000067711\n",
      "Performance on test set: Batch Loss = 5.29275035858, Accuracy = 0.781000077724\n",
      "Training epochs #5278000:   Batch Loss = 5.321221, Accuracy = 0.766000032425\n",
      "Performance on test set: Batch Loss = 5.30811405182, Accuracy = 0.766999959946\n",
      "Training epochs #5280000:   Batch Loss = 5.383171, Accuracy = 0.751999974251\n",
      "Performance on test set: Batch Loss = 5.36071825027, Accuracy = 0.7650000453\n",
      "Training epochs #5282000:   Batch Loss = 5.274547, Accuracy = 0.799000024796\n",
      "Performance on test set: Batch Loss = 5.93707799911, Accuracy = 0.809000015259\n",
      "Training epochs #5284000:   Batch Loss = 5.433174, Accuracy = 0.776000022888\n",
      "Performance on test set: Batch Loss = 5.59370136261, Accuracy = 0.761000037193\n",
      "Training epochs #5286000:   Batch Loss = 5.792418, Accuracy = 0.726000010967\n",
      "Performance on test set: Batch Loss = 5.70665216446, Accuracy = 0.772999942303\n",
      "Training epochs #5288000:   Batch Loss = 5.778918, Accuracy = 0.739999949932\n",
      "Performance on test set: Batch Loss = 5.75758171082, Accuracy = 0.748000025749\n",
      "Training epochs #5290000:   Batch Loss = 5.751587, Accuracy = 0.755999982357\n",
      "Performance on test set: Batch Loss = 5.65841388702, Accuracy = 0.775000035763\n",
      "Training epochs #5292000:   Batch Loss = 5.778117, Accuracy = 0.755999922752\n",
      "Performance on test set: Batch Loss = 5.73770093918, Accuracy = 0.744000017643\n",
      "Training epochs #5294000:   Batch Loss = 5.711614, Accuracy = 0.744000017643\n",
      "Performance on test set: Batch Loss = 5.75150299072, Accuracy = 0.743000030518\n",
      "Training epochs #5296000:   Batch Loss = 5.723774, Accuracy = 0.736000001431\n",
      "Performance on test set: Batch Loss = 5.5406036377, Accuracy = 0.781999945641\n",
      "Training epochs #5298000:   Batch Loss = 5.683216, Accuracy = 0.733000040054\n",
      "Performance on test set: Batch Loss = 5.56316566467, Accuracy = 0.769000053406\n",
      "Training epochs #5300000:   Batch Loss = 5.619120, Accuracy = 0.761000037193\n",
      "Performance on test set: Batch Loss = 5.62400865555, Accuracy = 0.737000048161\n",
      "Training epochs #5302000:   Batch Loss = 5.634628, Accuracy = 0.741999983788\n",
      "Performance on test set: Batch Loss = 5.5748000145, Accuracy = 0.756999969482\n",
      "Training epochs #5304000:   Batch Loss = 5.524102, Accuracy = 0.752999961376\n",
      "Performance on test set: Batch Loss = 5.53778743744, Accuracy = 0.760000050068\n",
      "Training epochs #5306000:   Batch Loss = 5.608724, Accuracy = 0.738999962807\n",
      "Performance on test set: Batch Loss = 5.48799085617, Accuracy = 0.766999959946\n",
      "Training epochs #5308000:   Batch Loss = 5.586516, Accuracy = 0.736000061035\n",
      "Performance on test set: Batch Loss = 5.53107881546, Accuracy = 0.748000025749\n",
      "Training epochs #5310000:   Batch Loss = 5.559643, Accuracy = 0.754000008106\n",
      "Performance on test set: Batch Loss = 5.47139167786, Accuracy = 0.773999929428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #5312000:   Batch Loss = 5.533848, Accuracy = 0.751999974251\n",
      "Performance on test set: Batch Loss = 5.55362081528, Accuracy = 0.744000017643\n",
      "Training epochs #5314000:   Batch Loss = 5.511613, Accuracy = 0.754999995232\n",
      "Performance on test set: Batch Loss = 5.56036663055, Accuracy = 0.742999970913\n",
      "Training epochs #5316000:   Batch Loss = 5.445181, Accuracy = 0.766000032425\n",
      "Performance on test set: Batch Loss = 5.42051362991, Accuracy = 0.78100001812\n",
      "Training epochs #5318000:   Batch Loss = 5.467199, Accuracy = 0.757000029087\n",
      "Performance on test set: Batch Loss = 5.42883634567, Accuracy = 0.768000006676\n",
      "Training epochs #5320000:   Batch Loss = 5.391723, Accuracy = 0.776000022888\n",
      "Performance on test set: Batch Loss = 5.48357820511, Accuracy = 0.737999975681\n",
      "Training epochs #5322000:   Batch Loss = 5.434459, Accuracy = 0.771000027657\n",
      "Performance on test set: Batch Loss = 5.43186187744, Accuracy = 0.756999969482\n",
      "Training epochs #5324000:   Batch Loss = 5.462274, Accuracy = 0.762000024319\n",
      "Performance on test set: Batch Loss = 5.41227865219, Accuracy = 0.758999943733\n",
      "Training epochs #5326000:   Batch Loss = 5.383954, Accuracy = 0.767000079155\n",
      "Performance on test set: Batch Loss = 5.36769008636, Accuracy = 0.768000006676\n",
      "Training epochs #5328000:   Batch Loss = 5.465217, Accuracy = 0.741000056267\n",
      "Performance on test set: Batch Loss = 5.4259262085, Accuracy = 0.748000025749\n",
      "Training epochs #5330000:   Batch Loss = 5.406024, Accuracy = 0.759999990463\n",
      "Performance on test set: Batch Loss = 5.34655237198, Accuracy = 0.774000048637\n",
      "Training epochs #5332000:   Batch Loss = 5.400278, Accuracy = 0.740999996662\n",
      "Performance on test set: Batch Loss = 5.44584751129, Accuracy = 0.743000030518\n",
      "Training epochs #5334000:   Batch Loss = 5.402680, Accuracy = 0.753999948502\n",
      "Performance on test set: Batch Loss = 5.4413228035, Accuracy = 0.743000030518\n",
      "Training epochs #5336000:   Batch Loss = 5.349449, Accuracy = 0.762999951839\n",
      "Performance on test set: Batch Loss = 5.32820129395, Accuracy = 0.78100001812\n",
      "Training epochs #5338000:   Batch Loss = 5.311452, Accuracy = 0.773999989033\n",
      "Performance on test set: Batch Loss = 5.31382608414, Accuracy = 0.767999947071\n",
      "Training epochs #5340000:   Batch Loss = 5.349231, Accuracy = 0.759999990463\n",
      "Performance on test set: Batch Loss = 5.38134670258, Accuracy = 0.737999975681\n",
      "Training epochs #5342000:   Batch Loss = 5.280735, Accuracy = 0.788000047207\n",
      "Performance on test set: Batch Loss = 5.33915424347, Accuracy = 0.758000016212\n",
      "Training epochs #5344000:   Batch Loss = 5.329897, Accuracy = 0.746999979019\n",
      "Performance on test set: Batch Loss = 5.33270931244, Accuracy = 0.759000003338\n",
      "Training epochs #5346000:   Batch Loss = 5.339353, Accuracy = 0.753000020981\n",
      "Performance on test set: Batch Loss = 5.29502010345, Accuracy = 0.775000035763\n",
      "Training epochs #5348000:   Batch Loss = 5.323130, Accuracy = 0.783999979496\n",
      "Performance on test set: Batch Loss = 5.34111070633, Accuracy = 0.782000005245\n",
      "Training epochs #5350000:   Batch Loss = 5.423148, Accuracy = 0.767000079155\n",
      "Performance on test set: Batch Loss = 5.27539253235, Accuracy = 0.804000020027\n",
      "Training epochs #5352000:   Batch Loss = 5.296857, Accuracy = 0.796000063419\n",
      "Performance on test set: Batch Loss = 5.37592887878, Accuracy = 0.763999998569\n",
      "Training epochs #5354000:   Batch Loss = 5.292114, Accuracy = 0.794999957085\n",
      "Performance on test set: Batch Loss = 5.36831855774, Accuracy = 0.771000027657\n",
      "Training epochs #5356000:   Batch Loss = 5.358537, Accuracy = 0.774000048637\n",
      "Performance on test set: Batch Loss = 5.26620674133, Accuracy = 0.816000044346\n",
      "Training epochs #5358000:   Batch Loss = 5.287242, Accuracy = 0.81400001049\n",
      "Performance on test set: Batch Loss = 5.24397563934, Accuracy = 0.821000039577\n",
      "Training epochs #5360000:   Batch Loss = 5.294580, Accuracy = 0.801999986172\n",
      "Performance on test set: Batch Loss = 5.31940031052, Accuracy = 0.782999992371\n",
      "Training epochs #5362000:   Batch Loss = 5.285779, Accuracy = 0.811000049114\n",
      "Performance on test set: Batch Loss = 5.27573776245, Accuracy = 0.817000031471\n",
      "Training epochs #5364000:   Batch Loss = 5.231658, Accuracy = 0.831999897957\n",
      "Performance on test set: Batch Loss = 5.27240610123, Accuracy = 0.811999976635\n",
      "Training epochs #5366000:   Batch Loss = 5.302449, Accuracy = 0.800000071526\n",
      "Performance on test set: Batch Loss = 5.24302101135, Accuracy = 0.82900005579\n",
      "Training epochs #5368000:   Batch Loss = 5.296740, Accuracy = 0.815999984741\n",
      "Performance on test set: Batch Loss = 5.29316711426, Accuracy = 0.820999979973\n",
      "Training epochs #5370000:   Batch Loss = 5.324288, Accuracy = 0.825000047684\n",
      "Performance on test set: Batch Loss = 5.22659873962, Accuracy = 0.846000015736\n",
      "Training epochs #5372000:   Batch Loss = 5.311175, Accuracy = 0.817999958992\n",
      "Performance on test set: Batch Loss = 5.33198976517, Accuracy = 0.804000020027\n",
      "Training epochs #5374000:   Batch Loss = 5.273918, Accuracy = 0.837000012398\n",
      "Performance on test set: Batch Loss = 5.32252073288, Accuracy = 0.81200003624\n",
      "Training epochs #5376000:   Batch Loss = 5.327072, Accuracy = 0.81200003624\n",
      "Performance on test set: Batch Loss = 5.23122787476, Accuracy = 0.831000030041\n",
      "Training epochs #5378000:   Batch Loss = 5.311705, Accuracy = 0.820999979973\n",
      "Performance on test set: Batch Loss = 5.19879007339, Accuracy = 0.856999993324\n",
      "Training epochs #5380000:   Batch Loss = 5.259120, Accuracy = 0.837000012398\n",
      "Performance on test set: Batch Loss = 5.27640247345, Accuracy = 0.822000026703\n",
      "Training epochs #5382000:   Batch Loss = 5.298759, Accuracy = 0.817999958992\n",
      "Performance on test set: Batch Loss = 5.23534345627, Accuracy = 0.841000080109\n",
      "Training epochs #5384000:   Batch Loss = 5.216068, Accuracy = 0.842000007629\n",
      "Performance on test set: Batch Loss = 5.23509025574, Accuracy = 0.837000012398\n",
      "Training epochs #5386000:   Batch Loss = 5.318006, Accuracy = 0.811000049114\n",
      "Performance on test set: Batch Loss = 5.20989990234, Accuracy = 0.841000080109\n",
      "Training epochs #5388000:   Batch Loss = 5.262483, Accuracy = 0.824000060558\n",
      "Performance on test set: Batch Loss = 5.25722551346, Accuracy = 0.822999954224\n",
      "Training epochs #5390000:   Batch Loss = 5.274406, Accuracy = 0.825999975204\n",
      "Performance on test set: Batch Loss = 5.19208049774, Accuracy = 0.848999977112\n",
      "Training epochs #5392000:   Batch Loss = 5.268770, Accuracy = 0.824000000954\n",
      "Performance on test set: Batch Loss = 5.30165195465, Accuracy = 0.805000007153\n",
      "Training epochs #5394000:   Batch Loss = 5.268444, Accuracy = 0.814000070095\n",
      "Performance on test set: Batch Loss = 5.2932934761, Accuracy = 0.811999976635\n",
      "Training epochs #5396000:   Batch Loss = 5.223250, Accuracy = 0.833999991417\n",
      "Performance on test set: Batch Loss = 5.2062702179, Accuracy = 0.831000030041\n",
      "Training epochs #5398000:   Batch Loss = 5.225397, Accuracy = 0.835000038147\n",
      "Performance on test set: Batch Loss = 5.16921615601, Accuracy = 0.858000040054\n",
      "Training epochs #5400000:   Batch Loss = 5.148667, Accuracy = 0.853999972343\n",
      "Performance on test set: Batch Loss = 5.24817752838, Accuracy = 0.822000026703\n",
      "Training epochs #5402000:   Batch Loss = 5.238307, Accuracy = 0.832000017166\n",
      "Performance on test set: Batch Loss = 5.20900583267, Accuracy = 0.841000020504\n",
      "Training epochs #5404000:   Batch Loss = 5.267710, Accuracy = 0.830000042915\n",
      "Performance on test set: Batch Loss = 5.20741701126, Accuracy = 0.837000012398\n",
      "Training epochs #5406000:   Batch Loss = 5.206940, Accuracy = 0.837000012398\n",
      "Performance on test set: Batch Loss = 5.18370103836, Accuracy = 0.841000020504\n",
      "Training epochs #5408000:   Batch Loss = 5.289994, Accuracy = 0.822000026703\n",
      "Performance on test set: Batch Loss = 5.23402404785, Accuracy = 0.823000013828\n",
      "Training epochs #5410000:   Batch Loss = 5.236754, Accuracy = 0.827000081539\n",
      "Performance on test set: Batch Loss = 5.17256546021, Accuracy = 0.847000002861\n",
      "Training epochs #5412000:   Batch Loss = 5.213576, Accuracy = 0.826000034809\n",
      "Performance on test set: Batch Loss = 5.27737760544, Accuracy = 0.804999947548\n",
      "Training epochs #5414000:   Batch Loss = 5.223636, Accuracy = 0.830000042915\n",
      "Performance on test set: Batch Loss = 5.27072429657, Accuracy = 0.820000052452\n",
      "Training epochs #5416000:   Batch Loss = 5.197854, Accuracy = 0.837000072002\n",
      "Performance on test set: Batch Loss = 5.18822288513, Accuracy = 0.830999970436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #5418000:   Batch Loss = 5.162836, Accuracy = 0.845999956131\n",
      "Performance on test set: Batch Loss = 5.15030288696, Accuracy = 0.85799998045\n",
      "Training epochs #5420000:   Batch Loss = 5.205009, Accuracy = 0.831000030041\n",
      "Performance on test set: Batch Loss = 5.22669458389, Accuracy = 0.822000086308\n",
      "Training epochs #5422000:   Batch Loss = 5.138040, Accuracy = 0.855000019073\n",
      "Performance on test set: Batch Loss = 5.1852312088, Accuracy = 0.842000007629\n",
      "Training epochs #5424000:   Batch Loss = 5.187533, Accuracy = 0.823000073433\n",
      "Performance on test set: Batch Loss = 5.19027662277, Accuracy = 0.837000072002\n",
      "Training epochs #5426000:   Batch Loss = 5.204751, Accuracy = 0.835000038147\n",
      "Performance on test set: Batch Loss = 5.16432952881, Accuracy = 0.841000020504\n",
      "Training epochs #5428000:   Batch Loss = 5.194727, Accuracy = 0.841000020504\n",
      "Performance on test set: Batch Loss = 5.21654319763, Accuracy = 0.820999979973\n",
      "Training epochs #5430000:   Batch Loss = 5.302568, Accuracy = 0.807999968529\n",
      "Performance on test set: Batch Loss = 5.1488237381, Accuracy = 0.849000036716\n",
      "Training epochs #5432000:   Batch Loss = 5.184276, Accuracy = 0.839999973774\n",
      "Performance on test set: Batch Loss = 5.2751660347, Accuracy = 0.805999994278\n",
      "Training epochs #5434000:   Batch Loss = 5.173835, Accuracy = 0.839999973774\n",
      "Performance on test set: Batch Loss = 5.27102804184, Accuracy = 0.805999994278\n",
      "Training epochs #5436000:   Batch Loss = 5.250047, Accuracy = 0.81300008297\n",
      "Performance on test set: Batch Loss = 5.17283010483, Accuracy = 0.831000030041\n",
      "Training epochs #5438000:   Batch Loss = 5.192441, Accuracy = 0.835000097752\n",
      "Performance on test set: Batch Loss = 5.1312046051, Accuracy = 0.85799998045\n",
      "Training epochs #5440000:   Batch Loss = 5.197678, Accuracy = 0.824000060558\n",
      "Performance on test set: Batch Loss = 5.20600032806, Accuracy = 0.821999967098\n",
      "Training epochs #5442000:   Batch Loss = 5.188158, Accuracy = 0.837000012398\n",
      "Performance on test set: Batch Loss = 5.18383216858, Accuracy = 0.844000041485\n",
      "Training epochs #5444000:   Batch Loss = 5.126892, Accuracy = 0.850000023842\n",
      "Performance on test set: Batch Loss = 5.26075124741, Accuracy = 0.796000003815\n",
      "Training epochs #5446000:   Batch Loss = 5.192821, Accuracy = 0.826000034809\n",
      "Performance on test set: Batch Loss = 5.22222423553, Accuracy = 0.844000041485\n",
      "Training epochs #5448000:   Batch Loss = 5.392357, Accuracy = 0.775000035763\n",
      "Performance on test set: Batch Loss = 5.47528266907, Accuracy = 0.749999940395\n",
      "Training epochs #5450000:   Batch Loss = 5.553304, Accuracy = 0.757999956608\n",
      "Performance on test set: Batch Loss = 5.4754281044, Accuracy = 0.773999929428\n",
      "Training epochs #5452000:   Batch Loss = 5.494640, Accuracy = 0.757000029087\n",
      "Performance on test set: Batch Loss = 5.56579971313, Accuracy = 0.744000017643\n",
      "Training epochs #5454000:   Batch Loss = 5.549072, Accuracy = 0.743000030518\n",
      "Performance on test set: Batch Loss = 5.54227638245, Accuracy = 0.743000030518\n",
      "Training epochs #5456000:   Batch Loss = 5.576462, Accuracy = 0.736000001431\n",
      "Performance on test set: Batch Loss = 5.51399517059, Accuracy = 0.780000030994\n",
      "Training epochs #5458000:   Batch Loss = 5.604895, Accuracy = 0.73400002718\n",
      "Performance on test set: Batch Loss = 5.41025209427, Accuracy = 0.768000006676\n",
      "Training epochs #5460000:   Batch Loss = 5.453973, Accuracy = 0.761000037193\n",
      "Performance on test set: Batch Loss = 5.40692043304, Accuracy = 0.766000032425\n",
      "Training epochs #5462000:   Batch Loss = 5.409913, Accuracy = 0.75\n",
      "Performance on test set: Batch Loss = 5.34680080414, Accuracy = 0.757000029087\n",
      "Training epochs #5464000:   Batch Loss = 5.325777, Accuracy = 0.752999961376\n",
      "Performance on test set: Batch Loss = 5.37514829636, Accuracy = 0.762000083923\n",
      "Training epochs #5466000:   Batch Loss = 5.417964, Accuracy = 0.740000009537\n",
      "Performance on test set: Batch Loss = 5.30528402328, Accuracy = 0.7650000453\n",
      "Training epochs #5468000:   Batch Loss = 5.393991, Accuracy = 0.738999962807\n",
      "Performance on test set: Batch Loss = 5.36965751648, Accuracy = 0.750999987125\n",
      "Training epochs #5470000:   Batch Loss = 5.370678, Accuracy = 0.764999985695\n",
      "Performance on test set: Batch Loss = 5.28763055801, Accuracy = 0.788999974728\n",
      "Training epochs #5472000:   Batch Loss = 5.326087, Accuracy = 0.789999961853\n",
      "Performance on test set: Batch Loss = 5.39353752136, Accuracy = 0.775000035763\n",
      "Training epochs #5474000:   Batch Loss = 5.321936, Accuracy = 0.799000024796\n",
      "Performance on test set: Batch Loss = 5.57815074921, Accuracy = 0.753999948502\n",
      "Training epochs #5476000:   Batch Loss = 5.424363, Accuracy = 0.7650000453\n",
      "Performance on test set: Batch Loss = 5.38242435455, Accuracy = 0.789000034332\n",
      "Training epochs #5478000:   Batch Loss = 5.328329, Accuracy = 0.791999995708\n",
      "Performance on test set: Batch Loss = 5.42487621307, Accuracy = 0.695999979973\n",
      "Training epochs #5480000:   Batch Loss = 5.375967, Accuracy = 0.707000017166\n",
      "Performance on test set: Batch Loss = 5.48933506012, Accuracy = 0.659999966621\n",
      "Training epochs #5482000:   Batch Loss = 5.402104, Accuracy = 0.686999976635\n",
      "Performance on test set: Batch Loss = 5.31508016586, Accuracy = 0.821999967098\n",
      "Training epochs #5484000:   Batch Loss = 5.361972, Accuracy = 0.784999966621\n",
      "Performance on test set: Batch Loss = 5.30190658569, Accuracy = 0.787999987602\n",
      "Training epochs #5486000:   Batch Loss = 5.299296, Accuracy = 0.773000001907\n",
      "Performance on test set: Batch Loss = 5.27571487427, Accuracy = 0.773000001907\n",
      "Training epochs #5488000:   Batch Loss = 5.347668, Accuracy = 0.762000024319\n",
      "Performance on test set: Batch Loss = 5.30665445328, Accuracy = 0.78100001812\n",
      "Training epochs #5490000:   Batch Loss = 5.288882, Accuracy = 0.793000042439\n",
      "Performance on test set: Batch Loss = 5.21339797974, Accuracy = 0.805000007153\n",
      "Training epochs #5492000:   Batch Loss = 5.252791, Accuracy = 0.791000008583\n",
      "Performance on test set: Batch Loss = 5.33109283447, Accuracy = 0.779000043869\n",
      "Training epochs #5494000:   Batch Loss = 5.274640, Accuracy = 0.797000050545\n",
      "Performance on test set: Batch Loss = 5.30208396912, Accuracy = 0.797000050545\n",
      "Training epochs #5496000:   Batch Loss = 5.212435, Accuracy = 0.805999994278\n",
      "Performance on test set: Batch Loss = 5.20411491394, Accuracy = 0.814999997616\n",
      "Training epochs #5498000:   Batch Loss = 5.178029, Accuracy = 0.81299996376\n",
      "Performance on test set: Batch Loss = 5.1943936348, Accuracy = 0.81299996376\n",
      "Training epochs #5500000:   Batch Loss = 5.226004, Accuracy = 0.792999982834\n",
      "Performance on test set: Batch Loss = 5.25472068787, Accuracy = 0.787000060081\n",
      "Training epochs #5502000:   Batch Loss = 5.155869, Accuracy = 0.834000110626\n",
      "Performance on test set: Batch Loss = 5.20396757126, Accuracy = 0.802999973297\n",
      "Training epochs #5504000:   Batch Loss = 5.207988, Accuracy = 0.79800003767\n",
      "Performance on test set: Batch Loss = 5.20190906525, Accuracy = 0.817999958992\n",
      "Training epochs #5506000:   Batch Loss = 5.212610, Accuracy = 0.81299996376\n",
      "Performance on test set: Batch Loss = 5.18143320084, Accuracy = 0.805999994278\n",
      "Training epochs #5508000:   Batch Loss = 5.196797, Accuracy = 0.813000023365\n",
      "Performance on test set: Batch Loss = 5.2211523056, Accuracy = 0.791999995708\n",
      "Training epochs #5510000:   Batch Loss = 5.285608, Accuracy = 0.787999987602\n",
      "Performance on test set: Batch Loss = 5.16377496719, Accuracy = 0.816999971867\n",
      "Training epochs #5512000:   Batch Loss = 5.171920, Accuracy = 0.808000028133\n",
      "Performance on test set: Batch Loss = 5.26184654236, Accuracy = 0.778999984264\n",
      "Training epochs #5514000:   Batch Loss = 5.178514, Accuracy = 0.810000002384\n",
      "Performance on test set: Batch Loss = 5.23213911057, Accuracy = 0.792999982834\n",
      "Training epochs #5516000:   Batch Loss = 5.233431, Accuracy = 0.791000008583\n",
      "Performance on test set: Batch Loss = 5.16241598129, Accuracy = 0.814999997616\n",
      "Training epochs #5518000:   Batch Loss = 5.183951, Accuracy = 0.801999986172\n",
      "Performance on test set: Batch Loss = 5.13020324707, Accuracy = 0.825999975204\n",
      "Training epochs #5520000:   Batch Loss = 5.175560, Accuracy = 0.807000041008\n",
      "Performance on test set: Batch Loss = 5.22084474564, Accuracy = 0.783999919891\n",
      "Training epochs #5522000:   Batch Loss = 5.166358, Accuracy = 0.809000015259\n",
      "Performance on test set: Batch Loss = 5.15417289734, Accuracy = 0.817999958992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #5524000:   Batch Loss = 5.132688, Accuracy = 0.816999971867\n",
      "Performance on test set: Batch Loss = 5.17839384079, Accuracy = 0.815999984741\n",
      "Training epochs #5526000:   Batch Loss = 5.194003, Accuracy = 0.794999957085\n",
      "Performance on test set: Batch Loss = 5.14839458466, Accuracy = 0.81200003624\n",
      "Training epochs #5528000:   Batch Loss = 5.204936, Accuracy = 0.789999961853\n",
      "Performance on test set: Batch Loss = 5.19012975693, Accuracy = 0.802999973297\n",
      "Training epochs #5530000:   Batch Loss = 5.212590, Accuracy = 0.802999973297\n",
      "Performance on test set: Batch Loss = 5.14388132095, Accuracy = 0.824000060558\n",
      "Training epochs #5532000:   Batch Loss = 5.188981, Accuracy = 0.81299996376\n",
      "Performance on test set: Batch Loss = 5.22940301895, Accuracy = 0.77999997139\n",
      "Training epochs #5534000:   Batch Loss = 5.172322, Accuracy = 0.805000066757\n",
      "Performance on test set: Batch Loss = 5.20578336716, Accuracy = 0.797999978065\n",
      "Training epochs #5536000:   Batch Loss = 5.224215, Accuracy = 0.795000076294\n",
      "Performance on test set: Batch Loss = 5.12981271744, Accuracy = 0.815999984741\n",
      "Training epochs #5538000:   Batch Loss = 5.204392, Accuracy = 0.799000024796\n",
      "Performance on test set: Batch Loss = 5.10219812393, Accuracy = 0.836000025272\n",
      "Training epochs #5540000:   Batch Loss = 5.140338, Accuracy = 0.818000018597\n",
      "Performance on test set: Batch Loss = 5.18038368225, Accuracy = 0.792999982834\n",
      "Training epochs #5542000:   Batch Loss = 5.188298, Accuracy = 0.796000063419\n",
      "Performance on test set: Batch Loss = 5.11801815033, Accuracy = 0.818999946117\n",
      "Training epochs #5544000:   Batch Loss = 5.115923, Accuracy = 0.813000023365\n",
      "Performance on test set: Batch Loss = 5.14417362213, Accuracy = 0.816000044346\n",
      "Training epochs #5546000:   Batch Loss = 5.204232, Accuracy = 0.789000034332\n",
      "Performance on test set: Batch Loss = 5.11315727234, Accuracy = 0.81200003624\n",
      "Training epochs #5548000:   Batch Loss = 5.150165, Accuracy = 0.804000079632\n",
      "Performance on test set: Batch Loss = 5.15574216843, Accuracy = 0.805000007153\n",
      "Training epochs #5550000:   Batch Loss = 5.152378, Accuracy = 0.81400001049\n",
      "Performance on test set: Batch Loss = 5.10106086731, Accuracy = 0.825000047684\n",
      "Training epochs #5552000:   Batch Loss = 5.154252, Accuracy = 0.805000007153\n",
      "Performance on test set: Batch Loss = 5.19270181656, Accuracy = 0.782999992371\n",
      "Training epochs #5554000:   Batch Loss = 5.140755, Accuracy = 0.803999960423\n",
      "Performance on test set: Batch Loss = 5.1649723053, Accuracy = 0.797999978065\n",
      "Training epochs #5556000:   Batch Loss = 5.121778, Accuracy = 0.809000074863\n",
      "Performance on test set: Batch Loss = 5.09307479858, Accuracy = 0.818999886513\n",
      "Training epochs #5558000:   Batch Loss = 5.118824, Accuracy = 0.808000028133\n",
      "Performance on test set: Batch Loss = 5.06793642044, Accuracy = 0.832000017166\n",
      "Training epochs #5560000:   Batch Loss = 5.047435, Accuracy = 0.832000076771\n",
      "Performance on test set: Batch Loss = 5.1342010498, Accuracy = 0.795000016689\n",
      "Training epochs #5562000:   Batch Loss = 5.123474, Accuracy = 0.81200003624\n",
      "Performance on test set: Batch Loss = 5.08236169815, Accuracy = 0.831000089645\n",
      "Training epochs #5564000:   Batch Loss = 5.145380, Accuracy = 0.810000061989\n",
      "Performance on test set: Batch Loss = 5.10905313492, Accuracy = 0.815999984741\n",
      "Training epochs #5566000:   Batch Loss = 5.085620, Accuracy = 0.81500005722\n",
      "Performance on test set: Batch Loss = 5.07200145721, Accuracy = 0.819000005722\n",
      "Training epochs #5568000:   Batch Loss = 5.121096, Accuracy = 0.804000020027\n",
      "Performance on test set: Batch Loss = 5.10316944122, Accuracy = 0.799000024796\n",
      "Training epochs #5570000:   Batch Loss = 5.104048, Accuracy = 0.805999994278\n",
      "Performance on test set: Batch Loss = 5.0524687767, Accuracy = 0.825999975204\n",
      "Training epochs #5572000:   Batch Loss = 5.082634, Accuracy = 0.799999952316\n",
      "Performance on test set: Batch Loss = 5.1388220787, Accuracy = 0.78100001812\n",
      "Training epochs #5574000:   Batch Loss = 5.077230, Accuracy = 0.808999955654\n",
      "Performance on test set: Batch Loss = 5.10140323639, Accuracy = 0.806999921799\n",
      "Training epochs #5576000:   Batch Loss = 5.052853, Accuracy = 0.824000000954\n",
      "Performance on test set: Batch Loss = 5.04422473907, Accuracy = 0.830999970436\n",
      "Training epochs #5578000:   Batch Loss = 5.025006, Accuracy = 0.832000017166\n",
      "Performance on test set: Batch Loss = 5.0013217926, Accuracy = 0.856999993324\n",
      "Training epochs #5580000:   Batch Loss = 5.043299, Accuracy = 0.813999950886\n",
      "Performance on test set: Batch Loss = 5.0704741478, Accuracy = 0.805000066757\n",
      "Training epochs #5582000:   Batch Loss = 4.967473, Accuracy = 0.852000057697\n",
      "Performance on test set: Batch Loss = 5.023832798, Accuracy = 0.830000042915\n",
      "Training epochs #5584000:   Batch Loss = 5.030911, Accuracy = 0.817999958992\n",
      "Performance on test set: Batch Loss = 5.07189226151, Accuracy = 0.825999975204\n",
      "Training epochs #5586000:   Batch Loss = 5.065130, Accuracy = 0.82699996233\n",
      "Performance on test set: Batch Loss = 5.02050733566, Accuracy = 0.824000060558\n",
      "Training epochs #5588000:   Batch Loss = 5.017694, Accuracy = 0.824999928474\n",
      "Performance on test set: Batch Loss = 5.0547118187, Accuracy = 0.81400001049\n",
      "Training epochs #5590000:   Batch Loss = 5.131513, Accuracy = 0.818000018597\n",
      "Performance on test set: Batch Loss = 5.06894636154, Accuracy = 0.84500002861\n",
      "Training epochs #5592000:   Batch Loss = 5.022758, Accuracy = 0.829999923706\n",
      "Performance on test set: Batch Loss = 5.10393238068, Accuracy = 0.789999961853\n",
      "Training epochs #5594000:   Batch Loss = 5.031347, Accuracy = 0.827000021935\n",
      "Performance on test set: Batch Loss = 5.07891559601, Accuracy = 0.813999950886\n",
      "Training epochs #5596000:   Batch Loss = 5.047492, Accuracy = 0.808000087738\n",
      "Performance on test set: Batch Loss = 5.02584934235, Accuracy = 0.844000041485\n",
      "Training epochs #5598000:   Batch Loss = 5.013165, Accuracy = 0.835000038147\n",
      "Performance on test set: Batch Loss = 4.93315124512, Accuracy = 0.849999964237\n",
      "Training epochs #5600000:   Batch Loss = 5.101695, Accuracy = 0.827000021935\n",
      "Performance on test set: Batch Loss = 5.26086425781, Accuracy = 0.777999997139\n",
      "Training epochs #5602000:   Batch Loss = 5.364680, Accuracy = 0.782000005245\n",
      "Performance on test set: Batch Loss = 5.34748888016, Accuracy = 0.769999980927\n",
      "Training epochs #5604000:   Batch Loss = 5.266270, Accuracy = 0.786000013351\n",
      "Performance on test set: Batch Loss = 5.38591384888, Accuracy = 0.76800006628\n",
      "Training epochs #5606000:   Batch Loss = 5.428259, Accuracy = 0.73199993372\n",
      "Performance on test set: Batch Loss = 5.31942462921, Accuracy = 0.769000053406\n",
      "Training epochs #5608000:   Batch Loss = 5.360109, Accuracy = 0.744000017643\n",
      "Performance on test set: Batch Loss = 5.39041280746, Accuracy = 0.769999980927\n",
      "Training epochs #5610000:   Batch Loss = 5.334021, Accuracy = 0.772000014782\n",
      "Performance on test set: Batch Loss = 5.20717716217, Accuracy = 0.778000056744\n",
      "Training epochs #5612000:   Batch Loss = 5.175509, Accuracy = 0.776000022888\n",
      "Performance on test set: Batch Loss = 5.28299331665, Accuracy = 0.754999995232\n",
      "Training epochs #5614000:   Batch Loss = 5.324440, Accuracy = 0.759000062943\n",
      "Performance on test set: Batch Loss = 5.2726650238, Accuracy = 0.757000029087\n",
      "Training epochs #5616000:   Batch Loss = 5.266710, Accuracy = 0.752999961376\n",
      "Performance on test set: Batch Loss = 5.14889478683, Accuracy = 0.796000003815\n",
      "Training epochs #5618000:   Batch Loss = 5.260183, Accuracy = 0.747999966145\n",
      "Performance on test set: Batch Loss = 5.17328023911, Accuracy = 0.789999961853\n",
      "Training epochs #5620000:   Batch Loss = 5.196236, Accuracy = 0.774999976158\n",
      "Performance on test set: Batch Loss = 5.22215604782, Accuracy = 0.749000012875\n",
      "Training epochs #5622000:   Batch Loss = 5.220262, Accuracy = 0.754000008106\n",
      "Performance on test set: Batch Loss = 5.19246339798, Accuracy = 0.770999968052\n",
      "Training epochs #5624000:   Batch Loss = 5.158915, Accuracy = 0.773000001907\n",
      "Performance on test set: Batch Loss = 5.17013406754, Accuracy = 0.772000074387\n",
      "Training epochs #5626000:   Batch Loss = 5.186087, Accuracy = 0.754999995232\n",
      "Performance on test set: Batch Loss = 5.07415771484, Accuracy = 0.773000001907\n",
      "Training epochs #5628000:   Batch Loss = 5.160966, Accuracy = 0.755999982357\n",
      "Performance on test set: Batch Loss = 5.11954641342, Accuracy = 0.773999989033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #5630000:   Batch Loss = 5.084791, Accuracy = 0.769000053406\n",
      "Performance on test set: Batch Loss = 5.04993629456, Accuracy = 0.779000043869\n",
      "Training epochs #5632000:   Batch Loss = 5.085443, Accuracy = 0.766999959946\n",
      "Performance on test set: Batch Loss = 5.10726261139, Accuracy = 0.756999969482\n",
      "Training epochs #5634000:   Batch Loss = 5.068117, Accuracy = 0.776000022888\n",
      "Performance on test set: Batch Loss = 5.06955480576, Accuracy = 0.757000029087\n",
      "Training epochs #5636000:   Batch Loss = 5.023280, Accuracy = 0.775999963284\n",
      "Performance on test set: Batch Loss = 5.00548887253, Accuracy = 0.82800000906\n",
      "Training epochs #5638000:   Batch Loss = 5.020061, Accuracy = 0.821000039577\n",
      "Performance on test set: Batch Loss = 4.98724222183, Accuracy = 0.866999983788\n",
      "Training epochs #5640000:   Batch Loss = 4.950034, Accuracy = 0.85799998045\n",
      "Performance on test set: Batch Loss = 5.01543521881, Accuracy = 0.81200003624\n",
      "Training epochs #5642000:   Batch Loss = 4.995284, Accuracy = 0.819999992847\n",
      "Performance on test set: Batch Loss = 4.99277448654, Accuracy = 0.819999992847\n",
      "Training epochs #5644000:   Batch Loss = 5.031451, Accuracy = 0.808000028133\n",
      "Performance on test set: Batch Loss = 5.01560592651, Accuracy = 0.818000018597\n",
      "Training epochs #5646000:   Batch Loss = 4.978367, Accuracy = 0.814999997616\n",
      "Performance on test set: Batch Loss = 4.95587444305, Accuracy = 0.815999984741\n",
      "Training epochs #5648000:   Batch Loss = 4.991389, Accuracy = 0.817000031471\n",
      "Performance on test set: Batch Loss = 4.97217035294, Accuracy = 0.818000078201\n",
      "Training epochs #5650000:   Batch Loss = 4.979152, Accuracy = 0.851999998093\n",
      "Performance on test set: Batch Loss = 4.9485502243, Accuracy = 0.850000023842\n",
      "Training epochs #5652000:   Batch Loss = 4.971498, Accuracy = 0.839000046253\n",
      "Performance on test set: Batch Loss = 4.99958848953, Accuracy = 0.827000021935\n",
      "Training epochs #5654000:   Batch Loss = 4.973352, Accuracy = 0.832000017166\n",
      "Performance on test set: Batch Loss = 4.97436714172, Accuracy = 0.841000020504\n",
      "Training epochs #5656000:   Batch Loss = 4.930655, Accuracy = 0.851999998093\n",
      "Performance on test set: Batch Loss = 4.93188285828, Accuracy = 0.858000040054\n",
      "Training epochs #5658000:   Batch Loss = 4.920582, Accuracy = 0.858000099659\n",
      "Performance on test set: Batch Loss = 4.90519618988, Accuracy = 0.867000043392\n",
      "Training epochs #5660000:   Batch Loss = 4.916808, Accuracy = 0.838000059128\n",
      "Performance on test set: Batch Loss = 4.94844484329, Accuracy = 0.845999956131\n",
      "Training epochs #5662000:   Batch Loss = 4.876575, Accuracy = 0.875\n",
      "Performance on test set: Batch Loss = 4.92936086655, Accuracy = 0.861999988556\n",
      "Training epochs #5664000:   Batch Loss = 4.924268, Accuracy = 0.853999972343\n",
      "Performance on test set: Batch Loss = 4.953083992, Accuracy = 0.851999998093\n",
      "Training epochs #5666000:   Batch Loss = 4.942241, Accuracy = 0.842000067234\n",
      "Performance on test set: Batch Loss = 4.90822172165, Accuracy = 0.850000023842\n",
      "Training epochs #5668000:   Batch Loss = 4.905570, Accuracy = 0.849000036716\n",
      "Performance on test set: Batch Loss = 4.92216968536, Accuracy = 0.849999964237\n",
      "Training epochs #5670000:   Batch Loss = 4.991003, Accuracy = 0.835000038147\n",
      "Performance on test set: Batch Loss = 4.90325737, Accuracy = 0.850000083447\n",
      "Training epochs #5672000:   Batch Loss = 4.878357, Accuracy = 0.865999996662\n",
      "Performance on test set: Batch Loss = 4.95879173279, Accuracy = 0.825999975204\n",
      "Training epochs #5674000:   Batch Loss = 4.876213, Accuracy = 0.852999985218\n",
      "Performance on test set: Batch Loss = 4.9373884201, Accuracy = 0.841000020504\n",
      "Training epochs #5676000:   Batch Loss = 4.956881, Accuracy = 0.835000038147\n",
      "Performance on test set: Batch Loss = 4.90248775482, Accuracy = 0.860000014305\n",
      "Training epochs #5678000:   Batch Loss = 4.878142, Accuracy = 0.847000002861\n",
      "Performance on test set: Batch Loss = 4.87533521652, Accuracy = 0.867999970913\n",
      "Training epochs #5680000:   Batch Loss = 4.908966, Accuracy = 0.851000010967\n",
      "Performance on test set: Batch Loss = 4.92055892944, Accuracy = 0.846000015736\n",
      "Training epochs #5682000:   Batch Loss = 4.864769, Accuracy = 0.858000040054\n",
      "Performance on test set: Batch Loss = 4.90098762512, Accuracy = 0.862000048161\n",
      "Training epochs #5684000:   Batch Loss = 4.887671, Accuracy = 0.854000091553\n",
      "Performance on test set: Batch Loss = 4.92927503586, Accuracy = 0.852000057697\n",
      "Training epochs #5686000:   Batch Loss = 4.896824, Accuracy = 0.84399998188\n",
      "Performance on test set: Batch Loss = 4.88097238541, Accuracy = 0.849000036716\n",
      "Training epochs #5688000:   Batch Loss = 4.913289, Accuracy = 0.844000041485\n",
      "Performance on test set: Batch Loss = 4.90470695496, Accuracy = 0.848999977112\n",
      "Training epochs #5690000:   Batch Loss = 4.949953, Accuracy = 0.835000038147\n",
      "Performance on test set: Batch Loss = 4.87342643738, Accuracy = 0.852000057697\n",
      "Training epochs #5692000:   Batch Loss = 4.885609, Accuracy = 0.859000086784\n",
      "Performance on test set: Batch Loss = 4.93075752258, Accuracy = 0.824999928474\n",
      "Training epochs #5694000:   Batch Loss = 4.878752, Accuracy = 0.851999998093\n",
      "Performance on test set: Batch Loss = 4.91259002686, Accuracy = 0.841000020504\n",
      "Training epochs #5696000:   Batch Loss = 4.918019, Accuracy = 0.845999956131\n",
      "Performance on test set: Batch Loss = 4.87595176697, Accuracy = 0.8599999547\n",
      "Training epochs #5698000:   Batch Loss = 4.923562, Accuracy = 0.834999978542\n",
      "Performance on test set: Batch Loss = 4.85864496231, Accuracy = 0.867000043392\n",
      "Training epochs #5700000:   Batch Loss = 4.895706, Accuracy = 0.852999985218\n",
      "Performance on test set: Batch Loss = 4.8879570961, Accuracy = 0.847000062466\n",
      "Training epochs #5702000:   Batch Loss = 4.930465, Accuracy = 0.839000046253\n",
      "Performance on test set: Batch Loss = 4.86931371689, Accuracy = 0.861000001431\n",
      "Training epochs #5704000:   Batch Loss = 4.845922, Accuracy = 0.861000061035\n",
      "Performance on test set: Batch Loss = 4.90245294571, Accuracy = 0.854000031948\n",
      "Training epochs #5706000:   Batch Loss = 4.932150, Accuracy = 0.834000051022\n",
      "Performance on test set: Batch Loss = 4.85685825348, Accuracy = 0.849000036716\n",
      "Training epochs #5708000:   Batch Loss = 4.903296, Accuracy = 0.853000104427\n",
      "Performance on test set: Batch Loss = 4.8771276474, Accuracy = 0.848999977112\n",
      "Training epochs #5710000:   Batch Loss = 4.858751, Accuracy = 0.857000052929\n",
      "Performance on test set: Batch Loss = 4.85097694397, Accuracy = 0.851999998093\n",
      "Training epochs #5712000:   Batch Loss = 4.900629, Accuracy = 0.834999978542\n",
      "Performance on test set: Batch Loss = 4.90697526932, Accuracy = 0.8259999156\n",
      "Training epochs #5714000:   Batch Loss = 4.860498, Accuracy = 0.86000007391\n",
      "Performance on test set: Batch Loss = 4.89598417282, Accuracy = 0.842000007629\n",
      "Training epochs #5716000:   Batch Loss = 4.851980, Accuracy = 0.847999930382\n",
      "Performance on test set: Batch Loss = 4.8512878418, Accuracy = 0.860000014305\n",
      "Training epochs #5718000:   Batch Loss = 4.851365, Accuracy = 0.847999930382\n",
      "Performance on test set: Batch Loss = 4.83805608749, Accuracy = 0.867000043392\n",
      "Training epochs #5720000:   Batch Loss = 4.802214, Accuracy = 0.861000001431\n",
      "Performance on test set: Batch Loss = 4.8690328598, Accuracy = 0.847000062466\n",
      "Training epochs #5722000:   Batch Loss = 4.865899, Accuracy = 0.852000057697\n",
      "Performance on test set: Batch Loss = 4.84320926666, Accuracy = 0.862000048161\n",
      "Training epochs #5724000:   Batch Loss = 4.910108, Accuracy = 0.833999991417\n",
      "Performance on test set: Batch Loss = 4.87746429443, Accuracy = 0.855000078678\n",
      "Training epochs #5726000:   Batch Loss = 4.838884, Accuracy = 0.853999972343\n",
      "Performance on test set: Batch Loss = 4.83478975296, Accuracy = 0.847000002861\n",
      "Training epochs #5728000:   Batch Loss = 4.867654, Accuracy = 0.850000083447\n",
      "Performance on test set: Batch Loss = 4.85981225967, Accuracy = 0.848999917507\n",
      "Training epochs #5730000:   Batch Loss = 4.856565, Accuracy = 0.852999985218\n",
      "Performance on test set: Batch Loss = 4.83643913269, Accuracy = 0.852000057697\n",
      "Training epochs #5732000:   Batch Loss = 4.854604, Accuracy = 0.838000059128\n",
      "Performance on test set: Batch Loss = 4.88556909561, Accuracy = 0.825999975204\n",
      "Training epochs #5734000:   Batch Loss = 4.852975, Accuracy = 0.833999991417\n",
      "Performance on test set: Batch Loss = 4.87134075165, Accuracy = 0.841000020504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #5736000:   Batch Loss = 4.827989, Accuracy = 0.858000040054\n",
      "Performance on test set: Batch Loss = 4.83002281189, Accuracy = 0.861000001431\n",
      "Training epochs #5738000:   Batch Loss = 4.825511, Accuracy = 0.85900002718\n",
      "Performance on test set: Batch Loss = 4.81808185577, Accuracy = 0.866999924183\n",
      "Training epochs #5740000:   Batch Loss = 4.831264, Accuracy = 0.839000046253\n",
      "Performance on test set: Batch Loss = 4.84019565582, Accuracy = 0.848000049591\n",
      "Training epochs #5742000:   Batch Loss = 4.778972, Accuracy = 0.875\n",
      "Performance on test set: Batch Loss = 4.82389116287, Accuracy = 0.861999988556\n",
      "Training epochs #5744000:   Batch Loss = 4.832574, Accuracy = 0.854000031948\n",
      "Performance on test set: Batch Loss = 4.8630065918, Accuracy = 0.855000078678\n",
      "Training epochs #5746000:   Batch Loss = 4.850618, Accuracy = 0.841000020504\n",
      "Performance on test set: Batch Loss = 4.8126707077, Accuracy = 0.848999977112\n",
      "Training epochs #5748000:   Batch Loss = 4.814761, Accuracy = 0.848999917507\n",
      "Performance on test set: Batch Loss = 4.83729791641, Accuracy = 0.848999917507\n",
      "Training epochs #5750000:   Batch Loss = 5.402304, Accuracy = 0.836000025272\n",
      "Performance on test set: Batch Loss = 8.41028213501, Accuracy = 0.0640000030398\n",
      "Training epochs #5752000:   Batch Loss = 8.613287, Accuracy = 0.0439999997616\n",
      "Performance on test set: Batch Loss = 8.57063674927, Accuracy = 0.0489999987185\n",
      "Training epochs #5754000:   Batch Loss = 8.198524, Accuracy = 0.0410000048578\n",
      "Performance on test set: Batch Loss = 7.9710483551, Accuracy = 0.0529999993742\n",
      "Training epochs #5756000:   Batch Loss = 7.959828, Accuracy = 0.0530000030994\n",
      "Performance on test set: Batch Loss = 7.51640129089, Accuracy = 0.0530000030994\n",
      "Training epochs #5758000:   Batch Loss = 7.061705, Accuracy = 0.665999948978\n",
      "Performance on test set: Batch Loss = 6.97429466248, Accuracy = 0.658999919891\n",
      "Training epochs #5760000:   Batch Loss = 6.985926, Accuracy = 0.648999929428\n",
      "Performance on test set: Batch Loss = 7.05146074295, Accuracy = 0.627000033855\n",
      "Training epochs #5762000:   Batch Loss = 6.981716, Accuracy = 0.658999979496\n",
      "Performance on test set: Batch Loss = 6.96427583694, Accuracy = 0.662000000477\n",
      "Training epochs #5764000:   Batch Loss = 7.209532, Accuracy = 0.650999963284\n",
      "Performance on test set: Batch Loss = 7.23547554016, Accuracy = 0.638999998569\n",
      "Training epochs #5766000:   Batch Loss = 7.175198, Accuracy = 0.619999945164\n",
      "Performance on test set: Batch Loss = 6.86009454727, Accuracy = 0.666999995708\n",
      "Training epochs #5768000:   Batch Loss = 7.201615, Accuracy = 0.616999983788\n",
      "Performance on test set: Batch Loss = 6.92410850525, Accuracy = 0.637000024319\n",
      "Training epochs #5770000:   Batch Loss = 6.778943, Accuracy = 0.652000010014\n",
      "Performance on test set: Batch Loss = 6.67621803284, Accuracy = 0.659999966621\n",
      "Training epochs #5772000:   Batch Loss = 6.708094, Accuracy = 0.64099997282\n",
      "Performance on test set: Batch Loss = 6.82222080231, Accuracy = 0.623000025749\n",
      "Training epochs #5774000:   Batch Loss = 6.666301, Accuracy = 0.633999943733\n",
      "Performance on test set: Batch Loss = 6.60850954056, Accuracy = 0.638999998569\n",
      "Training epochs #5776000:   Batch Loss = 6.617393, Accuracy = 0.635000050068\n",
      "Performance on test set: Batch Loss = 6.63215303421, Accuracy = 0.661000013351\n",
      "Training epochs #5778000:   Batch Loss = 6.575783, Accuracy = 0.627000033855\n",
      "Performance on test set: Batch Loss = 6.45383739471, Accuracy = 0.658999979496\n",
      "Training epochs #5780000:   Batch Loss = 6.481104, Accuracy = 0.650000035763\n",
      "Performance on test set: Batch Loss = 6.42472505569, Accuracy = 0.628000020981\n",
      "Training epochs #5782000:   Batch Loss = 6.379577, Accuracy = 0.634999990463\n",
      "Performance on test set: Batch Loss = 6.2376074791, Accuracy = 0.662999987602\n",
      "Training epochs #5784000:   Batch Loss = 6.283647, Accuracy = 0.643000006676\n",
      "Performance on test set: Batch Loss = 6.34481811523, Accuracy = 0.638999998569\n",
      "Training epochs #5786000:   Batch Loss = 6.256626, Accuracy = 0.638999998569\n",
      "Performance on test set: Batch Loss = 6.12749004364, Accuracy = 0.666999995708\n",
      "Training epochs #5788000:   Batch Loss = 6.235503, Accuracy = 0.636000037193\n",
      "Performance on test set: Batch Loss = 6.21080446243, Accuracy = 0.636999964714\n",
      "Training epochs #5790000:   Batch Loss = 6.191837, Accuracy = 0.639999985695\n",
      "Performance on test set: Batch Loss = 6.0754199028, Accuracy = 0.659999966621\n",
      "Training epochs #5792000:   Batch Loss = 6.101632, Accuracy = 0.646000027657\n",
      "Performance on test set: Batch Loss = 6.15425777435, Accuracy = 0.622000038624\n",
      "Training epochs #5794000:   Batch Loss = 6.006767, Accuracy = 0.644999980927\n",
      "Performance on test set: Batch Loss = 5.99568748474, Accuracy = 0.638999998569\n",
      "Training epochs #5796000:   Batch Loss = 5.972972, Accuracy = 0.651000022888\n",
      "Performance on test set: Batch Loss = 5.92925786972, Accuracy = 0.661000013351\n",
      "Training epochs #5798000:   Batch Loss = 5.872843, Accuracy = 0.662999987602\n",
      "Performance on test set: Batch Loss = 5.86401128769, Accuracy = 0.658999919891\n",
      "Training epochs #5800000:   Batch Loss = 5.793863, Accuracy = 0.6740000844\n",
      "Performance on test set: Batch Loss = 5.88890457153, Accuracy = 0.626999974251\n",
      "Training epochs #5802000:   Batch Loss = 5.825123, Accuracy = 0.65600001812\n",
      "Performance on test set: Batch Loss = 5.77421236038, Accuracy = 0.661999940872\n",
      "Training epochs #5804000:   Batch Loss = 5.792511, Accuracy = 0.652000010014\n",
      "Performance on test set: Batch Loss = 5.80446815491, Accuracy = 0.638999998569\n",
      "Training epochs #5806000:   Batch Loss = 5.753421, Accuracy = 0.646999955177\n",
      "Performance on test set: Batch Loss = 5.68093824387, Accuracy = 0.666999995708\n",
      "Training epochs #5808000:   Batch Loss = 5.739966, Accuracy = 0.633000016212\n",
      "Performance on test set: Batch Loss = 5.73130702972, Accuracy = 0.636000037193\n",
      "Training epochs #5810000:   Batch Loss = 5.705165, Accuracy = 0.638999938965\n",
      "Performance on test set: Batch Loss = 5.63598060608, Accuracy = 0.660000026226\n",
      "Training epochs #5812000:   Batch Loss = 5.639654, Accuracy = 0.654000043869\n",
      "Performance on test set: Batch Loss = 5.70991897583, Accuracy = 0.621999979019\n",
      "Training epochs #5814000:   Batch Loss = 5.674333, Accuracy = 0.634999990463\n",
      "Performance on test set: Batch Loss = 5.65193843842, Accuracy = 0.639000058174\n",
      "Training epochs #5816000:   Batch Loss = 5.572410, Accuracy = 0.665999948978\n",
      "Performance on test set: Batch Loss = 5.57013034821, Accuracy = 0.661000013351\n",
      "Training epochs #5818000:   Batch Loss = 5.567822, Accuracy = 0.666000008583\n",
      "Performance on test set: Batch Loss = 5.58462333679, Accuracy = 0.658999919891\n",
      "Training epochs #5820000:   Batch Loss = 5.613860, Accuracy = 0.638999998569\n",
      "Performance on test set: Batch Loss = 5.65840530396, Accuracy = 0.628000020981\n",
      "Training epochs #5822000:   Batch Loss = 5.528906, Accuracy = 0.680999994278\n",
      "Performance on test set: Batch Loss = 5.59134292603, Accuracy = 0.662000000477\n",
      "Training epochs #5824000:   Batch Loss = 5.624415, Accuracy = 0.638999998569\n",
      "Performance on test set: Batch Loss = 5.63199043274, Accuracy = 0.638999998569\n",
      "Training epochs #5826000:   Batch Loss = 5.620298, Accuracy = 0.647999942303\n",
      "Performance on test set: Batch Loss = 5.57806348801, Accuracy = 0.667000055313\n",
      "Training epochs #5828000:   Batch Loss = 5.585702, Accuracy = 0.660999953747\n",
      "Performance on test set: Batch Loss = 5.64920949936, Accuracy = 0.637000024319\n",
      "Training epochs #5830000:   Batch Loss = 5.678047, Accuracy = 0.633000016212\n",
      "Performance on test set: Batch Loss = 5.57287406921, Accuracy = 0.660000026226\n",
      "Training epochs #5832000:   Batch Loss = 5.582052, Accuracy = 0.636000037193\n",
      "Performance on test set: Batch Loss = 5.66743707657, Accuracy = 0.621999979019\n",
      "Training epochs #5834000:   Batch Loss = 5.552962, Accuracy = 0.673999965191\n",
      "Performance on test set: Batch Loss = 5.62964010239, Accuracy = 0.638999998569\n",
      "Training epochs #5836000:   Batch Loss = 5.639909, Accuracy = 0.622000038624\n",
      "Performance on test set: Batch Loss = 5.54534864426, Accuracy = 0.661000013351\n",
      "Training epochs #5838000:   Batch Loss = 5.571170, Accuracy = 0.664999961853\n",
      "Performance on test set: Batch Loss = 5.55890226364, Accuracy = 0.659000039101\n",
      "Training epochs #5840000:   Batch Loss = 5.576144, Accuracy = 0.649999916553\n",
      "Performance on test set: Batch Loss = 5.62729263306, Accuracy = 0.628000020981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #5842000:   Batch Loss = 5.567734, Accuracy = 0.658999979496\n",
      "Performance on test set: Batch Loss = 5.555975914, Accuracy = 0.664000034332\n",
      "Training epochs #5844000:   Batch Loss = 5.556731, Accuracy = 0.652000010014\n",
      "Performance on test set: Batch Loss = 5.59380722046, Accuracy = 0.639000058174\n",
      "Training epochs #5846000:   Batch Loss = 5.619984, Accuracy = 0.623000025749\n",
      "Performance on test set: Batch Loss = 5.52128744125, Accuracy = 0.668000042439\n",
      "Training epochs #5848000:   Batch Loss = 5.591550, Accuracy = 0.618999958038\n",
      "Performance on test set: Batch Loss = 5.53911399841, Accuracy = 0.639000058174\n",
      "Training epochs #5850000:   Batch Loss = 5.523529, Accuracy = 0.656999945641\n",
      "Performance on test set: Batch Loss = 5.49978017807, Accuracy = 0.661000072956\n",
      "Training epochs #5852000:   Batch Loss = 5.491428, Accuracy = 0.641999959946\n",
      "Performance on test set: Batch Loss = 5.5429816246, Accuracy = 0.624000012875\n",
      "Training epochs #5854000:   Batch Loss = 5.511574, Accuracy = 0.634000062943\n",
      "Performance on test set: Batch Loss = 5.4748878479, Accuracy = 0.646999955177\n",
      "Training epochs #5856000:   Batch Loss = 5.458329, Accuracy = 0.647000014782\n",
      "Performance on test set: Batch Loss = 5.37799024582, Accuracy = 0.672999978065\n",
      "Training epochs #5858000:   Batch Loss = 5.461430, Accuracy = 0.638999998569\n",
      "Performance on test set: Batch Loss = 5.36062860489, Accuracy = 0.677999973297\n",
      "Training epochs #5860000:   Batch Loss = 5.377103, Accuracy = 0.661000013351\n",
      "Performance on test set: Batch Loss = 5.39361906052, Accuracy = 0.638999938965\n",
      "Training epochs #5862000:   Batch Loss = 5.365448, Accuracy = 0.643000006676\n",
      "Performance on test set: Batch Loss = 5.32545423508, Accuracy = 0.671000003815\n",
      "Training epochs #5864000:   Batch Loss = 5.305101, Accuracy = 0.659000039101\n",
      "Performance on test set: Batch Loss = 5.33535814285, Accuracy = 0.644999980927\n",
      "Training epochs #5866000:   Batch Loss = 5.349105, Accuracy = 0.648999989033\n",
      "Performance on test set: Batch Loss = 5.26609611511, Accuracy = 0.672000050545\n",
      "Training epochs #5868000:   Batch Loss = 5.291855, Accuracy = 0.649000048637\n",
      "Performance on test set: Batch Loss = 5.27594137192, Accuracy = 0.65700006485\n",
      "Training epochs #5870000:   Batch Loss = 5.265274, Accuracy = 0.651000022888\n",
      "Performance on test set: Batch Loss = 5.20256328583, Accuracy = 0.669000029564\n",
      "Training epochs #5872000:   Batch Loss = 5.491674, Accuracy = 0.654000043869\n",
      "Performance on test set: Batch Loss = 6.03325557709, Accuracy = 0.070999994874\n",
      "Training epochs #5874000:   Batch Loss = 5.406733, Accuracy = 0.662999987602\n",
      "Performance on test set: Batch Loss = 5.69980573654, Accuracy = 0.64200001955\n",
      "Training epochs #5876000:   Batch Loss = 5.681683, Accuracy = 0.652999937534\n",
      "Performance on test set: Batch Loss = 5.48672914505, Accuracy = 0.65700006485\n",
      "Training epochs #5878000:   Batch Loss = 5.446424, Accuracy = 0.662999987602\n",
      "Performance on test set: Batch Loss = 5.3326292038, Accuracy = 0.657999992371\n",
      "Training epochs #5880000:   Batch Loss = 5.248712, Accuracy = 0.780000030994\n",
      "Performance on test set: Batch Loss = 5.35997962952, Accuracy = 0.742999970913\n",
      "Training epochs #5882000:   Batch Loss = 5.345922, Accuracy = 0.775999963284\n",
      "Performance on test set: Batch Loss = 5.28499650955, Accuracy = 0.761999964714\n",
      "Training epochs #5884000:   Batch Loss = 5.279087, Accuracy = 0.768999993801\n",
      "Performance on test set: Batch Loss = 5.31154680252, Accuracy = 0.764000058174\n",
      "Training epochs #5886000:   Batch Loss = 5.264675, Accuracy = 0.771000027657\n",
      "Performance on test set: Batch Loss = 5.25554275513, Accuracy = 0.772000074387\n",
      "Training epochs #5888000:   Batch Loss = 5.270285, Accuracy = 0.774000048637\n",
      "Performance on test set: Batch Loss = 5.26558113098, Accuracy = 0.771000027657\n",
      "Training epochs #5890000:   Batch Loss = 5.861925, Accuracy = 0.149000003934\n",
      "Performance on test set: Batch Loss = 5.54523801804, Accuracy = 0.782999992371\n",
      "Training epochs #5892000:   Batch Loss = 5.890376, Accuracy = 0.665000021458\n",
      "Performance on test set: Batch Loss = 5.85673713684, Accuracy = 0.634000003338\n",
      "Training epochs #5894000:   Batch Loss = 5.813710, Accuracy = 0.638999998569\n",
      "Performance on test set: Batch Loss = 5.78710651398, Accuracy = 0.64200001955\n",
      "Training epochs #5896000:   Batch Loss = 5.723221, Accuracy = 0.664999961853\n",
      "Performance on test set: Batch Loss = 5.77472114563, Accuracy = 0.656999945641\n",
      "Training epochs #5898000:   Batch Loss = 5.800566, Accuracy = 0.662000000477\n",
      "Performance on test set: Batch Loss = 5.7558927536, Accuracy = 0.657999992371\n",
      "Training epochs #5900000:   Batch Loss = 5.764530, Accuracy = 0.64099997282\n",
      "Performance on test set: Batch Loss = 5.7897439003, Accuracy = 0.629000008106\n",
      "Training epochs #5902000:   Batch Loss = 5.597822, Accuracy = 0.679000020027\n",
      "Performance on test set: Batch Loss = 5.65215826035, Accuracy = 0.658999979496\n",
      "Training epochs #5904000:   Batch Loss = 5.693716, Accuracy = 0.633999943733\n",
      "Performance on test set: Batch Loss = 5.6769695282, Accuracy = 0.640000104904\n",
      "Training epochs #5906000:   Batch Loss = 5.610962, Accuracy = 0.651999950409\n",
      "Performance on test set: Batch Loss = 5.58675956726, Accuracy = 0.668999969959\n",
      "Training epochs #5908000:   Batch Loss = 5.591262, Accuracy = 0.666999995708\n",
      "Performance on test set: Batch Loss = 5.65517997742, Accuracy = 0.634999990463\n",
      "Training epochs #5910000:   Batch Loss = 5.671485, Accuracy = 0.634000003338\n",
      "Performance on test set: Batch Loss = 5.56648302078, Accuracy = 0.667999982834\n",
      "Training epochs #5912000:   Batch Loss = 5.554300, Accuracy = 0.638999938965\n",
      "Performance on test set: Batch Loss = 5.65002346039, Accuracy = 0.626999974251\n",
      "Training epochs #5914000:   Batch Loss = 5.529319, Accuracy = 0.67099994421\n",
      "Performance on test set: Batch Loss = 5.60420560837, Accuracy = 0.641000032425\n",
      "Training epochs #5916000:   Batch Loss = 5.613226, Accuracy = 0.625\n",
      "Performance on test set: Batch Loss = 5.52439212799, Accuracy = 0.657000005245\n",
      "Training epochs #5918000:   Batch Loss = 5.531699, Accuracy = 0.666999936104\n",
      "Performance on test set: Batch Loss = 5.53172302246, Accuracy = 0.657999992371\n",
      "Training epochs #5920000:   Batch Loss = 5.561172, Accuracy = 0.648999989033\n",
      "Performance on test set: Batch Loss = 5.61176872253, Accuracy = 0.629000008106\n",
      "Training epochs #5922000:   Batch Loss = 5.554204, Accuracy = 0.659000039101\n",
      "Performance on test set: Batch Loss = 5.54697990417, Accuracy = 0.660000026226\n",
      "Training epochs #5924000:   Batch Loss = 5.537557, Accuracy = 0.652000010014\n",
      "Performance on test set: Batch Loss = 5.59148025513, Accuracy = 0.638000011444\n",
      "Training epochs #5926000:   Batch Loss = 5.621912, Accuracy = 0.619999945164\n",
      "Performance on test set: Batch Loss = 5.50808000565, Accuracy = 0.667999982834\n",
      "Training epochs #5928000:   Batch Loss = 5.595141, Accuracy = 0.615000009537\n",
      "Performance on test set: Batch Loss = 5.55968952179, Accuracy = 0.637000024319\n",
      "Training epochs #5930000:   Batch Loss = 5.528796, Accuracy = 0.650999963284\n",
      "Performance on test set: Batch Loss = 5.49829864502, Accuracy = 0.662000000477\n",
      "Training epochs #5932000:   Batch Loss = 5.527245, Accuracy = 0.64099997282\n",
      "Performance on test set: Batch Loss = 5.58211612701, Accuracy = 0.625000059605\n",
      "Training epochs #5934000:   Batch Loss = 5.551920, Accuracy = 0.632999956608\n",
      "Performance on test set: Batch Loss = 5.5499458313, Accuracy = 0.639999985695\n",
      "Training epochs #5936000:   Batch Loss = 5.528983, Accuracy = 0.635999977589\n",
      "Performance on test set: Batch Loss = 5.46116447449, Accuracy = 0.662000000477\n",
      "Training epochs #5938000:   Batch Loss = 5.551940, Accuracy = 0.629999995232\n",
      "Performance on test set: Batch Loss = 5.45759057999, Accuracy = 0.657999992371\n",
      "Training epochs #5940000:   Batch Loss = 5.493336, Accuracy = 0.649999976158\n",
      "Performance on test set: Batch Loss = 5.52218818665, Accuracy = 0.630000054836\n",
      "Training epochs #5942000:   Batch Loss = 5.509321, Accuracy = 0.637000024319\n",
      "Performance on test set: Batch Loss = 5.47116756439, Accuracy = 0.662999987602\n",
      "Training epochs #5944000:   Batch Loss = 5.472558, Accuracy = 0.644999980927\n",
      "Performance on test set: Batch Loss = 5.51556777954, Accuracy = 0.638999938965\n",
      "Training epochs #5946000:   Batch Loss = 5.532847, Accuracy = 0.641000032425\n",
      "Performance on test set: Batch Loss = 5.43033027649, Accuracy = 0.666999995708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #5948000:   Batch Loss = 5.484081, Accuracy = 0.638999998569\n",
      "Performance on test set: Batch Loss = 5.47871875763, Accuracy = 0.641000032425\n",
      "Training epochs #5950000:   Batch Loss = 5.467113, Accuracy = 0.642999947071\n",
      "Performance on test set: Batch Loss = 5.42327785492, Accuracy = 0.662000060081\n",
      "Training epochs #5952000:   Batch Loss = 5.427034, Accuracy = 0.645999968052\n",
      "Performance on test set: Batch Loss = 5.49550104141, Accuracy = 0.624000012875\n",
      "Training epochs #5954000:   Batch Loss = 5.419699, Accuracy = 0.660000026226\n",
      "Performance on test set: Batch Loss = 5.46933507919, Accuracy = 0.646000027657\n",
      "Training epochs #5956000:   Batch Loss = 5.425244, Accuracy = 0.658999979496\n",
      "Performance on test set: Batch Loss = 5.36384725571, Accuracy = 0.671000003815\n",
      "Training epochs #5958000:   Batch Loss = 5.433438, Accuracy = 0.668999969959\n",
      "Performance on test set: Batch Loss = 5.37033843994, Accuracy = 0.676999986172\n",
      "Training epochs #5960000:   Batch Loss = 5.354353, Accuracy = 0.678000032902\n",
      "Performance on test set: Batch Loss = 5.43953895569, Accuracy = 0.638000011444\n",
      "Training epochs #5962000:   Batch Loss = 5.375324, Accuracy = 0.669000029564\n",
      "Performance on test set: Batch Loss = 5.39601802826, Accuracy = 0.668999910355\n",
      "Training epochs #5964000:   Batch Loss = 5.377983, Accuracy = 0.659999966621\n",
      "Performance on test set: Batch Loss = 5.42584180832, Accuracy = 0.644000053406\n",
      "Training epochs #5966000:   Batch Loss = 5.360064, Accuracy = 0.657000005245\n",
      "Performance on test set: Batch Loss = 5.35128688812, Accuracy = 0.67300003767\n",
      "Training epochs #5968000:   Batch Loss = 5.366030, Accuracy = 0.653000056744\n",
      "Performance on test set: Batch Loss = 5.38521575928, Accuracy = 0.65700006485\n",
      "Training epochs #5970000:   Batch Loss = 5.382116, Accuracy = 0.646999955177\n",
      "Performance on test set: Batch Loss = 5.32882213593, Accuracy = 0.666000008583\n",
      "Training epochs #5972000:   Batch Loss = 5.391786, Accuracy = 0.663000047207\n",
      "Performance on test set: Batch Loss = 5.38936424255, Accuracy = 0.634999990463\n",
      "Training epochs #5974000:   Batch Loss = 5.364907, Accuracy = 0.646000027657\n",
      "Performance on test set: Batch Loss = 5.36677455902, Accuracy = 0.652000010014\n",
      "Training epochs #5976000:   Batch Loss = 5.275416, Accuracy = 0.679000020027\n",
      "Performance on test set: Batch Loss = 5.24790287018, Accuracy = 0.680999994278\n",
      "Training epochs #5978000:   Batch Loss = 5.273619, Accuracy = 0.681000053883\n",
      "Performance on test set: Batch Loss = 5.26290225983, Accuracy = 0.677999973297\n",
      "Training epochs #5980000:   Batch Loss = 5.270633, Accuracy = 0.652000010014\n",
      "Performance on test set: Batch Loss = 5.32054519653, Accuracy = 0.751999974251\n",
      "Training epochs #5982000:   Batch Loss = 5.183986, Accuracy = 0.803000032902\n",
      "Performance on test set: Batch Loss = 5.28139638901, Accuracy = 0.768999993801\n",
      "Training epochs #5984000:   Batch Loss = 5.287178, Accuracy = 0.759999990463\n",
      "Performance on test set: Batch Loss = 5.29211521149, Accuracy = 0.768000006676\n",
      "Training epochs #5986000:   Batch Loss = 5.279496, Accuracy = 0.761999964714\n",
      "Performance on test set: Batch Loss = 5.21426057816, Accuracy = 0.773999989033\n",
      "Training epochs #5988000:   Batch Loss = 5.198804, Accuracy = 0.782000005245\n",
      "Performance on test set: Batch Loss = 5.24105548859, Accuracy = 0.769999980927\n",
      "Training epochs #5990000:   Batch Loss = 5.289152, Accuracy = 0.754000008106\n",
      "Performance on test set: Batch Loss = 5.17850923538, Accuracy = 0.782000005245\n",
      "Training epochs #5992000:   Batch Loss = 5.149927, Accuracy = 0.784999966621\n",
      "Performance on test set: Batch Loss = 5.22410678864, Accuracy = 0.755999922752\n",
      "Training epochs #5994000:   Batch Loss = 5.137843, Accuracy = 0.777000069618\n",
      "Performance on test set: Batch Loss = 5.20952129364, Accuracy = 0.758000016212\n",
      "Training epochs #5996000:   Batch Loss = 5.180894, Accuracy = 0.766999959946\n",
      "Performance on test set: Batch Loss = 5.10087442398, Accuracy = 0.791000008583\n",
      "Training epochs #5998000:   Batch Loss = 5.105718, Accuracy = 0.78100001812\n",
      "Performance on test set: Batch Loss = 5.10949277878, Accuracy = 0.786000013351\n",
      "Training epochs #6000000:   Batch Loss = 5.147088, Accuracy = 0.764999985695\n",
      "Performance on test set: Batch Loss = 5.15786838531, Accuracy = 0.75\n",
      "Training epochs #6002000:   Batch Loss = 5.090582, Accuracy = 0.77999997139\n",
      "Performance on test set: Batch Loss = 5.1286482811, Accuracy = 0.768000006676\n",
      "Training epochs #6004000:   Batch Loss = 5.084332, Accuracy = 0.786000013351\n",
      "Performance on test set: Batch Loss = 5.14542484283, Accuracy = 0.772000014782\n",
      "Training epochs #6006000:   Batch Loss = 5.158643, Accuracy = 0.743000030518\n",
      "Performance on test set: Batch Loss = 5.07524681091, Accuracy = 0.771999955177\n",
      "Training epochs #6008000:   Batch Loss = 5.127208, Accuracy = 0.754000008106\n",
      "Performance on test set: Batch Loss = 5.09333705902, Accuracy = 0.770000040531\n",
      "Training epochs #6010000:   Batch Loss = 5.095663, Accuracy = 0.766000032425\n",
      "Performance on test set: Batch Loss = 5.03486776352, Accuracy = 0.78200006485\n",
      "Training epochs #6012000:   Batch Loss = 5.068427, Accuracy = 0.772999942303\n",
      "Performance on test set: Batch Loss = 5.09740877151, Accuracy = 0.758000016212\n",
      "Training epochs #6014000:   Batch Loss = 5.074785, Accuracy = 0.759999990463\n",
      "Performance on test set: Batch Loss = 5.0692152977, Accuracy = 0.758000016212\n",
      "Training epochs #6016000:   Batch Loss = 5.071650, Accuracy = 0.750999987125\n",
      "Performance on test set: Batch Loss = 4.99542093277, Accuracy = 0.792000055313\n",
      "Training epochs #6018000:   Batch Loss = 5.095637, Accuracy = 0.75100004673\n",
      "Performance on test set: Batch Loss = 4.99829769135, Accuracy = 0.789000034332\n",
      "Training epochs #6020000:   Batch Loss = 5.030948, Accuracy = 0.772999942303\n",
      "Performance on test set: Batch Loss = 5.04556274414, Accuracy = 0.752000033855\n",
      "Training epochs #6022000:   Batch Loss = 5.049548, Accuracy = 0.756000041962\n",
      "Performance on test set: Batch Loss = 5.03447341919, Accuracy = 0.76600009203\n",
      "Training epochs #6024000:   Batch Loss = 5.012411, Accuracy = 0.771999955177\n",
      "Performance on test set: Batch Loss = 5.04611206055, Accuracy = 0.773999929428\n",
      "Training epochs #6026000:   Batch Loss = 5.069333, Accuracy = 0.750000059605\n",
      "Performance on test set: Batch Loss = 4.99915599823, Accuracy = 0.802999973297\n",
      "Training epochs #6028000:   Batch Loss = 5.427282, Accuracy = 0.767999947071\n",
      "Performance on test set: Batch Loss = 5.13774967194, Accuracy = 0.7650000453\n",
      "Training epochs #6030000:   Batch Loss = 5.307185, Accuracy = 0.763000011444\n",
      "Performance on test set: Batch Loss = 5.26583576202, Accuracy = 0.669999957085\n",
      "Training epochs #6032000:   Batch Loss = 5.345504, Accuracy = 0.650999963284\n",
      "Performance on test set: Batch Loss = 5.35146808624, Accuracy = 0.747999966145\n",
      "Training epochs #6034000:   Batch Loss = 5.259523, Accuracy = 0.758000016212\n",
      "Performance on test set: Batch Loss = 5.3224363327, Accuracy = 0.748000025749\n",
      "Training epochs #6036000:   Batch Loss = 5.162662, Accuracy = 0.76700001955\n",
      "Performance on test set: Batch Loss = 5.10882091522, Accuracy = 0.778999984264\n",
      "Training epochs #6038000:   Batch Loss = 5.162471, Accuracy = 0.759000003338\n",
      "Performance on test set: Batch Loss = 5.08493280411, Accuracy = 0.768999993801\n",
      "Training epochs #6040000:   Batch Loss = 5.052694, Accuracy = 0.782999992371\n",
      "Performance on test set: Batch Loss = 5.17228889465, Accuracy = 0.744000017643\n",
      "Training epochs #6042000:   Batch Loss = 5.136821, Accuracy = 0.780000030994\n",
      "Performance on test set: Batch Loss = 5.13680458069, Accuracy = 0.766000032425\n",
      "Training epochs #6044000:   Batch Loss = 5.149342, Accuracy = 0.773999989033\n",
      "Performance on test set: Batch Loss = 5.15679836273, Accuracy = 0.770000100136\n",
      "Training epochs #6046000:   Batch Loss = 5.114892, Accuracy = 0.773000061512\n",
      "Performance on test set: Batch Loss = 5.09297323227, Accuracy = 0.774999976158\n",
      "Training epochs #6048000:   Batch Loss = 5.110293, Accuracy = 0.757000088692\n",
      "Performance on test set: Batch Loss = 5.1587138176, Accuracy = 0.754000008106\n",
      "Training epochs #6050000:   Batch Loss = 5.097289, Accuracy = 0.766000032425\n",
      "Performance on test set: Batch Loss = 5.06954860687, Accuracy = 0.782000005245\n",
      "Training epochs #6052000:   Batch Loss = 5.166645, Accuracy = 0.740999996662\n",
      "Performance on test set: Batch Loss = 5.11368274689, Accuracy = 0.757000029087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #6054000:   Batch Loss = 5.093656, Accuracy = 0.76599997282\n",
      "Performance on test set: Batch Loss = 5.12235450745, Accuracy = 0.758000016212\n",
      "Training epochs #6056000:   Batch Loss = 5.055477, Accuracy = 0.775000035763\n",
      "Performance on test set: Batch Loss = 5.01547193527, Accuracy = 0.783999979496\n",
      "Training epochs #6058000:   Batch Loss = 5.014437, Accuracy = 0.773999989033\n",
      "Performance on test set: Batch Loss = 5.02609539032, Accuracy = 0.768999993801\n",
      "Training epochs #6060000:   Batch Loss = 5.007033, Accuracy = 0.761000037193\n",
      "Performance on test set: Batch Loss = 5.06661176682, Accuracy = 0.744000017643\n",
      "Training epochs #6062000:   Batch Loss = 4.951725, Accuracy = 0.794999957085\n",
      "Performance on test set: Batch Loss = 5.04480552673, Accuracy = 0.766000032425\n",
      "Training epochs #6064000:   Batch Loss = 5.031845, Accuracy = 0.753999948502\n",
      "Performance on test set: Batch Loss = 5.05429172516, Accuracy = 0.770000040531\n",
      "Training epochs #6066000:   Batch Loss = 5.019749, Accuracy = 0.759000003338\n",
      "Performance on test set: Batch Loss = 4.99553108215, Accuracy = 0.76700001955\n",
      "Training epochs #6068000:   Batch Loss = 4.984343, Accuracy = 0.771000087261\n",
      "Performance on test set: Batch Loss = 5.00218629837, Accuracy = 0.754000067711\n",
      "Training epochs #6070000:   Batch Loss = 5.085162, Accuracy = 0.74899995327\n",
      "Performance on test set: Batch Loss = 4.9729218483, Accuracy = 0.78200006485\n",
      "Training epochs #6072000:   Batch Loss = 4.935037, Accuracy = 0.777999997139\n",
      "Performance on test set: Batch Loss = 5.02979040146, Accuracy = 0.75\n",
      "Training epochs #6074000:   Batch Loss = 4.925903, Accuracy = 0.773999989033\n",
      "Performance on test set: Batch Loss = 4.97685098648, Accuracy = 0.759000003338\n",
      "Training epochs #6076000:   Batch Loss = 5.011818, Accuracy = 0.766000032425\n",
      "Performance on test set: Batch Loss = 4.92858791351, Accuracy = 0.785000026226\n",
      "Training epochs #6078000:   Batch Loss = 4.928207, Accuracy = 0.772000074387\n",
      "Performance on test set: Batch Loss = 4.91495895386, Accuracy = 0.789000034332\n",
      "Training epochs #6080000:   Batch Loss = 4.937652, Accuracy = 0.793999969959\n",
      "Performance on test set: Batch Loss = 4.95400333405, Accuracy = 0.783999979496\n",
      "Training epochs #6082000:   Batch Loss = 4.889658, Accuracy = 0.782999992371\n",
      "Performance on test set: Batch Loss = 4.93127632141, Accuracy = 0.769000053406\n",
      "Training epochs #6084000:   Batch Loss = 4.920924, Accuracy = 0.78300011158\n",
      "Performance on test set: Batch Loss = 4.96872520447, Accuracy = 0.775000035763\n",
      "Training epochs #6086000:   Batch Loss = 4.990340, Accuracy = 0.740000069141\n",
      "Performance on test set: Batch Loss = 5.5881319046, Accuracy = 0.156000003219\n",
      "Training epochs #6088000:   Batch Loss = 5.666976, Accuracy = 0.15000000596\n",
      "Performance on test set: Batch Loss = 5.61235618591, Accuracy = 0.752000033855\n",
      "Training epochs #6090000:   Batch Loss = 5.610076, Accuracy = 0.761999964714\n",
      "Performance on test set: Batch Loss = 5.4361333847, Accuracy = 0.777000010014\n",
      "Training epochs #6092000:   Batch Loss = 5.383702, Accuracy = 0.757999956608\n",
      "Performance on test set: Batch Loss = 5.35316848755, Accuracy = 0.746999979019\n",
      "Training epochs #6094000:   Batch Loss = 5.188346, Accuracy = 0.746999979019\n",
      "Performance on test set: Batch Loss = 5.18314743042, Accuracy = 0.747999966145\n",
      "Training epochs #6096000:   Batch Loss = 5.524999, Accuracy = 0.737000048161\n",
      "Performance on test set: Batch Loss = 5.39324378967, Accuracy = 0.782999932766\n",
      "Training epochs #6098000:   Batch Loss = 5.238852, Accuracy = 0.739000082016\n",
      "Performance on test set: Batch Loss = 5.15032196045, Accuracy = 0.768000006676\n",
      "Training epochs #6100000:   Batch Loss = 5.255554, Accuracy = 0.762000024319\n",
      "Performance on test set: Batch Loss = 5.27949142456, Accuracy = 0.742000043392\n",
      "Training epochs #6102000:   Batch Loss = 5.250294, Accuracy = 0.745999991894\n",
      "Performance on test set: Batch Loss = 5.17367076874, Accuracy = 0.757999956608\n",
      "Training epochs #6104000:   Batch Loss = 5.106604, Accuracy = 0.757000088692\n",
      "Performance on test set: Batch Loss = 5.22349119186, Accuracy = 0.763000011444\n",
      "Training epochs #6106000:   Batch Loss = 5.205681, Accuracy = 0.740999937057\n",
      "Performance on test set: Batch Loss = 5.10820960999, Accuracy = 0.768999993801\n",
      "Training epochs #6108000:   Batch Loss = 5.262111, Accuracy = 0.738999962807\n",
      "Performance on test set: Batch Loss = 5.16119432449, Accuracy = 0.749000012875\n",
      "Training epochs #6110000:   Batch Loss = 5.085922, Accuracy = 0.76599997282\n",
      "Performance on test set: Batch Loss = 5.02741479874, Accuracy = 0.816000103951\n",
      "Training epochs #6112000:   Batch Loss = 5.048367, Accuracy = 0.802000045776\n",
      "Performance on test set: Batch Loss = 5.54455852509, Accuracy = 0.785000026226\n",
      "Training epochs #6114000:   Batch Loss = 5.221366, Accuracy = 0.76700001955\n",
      "Performance on test set: Batch Loss = 5.79771518707, Accuracy = 0.652999997139\n",
      "Training epochs #6116000:   Batch Loss = 6.048913, Accuracy = 0.655000030994\n",
      "Performance on test set: Batch Loss = 6.08204984665, Accuracy = 0.657999992371\n",
      "Training epochs #6118000:   Batch Loss = 6.058571, Accuracy = 0.662000000477\n",
      "Performance on test set: Batch Loss = 6.08749914169, Accuracy = 0.657999992371\n",
      "Training epochs #6120000:   Batch Loss = 5.981886, Accuracy = 0.675999999046\n",
      "Performance on test set: Batch Loss = 6.07968997955, Accuracy = 0.630999982357\n",
      "Training epochs #6122000:   Batch Loss = 5.923328, Accuracy = 0.658999979496\n",
      "Performance on test set: Batch Loss = 5.83804035187, Accuracy = 0.680000007153\n",
      "Training epochs #6124000:   Batch Loss = 5.791473, Accuracy = 0.681000053883\n",
      "Performance on test set: Batch Loss = 5.84635639191, Accuracy = 0.65700006485\n",
      "Training epochs #6126000:   Batch Loss = 6.035937, Accuracy = 0.68900001049\n",
      "Performance on test set: Batch Loss = 5.63736391068, Accuracy = 0.668999969959\n",
      "Training epochs #6128000:   Batch Loss = 5.662916, Accuracy = 0.636999964714\n",
      "Performance on test set: Batch Loss = 5.63247871399, Accuracy = 0.634000003338\n",
      "Training epochs #6130000:   Batch Loss = 5.599110, Accuracy = 0.638999998569\n",
      "Performance on test set: Batch Loss = 5.55110168457, Accuracy = 0.659000039101\n",
      "Training epochs #6132000:   Batch Loss = 5.595609, Accuracy = 0.651000022888\n",
      "Performance on test set: Batch Loss = 5.65185785294, Accuracy = 0.619000017643\n",
      "Training epochs #6134000:   Batch Loss = 5.637867, Accuracy = 0.632000029087\n",
      "Performance on test set: Batch Loss = 5.61893892288, Accuracy = 0.638000011444\n",
      "Training epochs #6136000:   Batch Loss = 5.457750, Accuracy = 0.659999966621\n",
      "Performance on test set: Batch Loss = 5.4738240242, Accuracy = 0.656999945641\n",
      "Training epochs #6138000:   Batch Loss = 5.446518, Accuracy = 0.666999936104\n",
      "Performance on test set: Batch Loss = 5.4665145874, Accuracy = 0.656999945641\n",
      "Training epochs #6140000:   Batch Loss = 5.528470, Accuracy = 0.64099997282\n",
      "Performance on test set: Batch Loss = 5.57265043259, Accuracy = 0.632000029087\n",
      "Training epochs #6142000:   Batch Loss = 5.410977, Accuracy = 0.682999968529\n",
      "Performance on test set: Batch Loss = 5.4775595665, Accuracy = 0.756999969482\n",
      "Training epochs #6144000:   Batch Loss = 5.530124, Accuracy = 0.746999979019\n",
      "Performance on test set: Batch Loss = 5.51557254791, Accuracy = 0.759000003338\n",
      "Training epochs #6146000:   Batch Loss = 5.499460, Accuracy = 0.752000033855\n",
      "Performance on test set: Batch Loss = 5.44468641281, Accuracy = 0.766000032425\n",
      "Training epochs #6148000:   Batch Loss = 5.444714, Accuracy = 0.7650000453\n",
      "Performance on test set: Batch Loss = 5.48815250397, Accuracy = 0.747000038624\n",
      "Training epochs #6150000:   Batch Loss = 5.549217, Accuracy = 0.736000061035\n",
      "Performance on test set: Batch Loss = 5.37654018402, Accuracy = 0.774000048637\n",
      "Training epochs #6152000:   Batch Loss = 5.368626, Accuracy = 0.759999990463\n",
      "Performance on test set: Batch Loss = 5.48241615295, Accuracy = 0.741999983788\n",
      "Training epochs #6154000:   Batch Loss = 5.352759, Accuracy = 0.768999934196\n",
      "Performance on test set: Batch Loss = 5.43832731247, Accuracy = 0.743000030518\n",
      "Training epochs #6156000:   Batch Loss = 5.424876, Accuracy = 0.751999974251\n",
      "Performance on test set: Batch Loss = 5.3435087204, Accuracy = 0.78100001812\n",
      "Training epochs #6158000:   Batch Loss = 5.345803, Accuracy = 0.7650000453\n",
      "Performance on test set: Batch Loss = 5.32531356812, Accuracy = 0.76700001955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #6160000:   Batch Loss = 5.366635, Accuracy = 0.75\n",
      "Performance on test set: Batch Loss = 5.3996181488, Accuracy = 0.738000035286\n",
      "Training epochs #6162000:   Batch Loss = 5.350624, Accuracy = 0.764000058174\n",
      "Performance on test set: Batch Loss = 5.33205604553, Accuracy = 0.755999982357\n",
      "Training epochs #6164000:   Batch Loss = 5.312064, Accuracy = 0.775000035763\n",
      "Performance on test set: Batch Loss = 5.34767150879, Accuracy = 0.760000050068\n",
      "Training epochs #6166000:   Batch Loss = 5.402242, Accuracy = 0.726999998093\n",
      "Performance on test set: Batch Loss = 5.28795623779, Accuracy = 0.766000032425\n",
      "Training epochs #6168000:   Batch Loss = 5.376288, Accuracy = 0.739999949932\n",
      "Performance on test set: Batch Loss = 5.3673620224, Accuracy = 0.747000038624\n",
      "Training epochs #6170000:   Batch Loss = 5.310534, Accuracy = 0.757000029087\n",
      "Performance on test set: Batch Loss = 5.25073719025, Accuracy = 0.773000001907\n",
      "Training epochs #6172000:   Batch Loss = 5.336252, Accuracy = 0.755999922752\n",
      "Performance on test set: Batch Loss = 5.36271381378, Accuracy = 0.742999970913\n",
      "Training epochs #6174000:   Batch Loss = 5.328258, Accuracy = 0.741999983788\n",
      "Performance on test set: Batch Loss = 5.33423614502, Accuracy = 0.743000030518\n",
      "Training epochs #6176000:   Batch Loss = 5.342829, Accuracy = 0.733999967575\n",
      "Performance on test set: Batch Loss = 5.24684667587, Accuracy = 0.777999997139\n",
      "Training epochs #6178000:   Batch Loss = 5.337066, Accuracy = 0.731999993324\n",
      "Performance on test set: Batch Loss = 5.23477363586, Accuracy = 0.766000032425\n",
      "Training epochs #6180000:   Batch Loss = 5.258577, Accuracy = 0.760999977589\n",
      "Performance on test set: Batch Loss = 5.31150960922, Accuracy = 0.736000061035\n",
      "Training epochs #6182000:   Batch Loss = 5.297678, Accuracy = 0.742999970913\n",
      "Performance on test set: Batch Loss = 5.24679803848, Accuracy = 0.757000029087\n",
      "Training epochs #6184000:   Batch Loss = 5.246535, Accuracy = 0.753000020981\n",
      "Performance on test set: Batch Loss = 5.25503015518, Accuracy = 0.760999917984\n",
      "Training epochs #6186000:   Batch Loss = 5.316908, Accuracy = 0.739000022411\n",
      "Performance on test set: Batch Loss = 5.19723463058, Accuracy = 0.763999998569\n",
      "Training epochs #6188000:   Batch Loss = 5.267580, Accuracy = 0.738999962807\n",
      "Performance on test set: Batch Loss = 5.28302574158, Accuracy = 0.746999979019\n",
      "Training epochs #6190000:   Batch Loss = 5.242425, Accuracy = 0.756000101566\n",
      "Performance on test set: Batch Loss = 5.16711616516, Accuracy = 0.773000001907\n",
      "Training epochs #6192000:   Batch Loss = 5.210017, Accuracy = 0.755000054836\n",
      "Performance on test set: Batch Loss = 5.28001737595, Accuracy = 0.744000017643\n",
      "Training epochs #6194000:   Batch Loss = 5.239077, Accuracy = 0.755000054836\n",
      "Performance on test set: Batch Loss = 5.24369716644, Accuracy = 0.744000017643\n",
      "Training epochs #6196000:   Batch Loss = 5.192432, Accuracy = 0.764999985695\n",
      "Performance on test set: Batch Loss = 5.17538022995, Accuracy = 0.778999984264\n",
      "Training epochs #6198000:   Batch Loss = 5.200659, Accuracy = 0.758000016212\n",
      "Performance on test set: Batch Loss = 5.16386127472, Accuracy = 0.768000006676\n",
      "Training epochs #6200000:   Batch Loss = 5.127479, Accuracy = 0.773999929428\n",
      "Performance on test set: Batch Loss = 5.24094247818, Accuracy = 0.736999988556\n",
      "Training epochs #6202000:   Batch Loss = 5.174797, Accuracy = 0.771000027657\n",
      "Performance on test set: Batch Loss = 5.18002653122, Accuracy = 0.757000029087\n",
      "Training epochs #6204000:   Batch Loss = 5.186508, Accuracy = 0.762000083923\n",
      "Performance on test set: Batch Loss = 5.18472766876, Accuracy = 0.761000037193\n",
      "Training epochs #6206000:   Batch Loss = 5.167813, Accuracy = 0.767000079155\n",
      "Performance on test set: Batch Loss = 5.12942504883, Accuracy = 0.763999998569\n",
      "Training epochs #6208000:   Batch Loss = 5.209675, Accuracy = 0.748000025749\n",
      "Performance on test set: Batch Loss = 5.2249712944, Accuracy = 0.75100004673\n",
      "Training epochs #6210000:   Batch Loss = 5.175272, Accuracy = 0.761999964714\n",
      "Performance on test set: Batch Loss = 5.10812950134, Accuracy = 0.777999997139\n",
      "Training epochs #6212000:   Batch Loss = 5.175646, Accuracy = 0.746999979019\n",
      "Performance on test set: Batch Loss = 5.21546554565, Accuracy = 0.75100004673\n",
      "Training epochs #6214000:   Batch Loss = 5.172519, Accuracy = 0.760999977589\n",
      "Performance on test set: Batch Loss = 5.17946767807, Accuracy = 0.753000020981\n",
      "Training epochs #6216000:   Batch Loss = 5.123014, Accuracy = 0.769999980927\n",
      "Performance on test set: Batch Loss = 5.11405563354, Accuracy = 0.782999992371\n",
      "Training epochs #6218000:   Batch Loss = 5.117876, Accuracy = 0.786000013351\n",
      "Performance on test set: Batch Loss = 5.10770606995, Accuracy = 0.773000001907\n",
      "Training epochs #6220000:   Batch Loss = 5.108611, Accuracy = 0.769999980927\n",
      "Performance on test set: Batch Loss = 5.18249702454, Accuracy = 0.740000009537\n",
      "Training epochs #6222000:   Batch Loss = 5.060300, Accuracy = 0.794999957085\n",
      "Performance on test set: Batch Loss = 5.12591552734, Accuracy = 0.764000058174\n",
      "Training epochs #6224000:   Batch Loss = 5.154866, Accuracy = 0.754000067711\n",
      "Performance on test set: Batch Loss = 5.13144445419, Accuracy = 0.768000006676\n",
      "Training epochs #6226000:   Batch Loss = 5.090946, Accuracy = 0.763999998569\n",
      "Performance on test set: Batch Loss = 5.07270765305, Accuracy = 0.772000014782\n",
      "Training epochs #6228000:   Batch Loss = 5.075937, Accuracy = 0.776000022888\n",
      "Performance on test set: Batch Loss = 5.16669845581, Accuracy = 0.753000020981\n",
      "Training epochs #6230000:   Batch Loss = 5.171587, Accuracy = 0.746999979019\n",
      "Performance on test set: Batch Loss = 5.05325317383, Accuracy = 0.779000043869\n",
      "Training epochs #6232000:   Batch Loss = 5.086871, Accuracy = 0.776000022888\n",
      "Performance on test set: Batch Loss = 5.15415382385, Accuracy = 0.75200009346\n",
      "Training epochs #6234000:   Batch Loss = 5.059194, Accuracy = 0.772999942303\n",
      "Performance on test set: Batch Loss = 5.11426019669, Accuracy = 0.753000020981\n",
      "Training epochs #6236000:   Batch Loss = 5.109165, Accuracy = 0.761000037193\n",
      "Performance on test set: Batch Loss = 5.05736875534, Accuracy = 0.783000051975\n",
      "Training epochs #6238000:   Batch Loss = 5.052311, Accuracy = 0.770999968052\n",
      "Performance on test set: Batch Loss = 5.04925823212, Accuracy = 0.773000061512\n",
      "Training epochs #6240000:   Batch Loss = 5.107518, Accuracy = 0.763000011444\n",
      "Performance on test set: Batch Loss = 5.11899185181, Accuracy = 0.741999983788\n",
      "Training epochs #6242000:   Batch Loss = 5.038898, Accuracy = 0.774000048637\n",
      "Performance on test set: Batch Loss = 5.06840896606, Accuracy = 0.763999998569\n",
      "Training epochs #6244000:   Batch Loss = 5.031405, Accuracy = 0.777999997139\n",
      "Performance on test set: Batch Loss = 5.07199907303, Accuracy = 0.768000006676\n",
      "Training epochs #6246000:   Batch Loss = 5.109476, Accuracy = 0.736000061035\n",
      "Performance on test set: Batch Loss = 5.00978755951, Accuracy = 0.772000014782\n",
      "Training epochs #6248000:   Batch Loss = 5.094784, Accuracy = 0.749000012875\n",
      "Performance on test set: Batch Loss = 5.10393190384, Accuracy = 0.753000080585\n",
      "Training epochs #6250000:   Batch Loss = 5.049858, Accuracy = 0.764000058174\n",
      "Performance on test set: Batch Loss = 4.99271869659, Accuracy = 0.779000043869\n",
      "Training epochs #6252000:   Batch Loss = 5.043158, Accuracy = 0.758999943733\n",
      "Performance on test set: Batch Loss = 5.08035421371, Accuracy = 0.751999974251\n",
      "Training epochs #6254000:   Batch Loss = 5.054039, Accuracy = 0.747000038624\n",
      "Performance on test set: Batch Loss = 5.03398132324, Accuracy = 0.752999961376\n",
      "Training epochs #6256000:   Batch Loss = 5.066477, Accuracy = 0.746999979019\n",
      "Performance on test set: Batch Loss = 4.98890829086, Accuracy = 0.783000051975\n",
      "Training epochs #6258000:   Batch Loss = 5.063801, Accuracy = 0.741999983788\n",
      "Performance on test set: Batch Loss = 4.98379278183, Accuracy = 0.773000001907\n",
      "Training epochs #6260000:   Batch Loss = 5.002420, Accuracy = 0.770000100136\n",
      "Performance on test set: Batch Loss = 5.04099702835, Accuracy = 0.742000043392\n",
      "Training epochs #6262000:   Batch Loss = 5.033669, Accuracy = 0.748000025749\n",
      "Performance on test set: Batch Loss = 4.99457025528, Accuracy = 0.765000104904\n",
      "Training epochs #6264000:   Batch Loss = 4.993277, Accuracy = 0.759000003338\n",
      "Performance on test set: Batch Loss = 5.00150442123, Accuracy = 0.768000006676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #6266000:   Batch Loss = 5.039619, Accuracy = 0.744999945164\n",
      "Performance on test set: Batch Loss = 4.93959140778, Accuracy = 0.771999955177\n",
      "Training epochs #6268000:   Batch Loss = 5.010798, Accuracy = 0.754999995232\n",
      "Performance on test set: Batch Loss = 5.03180027008, Accuracy = 0.753000080585\n",
      "Training epochs #6270000:   Batch Loss = 4.955862, Accuracy = 0.7650000453\n",
      "Performance on test set: Batch Loss = 4.92550325394, Accuracy = 0.779000043869\n",
      "Training epochs #6272000:   Batch Loss = 4.954653, Accuracy = 0.769000053406\n",
      "Performance on test set: Batch Loss = 5.00302028656, Accuracy = 0.752999961376\n",
      "Training epochs #6274000:   Batch Loss = 4.961077, Accuracy = 0.766000032425\n",
      "Performance on test set: Batch Loss = 4.9536242485, Accuracy = 0.754000067711\n",
      "Training epochs #6276000:   Batch Loss = 4.931481, Accuracy = 0.773000001907\n",
      "Performance on test set: Batch Loss = 4.91906404495, Accuracy = 0.782999992371\n",
      "Training epochs #6278000:   Batch Loss = 4.930819, Accuracy = 0.766000032425\n",
      "Performance on test set: Batch Loss = 4.91445636749, Accuracy = 0.773000001907\n",
      "Training epochs #6280000:   Batch Loss = 4.875574, Accuracy = 0.78100001812\n",
      "Performance on test set: Batch Loss = 4.96564722061, Accuracy = 0.742000043392\n",
      "Training epochs #6282000:   Batch Loss = 4.918548, Accuracy = 0.776000022888\n",
      "Performance on test set: Batch Loss = 4.92181682587, Accuracy = 0.764999985695\n",
      "Training epochs #6284000:   Batch Loss = 4.941227, Accuracy = 0.768000006676\n",
      "Performance on test set: Batch Loss = 4.93924474716, Accuracy = 0.768000006676\n",
      "Training epochs #6286000:   Batch Loss = 4.900891, Accuracy = 0.779000043869\n",
      "Performance on test set: Batch Loss = 4.87478256226, Accuracy = 0.784000039101\n",
      "Training epochs #6288000:   Batch Loss = 4.941884, Accuracy = 0.762999951839\n",
      "Performance on test set: Batch Loss = 4.95465564728, Accuracy = 0.786999940872\n",
      "Training epochs #6290000:   Batch Loss = 4.914697, Accuracy = 0.796000003815\n",
      "Performance on test set: Batch Loss = 4.86709165573, Accuracy = 0.820999979973\n",
      "Training epochs #6292000:   Batch Loss = 4.906193, Accuracy = 0.81500005722\n",
      "Performance on test set: Batch Loss = 4.9378285408, Accuracy = 0.804000020027\n",
      "Training epochs #6294000:   Batch Loss = 4.891705, Accuracy = 0.826000034809\n",
      "Performance on test set: Batch Loss = 4.88847160339, Accuracy = 0.832000076771\n",
      "Training epochs #6296000:   Batch Loss = 4.882719, Accuracy = 0.837000072002\n",
      "Performance on test set: Batch Loss = 4.86171770096, Accuracy = 0.834999978542\n",
      "Training epochs #6298000:   Batch Loss = 4.871425, Accuracy = 0.820999979973\n",
      "Performance on test set: Batch Loss = 4.85375404358, Accuracy = 0.833000004292\n",
      "Training epochs #6300000:   Batch Loss = 4.834017, Accuracy = 0.818999946117\n",
      "Performance on test set: Batch Loss = 4.89730978012, Accuracy = 0.806999981403\n",
      "Training epochs #6302000:   Batch Loss = 4.798527, Accuracy = 0.84399998188\n",
      "Performance on test set: Batch Loss = 4.85777425766, Accuracy = 0.826000034809\n",
      "Training epochs #6304000:   Batch Loss = 4.882300, Accuracy = 0.822000026703\n",
      "Performance on test set: Batch Loss = 4.88651323318, Accuracy = 0.826000094414\n",
      "Training epochs #6306000:   Batch Loss = 4.837595, Accuracy = 0.835000038147\n",
      "Performance on test set: Batch Loss = 4.81958055496, Accuracy = 0.819999992847\n",
      "Training epochs #6308000:   Batch Loss = 4.817312, Accuracy = 0.827000021935\n",
      "Performance on test set: Batch Loss = 4.88315296173, Accuracy = 0.809000015259\n",
      "Training epochs #6310000:   Batch Loss = 4.907092, Accuracy = 0.81400001049\n",
      "Performance on test set: Batch Loss = 4.80895280838, Accuracy = 0.830999970436\n",
      "Training epochs #6312000:   Batch Loss = 4.797175, Accuracy = 0.837999999523\n",
      "Performance on test set: Batch Loss = 4.87121391296, Accuracy = 0.801999986172\n",
      "Training epochs #6314000:   Batch Loss = 4.782275, Accuracy = 0.837999999523\n",
      "Performance on test set: Batch Loss = 4.82821178436, Accuracy = 0.830999970436\n",
      "Training epochs #6316000:   Batch Loss = 4.828300, Accuracy = 0.822000026703\n",
      "Performance on test set: Batch Loss = 4.79202365875, Accuracy = 0.836999952793\n",
      "Training epochs #6318000:   Batch Loss = 4.770777, Accuracy = 0.837000012398\n",
      "Performance on test set: Batch Loss = 4.78333997726, Accuracy = 0.847000002861\n",
      "Training epochs #6320000:   Batch Loss = 4.837205, Accuracy = 0.819999992847\n",
      "Performance on test set: Batch Loss = 4.82049369812, Accuracy = 0.820999979973\n",
      "Training epochs #6322000:   Batch Loss = 4.749042, Accuracy = 0.854000031948\n",
      "Performance on test set: Batch Loss = 4.79368925095, Accuracy = 0.838000059128\n",
      "Training epochs #6324000:   Batch Loss = 4.765615, Accuracy = 0.842999994755\n",
      "Performance on test set: Batch Loss = 4.81690883636, Accuracy = 0.834999978542\n",
      "Training epochs #6326000:   Batch Loss = 4.817891, Accuracy = 0.824000060558\n",
      "Performance on test set: Batch Loss = 4.75658035278, Accuracy = 0.82900005579\n",
      "Training epochs #6328000:   Batch Loss = 4.817235, Accuracy = 0.81400001049\n",
      "Performance on test set: Batch Loss = 4.81655168533, Accuracy = 0.816000044346\n",
      "Training epochs #6330000:   Batch Loss = 4.815639, Accuracy = 0.829999983311\n",
      "Performance on test set: Batch Loss = 4.75344419479, Accuracy = 0.844999969006\n",
      "Training epochs #6332000:   Batch Loss = 4.766527, Accuracy = 0.840999960899\n",
      "Performance on test set: Batch Loss = 4.8189330101, Accuracy = 0.807000041008\n",
      "Training epochs #6334000:   Batch Loss = 4.776302, Accuracy = 0.832000017166\n",
      "Performance on test set: Batch Loss = 4.78308391571, Accuracy = 0.837000012398\n",
      "Training epochs #6336000:   Batch Loss = 4.807601, Accuracy = 0.822000026703\n",
      "Performance on test set: Batch Loss = 4.75969696045, Accuracy = 0.839999973774\n",
      "Training epochs #6338000:   Batch Loss = 4.812147, Accuracy = 0.823000073433\n",
      "Performance on test set: Batch Loss = 4.73679351807, Accuracy = 0.848000049591\n",
      "Training epochs #6340000:   Batch Loss = 4.779961, Accuracy = 0.838999986649\n",
      "Performance on test set: Batch Loss = 4.78269338608, Accuracy = 0.82800000906\n",
      "Training epochs #6342000:   Batch Loss = 4.797262, Accuracy = 0.82800000906\n",
      "Performance on test set: Batch Loss = 4.77398014069, Accuracy = 0.839000046253\n",
      "Training epochs #6344000:   Batch Loss = 4.734430, Accuracy = 0.832000017166\n",
      "Performance on test set: Batch Loss = 4.78903961182, Accuracy = 0.836000025272\n",
      "Training epochs #6346000:   Batch Loss = 4.806569, Accuracy = 0.81299996376\n",
      "Performance on test set: Batch Loss = 4.7198805809, Accuracy = 0.830000042915\n",
      "Training epochs #6348000:   Batch Loss = 5.187941, Accuracy = 0.828000128269\n",
      "Performance on test set: Batch Loss = 6.19079780579, Accuracy = 0.0389999970794\n",
      "Training epochs #6350000:   Batch Loss = 6.746565, Accuracy = 0.0200000014156\n",
      "Performance on test set: Batch Loss = 6.97236251831, Accuracy = 0.0380000025034\n",
      "Training epochs #6352000:   Batch Loss = 6.885993, Accuracy = 0.643000006676\n",
      "Performance on test set: Batch Loss = 6.86711740494, Accuracy = 0.628000020981\n",
      "Training epochs #6354000:   Batch Loss = 6.609693, Accuracy = 0.648999989033\n",
      "Performance on test set: Batch Loss = 6.76423931122, Accuracy = 0.638999998569\n",
      "Training epochs #6356000:   Batch Loss = 6.853601, Accuracy = 0.652000010014\n",
      "Performance on test set: Batch Loss = 6.81132221222, Accuracy = 0.65600001812\n",
      "Training epochs #6358000:   Batch Loss = 6.569655, Accuracy = 0.661999940872\n",
      "Performance on test set: Batch Loss = 6.51749658585, Accuracy = 0.657999992371\n",
      "Training epochs #6360000:   Batch Loss = 6.362699, Accuracy = 0.675000011921\n",
      "Performance on test set: Batch Loss = 6.53853607178, Accuracy = 0.629000008106\n",
      "Training epochs #6362000:   Batch Loss = 6.526066, Accuracy = 0.657999992371\n",
      "Performance on test set: Batch Loss = 6.38532447815, Accuracy = 0.658999979496\n",
      "Training epochs #6364000:   Batch Loss = 6.439509, Accuracy = 0.653999984264\n",
      "Performance on test set: Batch Loss = 6.44692897797, Accuracy = 0.6400000453\n",
      "Training epochs #6366000:   Batch Loss = 6.402554, Accuracy = 0.646000027657\n",
      "Performance on test set: Batch Loss = 6.2058391571, Accuracy = 0.667000055313\n",
      "Training epochs #6368000:   Batch Loss = 6.313700, Accuracy = 0.630999922752\n",
      "Performance on test set: Batch Loss = 6.33714199066, Accuracy = 0.634000003338\n",
      "Training epochs #6370000:   Batch Loss = 6.283452, Accuracy = 0.638000011444\n",
      "Performance on test set: Batch Loss = 6.15941047668, Accuracy = 0.661000013351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #6372000:   Batch Loss = 6.094653, Accuracy = 0.652000010014\n",
      "Performance on test set: Batch Loss = 6.27447938919, Accuracy = 0.621000051498\n",
      "Training epochs #6374000:   Batch Loss = 6.209438, Accuracy = 0.632000029087\n",
      "Performance on test set: Batch Loss = 6.04749011993, Accuracy = 0.637000024319\n",
      "Training epochs #6376000:   Batch Loss = 5.881672, Accuracy = 0.659999966621\n",
      "Performance on test set: Batch Loss = 5.9716925621, Accuracy = 0.65600001812\n",
      "Training epochs #6378000:   Batch Loss = 5.958313, Accuracy = 0.662000060081\n",
      "Performance on test set: Batch Loss = 5.86596107483, Accuracy = 0.657999932766\n",
      "Training epochs #6380000:   Batch Loss = 5.900146, Accuracy = 0.6400000453\n",
      "Performance on test set: Batch Loss = 5.88189077377, Accuracy = 0.629000008106\n",
      "Training epochs #6382000:   Batch Loss = 5.721289, Accuracy = 0.679000020027\n",
      "Performance on test set: Batch Loss = 5.72367572784, Accuracy = 0.658999979496\n",
      "Training epochs #6384000:   Batch Loss = 5.818921, Accuracy = 0.633999943733\n",
      "Performance on test set: Batch Loss = 5.76853466034, Accuracy = 0.637999951839\n",
      "Training epochs #6386000:   Batch Loss = 5.686818, Accuracy = 0.648999989033\n",
      "Performance on test set: Batch Loss = 5.63876628876, Accuracy = 0.666999995708\n",
      "Training epochs #6388000:   Batch Loss = 5.636755, Accuracy = 0.661999940872\n",
      "Performance on test set: Batch Loss = 5.69041872025, Accuracy = 0.633999943733\n",
      "Training epochs #6390000:   Batch Loss = 5.668465, Accuracy = 0.629999995232\n",
      "Performance on test set: Batch Loss = 5.58670520782, Accuracy = 0.660999953747\n",
      "Training epochs #6392000:   Batch Loss = 5.574176, Accuracy = 0.635999977589\n",
      "Performance on test set: Batch Loss = 5.6729016304, Accuracy = 0.621000051498\n",
      "Training epochs #6394000:   Batch Loss = 5.516836, Accuracy = 0.669000029564\n",
      "Performance on test set: Batch Loss = 5.59551334381, Accuracy = 0.636000037193\n",
      "Training epochs #6396000:   Batch Loss = 5.603881, Accuracy = 0.618999958038\n",
      "Performance on test set: Batch Loss = 5.51685762405, Accuracy = 0.65600001812\n",
      "Training epochs #6398000:   Batch Loss = 5.516502, Accuracy = 0.664000034332\n",
      "Performance on test set: Batch Loss = 5.51737308502, Accuracy = 0.65700006485\n",
      "Training epochs #6400000:   Batch Loss = 5.539130, Accuracy = 0.644999980927\n",
      "Performance on test set: Batch Loss = 5.58613872528, Accuracy = 0.626999974251\n",
      "Training epochs #6402000:   Batch Loss = 5.517656, Accuracy = 0.657000005245\n",
      "Performance on test set: Batch Loss = 5.50305891037, Accuracy = 0.657000005245\n",
      "Training epochs #6404000:   Batch Loss = 5.526439, Accuracy = 0.648999989033\n",
      "Performance on test set: Batch Loss = 5.55179595947, Accuracy = 0.638000011444\n",
      "Training epochs #6406000:   Batch Loss = 5.618199, Accuracy = 0.617999970913\n",
      "Performance on test set: Batch Loss = 5.4956111908, Accuracy = 0.666000008583\n",
      "Training epochs #6408000:   Batch Loss = 5.594931, Accuracy = 0.615000009537\n",
      "Performance on test set: Batch Loss = 5.56507062912, Accuracy = 0.634000003338\n",
      "Training epochs #6410000:   Batch Loss = 5.515613, Accuracy = 0.648000001907\n",
      "Performance on test set: Batch Loss = 5.48582935333, Accuracy = 0.661000013351\n",
      "Training epochs #6412000:   Batch Loss = 5.509967, Accuracy = 0.636999964714\n",
      "Performance on test set: Batch Loss = 5.5829501152, Accuracy = 0.620999991894\n",
      "Training epochs #6414000:   Batch Loss = 5.552250, Accuracy = 0.631999969482\n",
      "Performance on test set: Batch Loss = 5.54416370392, Accuracy = 0.635999977589\n",
      "Training epochs #6416000:   Batch Loss = 5.532279, Accuracy = 0.632000029087\n",
      "Performance on test set: Batch Loss = 5.46210193634, Accuracy = 0.65600001812\n",
      "Training epochs #6418000:   Batch Loss = 5.557005, Accuracy = 0.625\n",
      "Performance on test set: Batch Loss = 5.46177959442, Accuracy = 0.657000005245\n",
      "Training epochs #6420000:   Batch Loss = 5.471231, Accuracy = 0.646000027657\n",
      "Performance on test set: Batch Loss = 5.53379297256, Accuracy = 0.626999974251\n",
      "Training epochs #6422000:   Batch Loss = 5.511950, Accuracy = 0.631999969482\n",
      "Performance on test set: Batch Loss = 5.45084667206, Accuracy = 0.656999945641\n",
      "Training epochs #6424000:   Batch Loss = 5.488551, Accuracy = 0.638999938965\n",
      "Performance on test set: Batch Loss = 5.50332546234, Accuracy = 0.637999951839\n",
      "Training epochs #6426000:   Batch Loss = 5.538907, Accuracy = 0.634000003338\n",
      "Performance on test set: Batch Loss = 5.43835496902, Accuracy = 0.666000008583\n",
      "Training epochs #6428000:   Batch Loss = 5.489709, Accuracy = 0.633000016212\n",
      "Performance on test set: Batch Loss = 5.51015758514, Accuracy = 0.634000003338\n",
      "Training epochs #6430000:   Batch Loss = 5.481208, Accuracy = 0.638000011444\n",
      "Performance on test set: Batch Loss = 5.4391040802, Accuracy = 0.661000013351\n",
      "Training epochs #6432000:   Batch Loss = 5.463166, Accuracy = 0.64200001955\n",
      "Performance on test set: Batch Loss = 5.54235649109, Accuracy = 0.621000051498\n",
      "Training epochs #6434000:   Batch Loss = 5.465760, Accuracy = 0.643999993801\n",
      "Performance on test set: Batch Loss = 5.50946998596, Accuracy = 0.636000037193\n",
      "Training epochs #6436000:   Batch Loss = 5.464590, Accuracy = 0.651000022888\n",
      "Performance on test set: Batch Loss = 5.43258857727, Accuracy = 0.65600001812\n",
      "Training epochs #6438000:   Batch Loss = 5.459837, Accuracy = 0.660000026226\n",
      "Performance on test set: Batch Loss = 5.43230438232, Accuracy = 0.657000005245\n",
      "Training epochs #6440000:   Batch Loss = 5.396506, Accuracy = 0.672000050545\n",
      "Performance on test set: Batch Loss = 5.50517272949, Accuracy = 0.627000033855\n",
      "Training epochs #6442000:   Batch Loss = 5.443779, Accuracy = 0.65399992466\n",
      "Performance on test set: Batch Loss = 5.42904376984, Accuracy = 0.656999945641\n",
      "Training epochs #6444000:   Batch Loss = 5.458063, Accuracy = 0.648000001907\n",
      "Performance on test set: Batch Loss = 5.47784519196, Accuracy = 0.638000011444\n",
      "Training epochs #6446000:   Batch Loss = 5.454410, Accuracy = 0.644999980927\n",
      "Performance on test set: Batch Loss = 5.41233539581, Accuracy = 0.666000008583\n",
      "Training epochs #6448000:   Batch Loss = 5.473835, Accuracy = 0.629999995232\n",
      "Performance on test set: Batch Loss = 5.48428630829, Accuracy = 0.633999943733\n",
      "Training epochs #6450000:   Batch Loss = 5.467687, Accuracy = 0.637000024319\n",
      "Performance on test set: Batch Loss = 5.41722536087, Accuracy = 0.661000072956\n",
      "Training epochs #6452000:   Batch Loss = 5.462413, Accuracy = 0.651999950409\n",
      "Performance on test set: Batch Loss = 5.51301670074, Accuracy = 0.621000051498\n",
      "Training epochs #6454000:   Batch Loss = 5.501882, Accuracy = 0.632000029087\n",
      "Performance on test set: Batch Loss = 5.48962974548, Accuracy = 0.636000037193\n",
      "Training epochs #6456000:   Batch Loss = 5.392223, Accuracy = 0.659999966621\n",
      "Performance on test set: Batch Loss = 5.41011238098, Accuracy = 0.65600001812\n",
      "Training epochs #6458000:   Batch Loss = 5.407551, Accuracy = 0.661000013351\n",
      "Performance on test set: Batch Loss = 5.41173171997, Accuracy = 0.657000005245\n",
      "Training epochs #6460000:   Batch Loss = 5.444998, Accuracy = 0.636999964714\n",
      "Performance on test set: Batch Loss = 5.4852514267, Accuracy = 0.627000033855\n",
      "Training epochs #6462000:   Batch Loss = 5.353649, Accuracy = 0.678999960423\n",
      "Performance on test set: Batch Loss = 5.41068410873, Accuracy = 0.657000005245\n",
      "Training epochs #6464000:   Batch Loss = 5.466974, Accuracy = 0.633999943733\n",
      "Performance on test set: Batch Loss = 5.45815134048, Accuracy = 0.638000011444\n",
      "Training epochs #6466000:   Batch Loss = 5.438102, Accuracy = 0.648999989033\n",
      "Performance on test set: Batch Loss = 5.39340257645, Accuracy = 0.666000008583\n",
      "Training epochs #6468000:   Batch Loss = 5.405065, Accuracy = 0.662000000477\n",
      "Performance on test set: Batch Loss = 5.46422386169, Accuracy = 0.633999943733\n",
      "Training epochs #6470000:   Batch Loss = 5.495291, Accuracy = 0.629999995232\n",
      "Performance on test set: Batch Loss = 5.39591217041, Accuracy = 0.661000013351\n",
      "Training epochs #6472000:   Batch Loss = 5.413002, Accuracy = 0.635999977589\n",
      "Performance on test set: Batch Loss = 5.49433946609, Accuracy = 0.620999932289\n",
      "Training epochs #6474000:   Batch Loss = 5.387552, Accuracy = 0.669000029564\n",
      "Performance on test set: Batch Loss = 5.47085762024, Accuracy = 0.636000037193\n",
      "Training epochs #6476000:   Batch Loss = 5.476246, Accuracy = 0.620000004768\n",
      "Performance on test set: Batch Loss = 5.39061260223, Accuracy = 0.65600001812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #6478000:   Batch Loss = 5.397042, Accuracy = 0.663999974728\n",
      "Performance on test set: Batch Loss = 5.39442968369, Accuracy = 0.656999945641\n",
      "Training epochs #6480000:   Batch Loss = 5.416501, Accuracy = 0.644999980927\n",
      "Performance on test set: Batch Loss = 5.46747684479, Accuracy = 0.627000033855\n",
      "Training epochs #6482000:   Batch Loss = 5.410598, Accuracy = 0.657000005245\n",
      "Performance on test set: Batch Loss = 5.39535140991, Accuracy = 0.657000005245\n",
      "Training epochs #6484000:   Batch Loss = 5.402384, Accuracy = 0.648999989033\n",
      "Performance on test set: Batch Loss = 5.43982934952, Accuracy = 0.638000011444\n",
      "Training epochs #6486000:   Batch Loss = 5.488625, Accuracy = 0.617999970913\n",
      "Performance on test set: Batch Loss = 5.37755680084, Accuracy = 0.666000008583\n",
      "Training epochs #6488000:   Batch Loss = 5.475036, Accuracy = 0.615000009537\n",
      "Performance on test set: Batch Loss = 5.4451417923, Accuracy = 0.633999943733\n",
      "Training epochs #6490000:   Batch Loss = 5.416153, Accuracy = 0.647999942303\n",
      "Performance on test set: Batch Loss = 5.38011217117, Accuracy = 0.661000013351\n",
      "Training epochs #6492000:   Batch Loss = 5.430085, Accuracy = 0.637000024319\n",
      "Performance on test set: Batch Loss = 5.47096157074, Accuracy = 0.620999991894\n",
      "Training epochs #6494000:   Batch Loss = 5.453194, Accuracy = 0.632000029087\n",
      "Performance on test set: Batch Loss = 5.45057010651, Accuracy = 0.636999964714\n",
      "Training epochs #6496000:   Batch Loss = 5.443471, Accuracy = 0.631999969482\n",
      "Performance on test set: Batch Loss = 5.36957216263, Accuracy = 0.655999958515\n",
      "Training epochs #6498000:   Batch Loss = 5.457529, Accuracy = 0.625\n",
      "Performance on test set: Batch Loss = 5.37647676468, Accuracy = 0.657000005245\n",
      "Training epochs #6500000:   Batch Loss = 5.396090, Accuracy = 0.646000027657\n",
      "Performance on test set: Batch Loss = 5.44699382782, Accuracy = 0.627000033855\n",
      "Training epochs #6502000:   Batch Loss = 5.431469, Accuracy = 0.631999969482\n",
      "Performance on test set: Batch Loss = 5.37764453888, Accuracy = 0.657000005245\n",
      "Training epochs #6504000:   Batch Loss = 5.399945, Accuracy = 0.638999938965\n",
      "Performance on test set: Batch Loss = 5.41980552673, Accuracy = 0.638000011444\n",
      "Training epochs #6506000:   Batch Loss = 5.449764, Accuracy = 0.634000062943\n",
      "Performance on test set: Batch Loss = 5.35780858994, Accuracy = 0.666000008583\n",
      "Training epochs #6508000:   Batch Loss = 5.417133, Accuracy = 0.633000016212\n",
      "Performance on test set: Batch Loss = 5.42403793335, Accuracy = 0.633999943733\n",
      "Training epochs #6510000:   Batch Loss = 5.410771, Accuracy = 0.637999951839\n",
      "Performance on test set: Batch Loss = 5.35699367523, Accuracy = 0.661000013351\n",
      "Training epochs #6512000:   Batch Loss = 5.386247, Accuracy = 0.64200001955\n",
      "Performance on test set: Batch Loss = 5.44666862488, Accuracy = 0.620999991894\n",
      "Training epochs #6514000:   Batch Loss = 5.381413, Accuracy = 0.643999993801\n",
      "Performance on test set: Batch Loss = 5.42838907242, Accuracy = 0.636000037193\n",
      "Training epochs #6516000:   Batch Loss = 5.377360, Accuracy = 0.651000022888\n",
      "Performance on test set: Batch Loss = 5.3423833847, Accuracy = 0.65600001812\n",
      "Training epochs #6518000:   Batch Loss = 5.380841, Accuracy = 0.660000026226\n",
      "Performance on test set: Batch Loss = 5.35150432587, Accuracy = 0.65700006485\n",
      "Training epochs #6520000:   Batch Loss = 5.313790, Accuracy = 0.672000050545\n",
      "Performance on test set: Batch Loss = 5.41744756699, Accuracy = 0.627000033855\n",
      "Training epochs #6522000:   Batch Loss = 5.356306, Accuracy = 0.653999984264\n",
      "Performance on test set: Batch Loss = 5.35130214691, Accuracy = 0.657000005245\n",
      "Training epochs #6524000:   Batch Loss = 5.370985, Accuracy = 0.648000001907\n",
      "Performance on test set: Batch Loss = 5.38788080215, Accuracy = 0.638000011444\n",
      "Training epochs #6526000:   Batch Loss = 5.358854, Accuracy = 0.644999980927\n",
      "Performance on test set: Batch Loss = 5.32309675217, Accuracy = 0.666000008583\n",
      "Training epochs #6528000:   Batch Loss = 5.381208, Accuracy = 0.629999995232\n",
      "Performance on test set: Batch Loss = 5.38632726669, Accuracy = 0.633999943733\n",
      "Training epochs #6530000:   Batch Loss = 5.366469, Accuracy = 0.637000024319\n",
      "Performance on test set: Batch Loss = 5.31482410431, Accuracy = 0.661000013351\n",
      "Training epochs #6532000:   Batch Loss = 5.363127, Accuracy = 0.651999950409\n",
      "Performance on test set: Batch Loss = 5.39981555939, Accuracy = 0.621000051498\n",
      "Training epochs #6534000:   Batch Loss = 5.384238, Accuracy = 0.631999969482\n",
      "Performance on test set: Batch Loss = 5.37509632111, Accuracy = 0.636000037193\n",
      "Training epochs #6536000:   Batch Loss = 5.283259, Accuracy = 0.659999966621\n",
      "Performance on test set: Batch Loss = 5.28199863434, Accuracy = 0.65600001812\n",
      "Training epochs #6538000:   Batch Loss = 5.279396, Accuracy = 0.661000013351\n",
      "Performance on test set: Batch Loss = 5.28265142441, Accuracy = 0.65700006485\n",
      "Training epochs #6540000:   Batch Loss = 5.308946, Accuracy = 0.636999964714\n",
      "Performance on test set: Batch Loss = 5.34303665161, Accuracy = 0.626999974251\n",
      "Training epochs #6542000:   Batch Loss = 5.209579, Accuracy = 0.678999960423\n",
      "Performance on test set: Batch Loss = 5.27281093597, Accuracy = 0.656999945641\n",
      "Training epochs #6544000:   Batch Loss = 5.309492, Accuracy = 0.633999943733\n",
      "Performance on test set: Batch Loss = 5.29550170898, Accuracy = 0.638000071049\n",
      "Training epochs #6546000:   Batch Loss = 5.273671, Accuracy = 0.648999989033\n",
      "Performance on test set: Batch Loss = 5.22507381439, Accuracy = 0.667000055313\n",
      "Training epochs #6548000:   Batch Loss = 5.215636, Accuracy = 0.662000000477\n",
      "Performance on test set: Batch Loss = 5.26811790466, Accuracy = 0.634999990463\n",
      "Training epochs #6550000:   Batch Loss = 5.302093, Accuracy = 0.629000008106\n",
      "Performance on test set: Batch Loss = 5.17484474182, Accuracy = 0.665000021458\n",
      "Training epochs #6552000:   Batch Loss = 5.193098, Accuracy = 0.762999951839\n",
      "Performance on test set: Batch Loss = 5.24053859711, Accuracy = 0.741999983788\n",
      "Training epochs #6554000:   Batch Loss = 5.146661, Accuracy = 0.670000016689\n",
      "Performance on test set: Batch Loss = 5.22139596939, Accuracy = 0.743000030518\n",
      "Training epochs #6556000:   Batch Loss = 5.187773, Accuracy = 0.754000067711\n",
      "Performance on test set: Batch Loss = 5.11394023895, Accuracy = 0.777999997139\n",
      "Training epochs #6558000:   Batch Loss = 5.108850, Accuracy = 0.763999998569\n",
      "Performance on test set: Batch Loss = 5.05726957321, Accuracy = 0.768999934196\n",
      "Training epochs #6560000:   Batch Loss = 5.149566, Accuracy = 0.750999927521\n",
      "Performance on test set: Batch Loss = 5.20445156097, Accuracy = 0.739000022411\n",
      "Training epochs #6562000:   Batch Loss = 5.215818, Accuracy = 0.763999998569\n",
      "Performance on test set: Batch Loss = 5.25505065918, Accuracy = 0.759000003338\n",
      "Training epochs #6564000:   Batch Loss = 5.190418, Accuracy = 0.776000022888\n",
      "Performance on test set: Batch Loss = 5.22807884216, Accuracy = 0.759999990463\n",
      "Training epochs #6566000:   Batch Loss = 5.244574, Accuracy = 0.726999998093\n",
      "Performance on test set: Batch Loss = 5.15758800507, Accuracy = 0.76700001955\n",
      "Training epochs #6568000:   Batch Loss = 5.198512, Accuracy = 0.740999996662\n",
      "Performance on test set: Batch Loss = 5.1606836319, Accuracy = 0.754000067711\n",
      "Training epochs #6570000:   Batch Loss = 5.373241, Accuracy = 0.769000053406\n",
      "Performance on test set: Batch Loss = 5.16130447388, Accuracy = 0.777000010014\n",
      "Training epochs #6572000:   Batch Loss = 5.499619, Accuracy = 0.638000011444\n",
      "Performance on test set: Batch Loss = 5.54065895081, Accuracy = 0.62600004673\n",
      "Training epochs #6574000:   Batch Loss = 5.522208, Accuracy = 0.636000037193\n",
      "Performance on test set: Batch Loss = 5.47566413879, Accuracy = 0.641000032425\n",
      "Training epochs #6576000:   Batch Loss = 5.442928, Accuracy = 0.633000016212\n",
      "Performance on test set: Batch Loss = 5.32082033157, Accuracy = 0.657999992371\n",
      "Training epochs #6578000:   Batch Loss = 5.401691, Accuracy = 0.634000003338\n",
      "Performance on test set: Batch Loss = 5.34310102463, Accuracy = 0.658999919891\n",
      "Training epochs #6580000:   Batch Loss = 5.382773, Accuracy = 0.652000010014\n",
      "Performance on test set: Batch Loss = 5.3999042511, Accuracy = 0.639999985695\n",
      "Training epochs #6582000:   Batch Loss = 5.384123, Accuracy = 0.647000014782\n",
      "Performance on test set: Batch Loss = 5.32803916931, Accuracy = 0.668999969959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #6584000:   Batch Loss = 5.302365, Accuracy = 0.652999997139\n",
      "Performance on test set: Batch Loss = 5.2922410965, Accuracy = 0.643000006676\n",
      "Training epochs #6586000:   Batch Loss = 5.296831, Accuracy = 0.6400000453\n",
      "Performance on test set: Batch Loss = 5.19850635529, Accuracy = 0.669999957085\n",
      "Training epochs #6588000:   Batch Loss = 5.276397, Accuracy = 0.634999990463\n",
      "Performance on test set: Batch Loss = 5.23316764832, Accuracy = 0.635999977589\n",
      "Training epochs #6590000:   Batch Loss = 5.214850, Accuracy = 0.641000032425\n",
      "Performance on test set: Batch Loss = 5.08418750763, Accuracy = 0.675999939442\n",
      "Training epochs #6592000:   Batch Loss = 5.137651, Accuracy = 0.664000034332\n",
      "Performance on test set: Batch Loss = 5.181224823, Accuracy = 0.755999982357\n",
      "Training epochs #6594000:   Batch Loss = 5.122275, Accuracy = 0.762000024319\n",
      "Performance on test set: Batch Loss = 5.16327524185, Accuracy = 0.74899995327\n",
      "Training epochs #6596000:   Batch Loss = 5.073149, Accuracy = 0.770000100136\n",
      "Performance on test set: Batch Loss = 5.03940010071, Accuracy = 0.786000072956\n",
      "Training epochs #6598000:   Batch Loss = 5.105133, Accuracy = 0.764999985695\n",
      "Performance on test set: Batch Loss = 5.06678295135, Accuracy = 0.776000022888\n",
      "Training epochs #6600000:   Batch Loss = 5.015093, Accuracy = 0.787000060081\n",
      "Performance on test set: Batch Loss = 5.09645080566, Accuracy = 0.751999974251\n",
      "Training epochs #6602000:   Batch Loss = 5.011078, Accuracy = 0.786000132561\n",
      "Performance on test set: Batch Loss = 5.06752157211, Accuracy = 0.776000022888\n",
      "Training epochs #6604000:   Batch Loss = 5.055252, Accuracy = 0.778999984264\n",
      "Performance on test set: Batch Loss = 5.05785083771, Accuracy = 0.773000001907\n",
      "Training epochs #6606000:   Batch Loss = 4.995211, Accuracy = 0.77999997139\n",
      "Performance on test set: Batch Loss = 4.99886512756, Accuracy = 0.770999968052\n",
      "Training epochs #6608000:   Batch Loss = 5.050343, Accuracy = 0.758000016212\n",
      "Performance on test set: Batch Loss = 5.05744123459, Accuracy = 0.756999969482\n",
      "Training epochs #6610000:   Batch Loss = 5.022167, Accuracy = 0.774999976158\n",
      "Performance on test set: Batch Loss = 4.94674301147, Accuracy = 0.791999995708\n",
      "Training epochs #6612000:   Batch Loss = 5.025669, Accuracy = 0.75100004673\n",
      "Performance on test set: Batch Loss = 5.01541137695, Accuracy = 0.766000032425\n",
      "Training epochs #6614000:   Batch Loss = 4.995908, Accuracy = 0.773999989033\n",
      "Performance on test set: Batch Loss = 5.01599836349, Accuracy = 0.763999938965\n",
      "Training epochs #6616000:   Batch Loss = 4.946497, Accuracy = 0.792000055313\n",
      "Performance on test set: Batch Loss = 4.92863988876, Accuracy = 0.794000029564\n",
      "Training epochs #6618000:   Batch Loss = 4.938485, Accuracy = 0.793999969959\n",
      "Performance on test set: Batch Loss = 4.94966125488, Accuracy = 0.78100001812\n",
      "Training epochs #6620000:   Batch Loss = 4.957182, Accuracy = 0.782000005245\n",
      "Performance on test set: Batch Loss = 5.02034568787, Accuracy = 0.754999995232\n",
      "Training epochs #6622000:   Batch Loss = 4.884088, Accuracy = 0.805000007153\n",
      "Performance on test set: Batch Loss = 4.96701097488, Accuracy = 0.774999976158\n",
      "Training epochs #6624000:   Batch Loss = 4.979024, Accuracy = 0.761999964714\n",
      "Performance on test set: Batch Loss = 4.9904499054, Accuracy = 0.773000001907\n",
      "Training epochs #6626000:   Batch Loss = 4.940683, Accuracy = 0.774999976158\n",
      "Performance on test set: Batch Loss = 4.90891647339, Accuracy = 0.78200006485\n",
      "Training epochs #6628000:   Batch Loss = 4.897607, Accuracy = 0.786000013351\n",
      "Performance on test set: Batch Loss = 4.95231628418, Accuracy = 0.766000032425\n",
      "Training epochs #6630000:   Batch Loss = 4.976359, Accuracy = 0.759999990463\n",
      "Performance on test set: Batch Loss = 4.87408924103, Accuracy = 0.796000003815\n",
      "Training epochs #6632000:   Batch Loss = 4.923605, Accuracy = 0.787999987602\n",
      "Performance on test set: Batch Loss = 4.93320417404, Accuracy = 0.767000079155\n",
      "Training epochs #6634000:   Batch Loss = 4.881670, Accuracy = 0.787999987602\n",
      "Performance on test set: Batch Loss = 4.95677089691, Accuracy = 0.763999998569\n",
      "Training epochs #6636000:   Batch Loss = 4.927348, Accuracy = 0.780000030994\n",
      "Performance on test set: Batch Loss = 4.85895204544, Accuracy = 0.794000029564\n",
      "Training epochs #6638000:   Batch Loss = 4.877000, Accuracy = 0.78100001812\n",
      "Performance on test set: Batch Loss = 4.89018678665, Accuracy = 0.78200006485\n",
      "Training epochs #6640000:   Batch Loss = 4.926585, Accuracy = 0.775000035763\n",
      "Performance on test set: Batch Loss = 4.94253063202, Accuracy = 0.755000054836\n",
      "Training epochs #6642000:   Batch Loss = 4.866490, Accuracy = 0.783000051975\n",
      "Performance on test set: Batch Loss = 4.89707517624, Accuracy = 0.775999963284\n",
      "Training epochs #6644000:   Batch Loss = 4.862086, Accuracy = 0.790000021458\n",
      "Performance on test set: Batch Loss = 4.9233212471, Accuracy = 0.779000043869\n",
      "Training epochs #6646000:   Batch Loss = 4.954835, Accuracy = 0.753000020981\n",
      "Performance on test set: Batch Loss = 5.15390825272, Accuracy = 0.787000000477\n",
      "Training epochs #6648000:   Batch Loss = 5.890975, Accuracy = 0.158000007272\n",
      "Performance on test set: Batch Loss = 5.76888561249, Accuracy = 0.143999993801\n",
      "Training epochs #6650000:   Batch Loss = 5.663421, Accuracy = 0.76599997282\n",
      "Performance on test set: Batch Loss = 5.52916622162, Accuracy = 0.774000048637\n",
      "Training epochs #6652000:   Batch Loss = 5.544602, Accuracy = 0.755999982357\n",
      "Performance on test set: Batch Loss = 5.50805664062, Accuracy = 0.624000012875\n",
      "Training epochs #6654000:   Batch Loss = 5.526197, Accuracy = 0.632999956608\n",
      "Performance on test set: Batch Loss = 5.54875516891, Accuracy = 0.635999977589\n",
      "Training epochs #6656000:   Batch Loss = 5.573604, Accuracy = 0.630999982357\n",
      "Performance on test set: Batch Loss = 5.435962677, Accuracy = 0.655999958515\n",
      "Training epochs #6658000:   Batch Loss = 5.597838, Accuracy = 0.625999987125\n",
      "Performance on test set: Batch Loss = 5.48904943466, Accuracy = 0.657999992371\n",
      "Training epochs #6660000:   Batch Loss = 5.518567, Accuracy = 0.646000027657\n",
      "Performance on test set: Batch Loss = 5.59774827957, Accuracy = 0.625\n",
      "Training epochs #6662000:   Batch Loss = 5.552855, Accuracy = 0.632000029087\n",
      "Performance on test set: Batch Loss = 5.45090007782, Accuracy = 0.657999992371\n",
      "Training epochs #6664000:   Batch Loss = 5.441048, Accuracy = 0.638999998569\n",
      "Performance on test set: Batch Loss = 5.42942714691, Accuracy = 0.637000024319\n",
      "Training epochs #6666000:   Batch Loss = 5.461725, Accuracy = 0.634000003338\n",
      "Performance on test set: Batch Loss = 5.33676338196, Accuracy = 0.666999995708\n",
      "Training epochs #6668000:   Batch Loss = 5.409993, Accuracy = 0.634000062943\n",
      "Performance on test set: Batch Loss = 5.41788101196, Accuracy = 0.634999990463\n",
      "Training epochs #6670000:   Batch Loss = 5.427746, Accuracy = 0.638000071049\n",
      "Performance on test set: Batch Loss = 5.36083602905, Accuracy = 0.660999953747\n",
      "Training epochs #6672000:   Batch Loss = 5.391576, Accuracy = 0.64200001955\n",
      "Performance on test set: Batch Loss = 5.44830036163, Accuracy = 0.623000025749\n",
      "Training epochs #6674000:   Batch Loss = 5.402407, Accuracy = 0.645999968052\n",
      "Performance on test set: Batch Loss = 5.4414563179, Accuracy = 0.636000037193\n",
      "Training epochs #6676000:   Batch Loss = 5.372192, Accuracy = 0.651000022888\n",
      "Performance on test set: Batch Loss = 5.33451557159, Accuracy = 0.65600001812\n",
      "Training epochs #6678000:   Batch Loss = 5.363975, Accuracy = 0.659999966621\n",
      "Performance on test set: Batch Loss = 5.32629871368, Accuracy = 0.658000051975\n",
      "Training epochs #6680000:   Batch Loss = 5.277027, Accuracy = 0.67199999094\n",
      "Performance on test set: Batch Loss = 5.39798212051, Accuracy = 0.626999974251\n",
      "Training epochs #6682000:   Batch Loss = 5.335024, Accuracy = 0.65499997139\n",
      "Performance on test set: Batch Loss = 5.32844257355, Accuracy = 0.657999992371\n",
      "Training epochs #6684000:   Batch Loss = 5.354577, Accuracy = 0.648000001907\n",
      "Performance on test set: Batch Loss = 5.35783910751, Accuracy = 0.637999951839\n",
      "Training epochs #6686000:   Batch Loss = 5.334765, Accuracy = 0.644999980927\n",
      "Performance on test set: Batch Loss = 5.29526424408, Accuracy = 0.666999995708\n",
      "Training epochs #6688000:   Batch Loss = 5.379125, Accuracy = 0.629999995232\n",
      "Performance on test set: Batch Loss = 5.37270259857, Accuracy = 0.634999930859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #6690000:   Batch Loss = 5.345402, Accuracy = 0.636999964714\n",
      "Performance on test set: Batch Loss = 5.29459190369, Accuracy = 0.661000013351\n",
      "Training epochs #6692000:   Batch Loss = 5.340658, Accuracy = 0.652999997139\n",
      "Performance on test set: Batch Loss = 5.378013134, Accuracy = 0.623000025749\n",
      "Training epochs #6694000:   Batch Loss = 5.364777, Accuracy = 0.632999956608\n",
      "Performance on test set: Batch Loss = 5.36924362183, Accuracy = 0.635999977589\n",
      "Training epochs #6696000:   Batch Loss = 5.288793, Accuracy = 0.659999966621\n",
      "Performance on test set: Batch Loss = 5.28256082535, Accuracy = 0.65600001812\n",
      "Training epochs #6698000:   Batch Loss = 5.283062, Accuracy = 0.661000013351\n",
      "Performance on test set: Batch Loss = 5.29624319077, Accuracy = 0.657999992371\n",
      "Training epochs #6700000:   Batch Loss = 5.322518, Accuracy = 0.637000024319\n",
      "Performance on test set: Batch Loss = 5.36173057556, Accuracy = 0.627000033855\n",
      "Training epochs #6702000:   Batch Loss = 5.241159, Accuracy = 0.678999960423\n",
      "Performance on test set: Batch Loss = 5.30014038086, Accuracy = 0.657999992371\n",
      "Training epochs #6704000:   Batch Loss = 5.334614, Accuracy = 0.634000003338\n",
      "Performance on test set: Batch Loss = 5.32923984528, Accuracy = 0.638000011444\n",
      "Training epochs #6706000:   Batch Loss = 5.320333, Accuracy = 0.650000035763\n",
      "Performance on test set: Batch Loss = 5.27066278458, Accuracy = 0.666999995708\n",
      "Training epochs #6708000:   Batch Loss = 5.283285, Accuracy = 0.662000060081\n",
      "Performance on test set: Batch Loss = 5.33933162689, Accuracy = 0.634999990463\n",
      "Training epochs #6710000:   Batch Loss = 5.377809, Accuracy = 0.629000008106\n",
      "Performance on test set: Batch Loss = 5.27019548416, Accuracy = 0.661000072956\n",
      "Training epochs #6712000:   Batch Loss = 5.290268, Accuracy = 0.635999977589\n",
      "Performance on test set: Batch Loss = 5.35909938812, Accuracy = 0.623000025749\n",
      "Training epochs #6714000:   Batch Loss = 5.265368, Accuracy = 0.668999969959\n",
      "Performance on test set: Batch Loss = 5.34553432465, Accuracy = 0.636000037193\n",
      "Training epochs #6716000:   Batch Loss = 5.347136, Accuracy = 0.620000004768\n",
      "Performance on test set: Batch Loss = 5.25603199005, Accuracy = 0.655999958515\n",
      "Training epochs #6718000:   Batch Loss = 5.272326, Accuracy = 0.663999974728\n",
      "Performance on test set: Batch Loss = 5.26645421982, Accuracy = 0.657999992371\n",
      "Training epochs #6720000:   Batch Loss = 5.294125, Accuracy = 0.644999980927\n",
      "Performance on test set: Batch Loss = 5.33423805237, Accuracy = 0.627000033855\n",
      "Training epochs #6722000:   Batch Loss = 5.277469, Accuracy = 0.657000005245\n",
      "Performance on test set: Batch Loss = 5.27233600616, Accuracy = 0.657999992371\n",
      "Training epochs #6724000:   Batch Loss = 5.258231, Accuracy = 0.648999989033\n",
      "Performance on test set: Batch Loss = 5.30157995224, Accuracy = 0.638000071049\n",
      "Training epochs #6726000:   Batch Loss = 5.346647, Accuracy = 0.618000030518\n",
      "Performance on test set: Batch Loss = 5.24005651474, Accuracy = 0.666999995708\n",
      "Training epochs #6728000:   Batch Loss = 5.325290, Accuracy = 0.615000009537\n",
      "Performance on test set: Batch Loss = 5.30059289932, Accuracy = 0.634999990463\n",
      "Training epochs #6730000:   Batch Loss = 5.284655, Accuracy = 0.648000001907\n",
      "Performance on test set: Batch Loss = 5.23470878601, Accuracy = 0.660999953747\n",
      "Training epochs #6732000:   Batch Loss = 5.291671, Accuracy = 0.636999964714\n",
      "Performance on test set: Batch Loss = 5.31591415405, Accuracy = 0.624000012875\n",
      "Training epochs #6734000:   Batch Loss = 5.303894, Accuracy = 0.633000075817\n",
      "Performance on test set: Batch Loss = 5.3092622757, Accuracy = 0.635999977589\n",
      "Training epochs #6736000:   Batch Loss = 5.298634, Accuracy = 0.631999969482\n",
      "Performance on test set: Batch Loss = 5.21058320999, Accuracy = 0.657000005245\n",
      "Training epochs #6738000:   Batch Loss = 5.306096, Accuracy = 0.62600004673\n",
      "Performance on test set: Batch Loss = 5.2181391716, Accuracy = 0.657999992371\n",
      "Training epochs #6740000:   Batch Loss = 5.245944, Accuracy = 0.645999968052\n",
      "Performance on test set: Batch Loss = 5.28263187408, Accuracy = 0.627000033855\n",
      "Training epochs #6742000:   Batch Loss = 5.273618, Accuracy = 0.632999956608\n",
      "Performance on test set: Batch Loss = 5.22189903259, Accuracy = 0.657999992371\n",
      "Training epochs #6744000:   Batch Loss = 5.224208, Accuracy = 0.638999938965\n",
      "Performance on test set: Batch Loss = 5.24717950821, Accuracy = 0.638000011444\n",
      "Training epochs #6746000:   Batch Loss = 5.286212, Accuracy = 0.634000062943\n",
      "Performance on test set: Batch Loss = 5.18114233017, Accuracy = 0.668000042439\n",
      "Training epochs #6748000:   Batch Loss = 5.248341, Accuracy = 0.634000003338\n",
      "Performance on test set: Batch Loss = 5.24109649658, Accuracy = 0.634999990463\n",
      "Training epochs #6750000:   Batch Loss = 5.235534, Accuracy = 0.638000011444\n",
      "Performance on test set: Batch Loss = 5.17802715302, Accuracy = 0.661000013351\n",
      "Training epochs #6752000:   Batch Loss = 5.204227, Accuracy = 0.643000006676\n",
      "Performance on test set: Batch Loss = 5.25006771088, Accuracy = 0.624000012875\n",
      "Training epochs #6754000:   Batch Loss = 5.201556, Accuracy = 0.645999968052\n",
      "Performance on test set: Batch Loss = 5.24598455429, Accuracy = 0.636999964714\n",
      "Training epochs #6756000:   Batch Loss = 5.181106, Accuracy = 0.651000022888\n",
      "Performance on test set: Batch Loss = 5.14733457565, Accuracy = 0.657000005245\n",
      "Training epochs #6758000:   Batch Loss = 5.190994, Accuracy = 0.660999953747\n",
      "Performance on test set: Batch Loss = 5.15658473969, Accuracy = 0.657999992371\n",
      "Training epochs #6760000:   Batch Loss = 5.110907, Accuracy = 0.674000024796\n",
      "Performance on test set: Batch Loss = 5.21705913544, Accuracy = 0.634000003338\n",
      "Training epochs #6762000:   Batch Loss = 5.153315, Accuracy = 0.657999992371\n",
      "Performance on test set: Batch Loss = 5.16318511963, Accuracy = 0.659999966621\n",
      "Training epochs #6764000:   Batch Loss = 5.173926, Accuracy = 0.654000043869\n",
      "Performance on test set: Batch Loss = 5.17925024033, Accuracy = 0.641000032425\n",
      "Training epochs #6766000:   Batch Loss = 5.140546, Accuracy = 0.651000022888\n",
      "Performance on test set: Batch Loss = 5.11260986328, Accuracy = 0.676999986172\n",
      "Training epochs #6768000:   Batch Loss = 5.186231, Accuracy = 0.741000056267\n",
      "Performance on test set: Batch Loss = 5.17739009857, Accuracy = 0.75\n",
      "Training epochs #6770000:   Batch Loss = 5.150640, Accuracy = 0.759999990463\n",
      "Performance on test set: Batch Loss = 5.10768175125, Accuracy = 0.774999976158\n",
      "Training epochs #6772000:   Batch Loss = 5.160214, Accuracy = 0.740999996662\n",
      "Performance on test set: Batch Loss = 5.17321205139, Accuracy = 0.744000017643\n",
      "Training epochs #6774000:   Batch Loss = 5.152167, Accuracy = 0.754000008106\n",
      "Performance on test set: Batch Loss = 5.17163228989, Accuracy = 0.744000077248\n",
      "Training epochs #6776000:   Batch Loss = 5.096116, Accuracy = 0.763000011444\n",
      "Performance on test set: Batch Loss = 5.07467889786, Accuracy = 0.78100001812\n",
      "Training epochs #6778000:   Batch Loss = 5.069010, Accuracy = 0.773999989033\n",
      "Performance on test set: Batch Loss = 5.08314800262, Accuracy = 0.76800006628\n",
      "Training epochs #6780000:   Batch Loss = 5.088308, Accuracy = 0.760000050068\n",
      "Performance on test set: Batch Loss = 5.14243888855, Accuracy = 0.740000009537\n",
      "Training epochs #6782000:   Batch Loss = 5.025052, Accuracy = 0.787999987602\n",
      "Performance on test set: Batch Loss = 5.09028673172, Accuracy = 0.757999956608\n",
      "Training epochs #6784000:   Batch Loss = 5.109179, Accuracy = 0.746000051498\n",
      "Performance on test set: Batch Loss = 5.10707569122, Accuracy = 0.761000037193\n",
      "Training epochs #6786000:   Batch Loss = 5.103808, Accuracy = 0.755999922752\n",
      "Performance on test set: Batch Loss = 5.03830814362, Accuracy = 0.768000006676\n",
      "Training epochs #6788000:   Batch Loss = 5.049561, Accuracy = 0.766000032425\n",
      "Performance on test set: Batch Loss = 5.09742021561, Accuracy = 0.75100004673\n",
      "Training epochs #6790000:   Batch Loss = 5.150548, Accuracy = 0.735000014305\n",
      "Performance on test set: Batch Loss = 5.03196430206, Accuracy = 0.773999929428\n",
      "Training epochs #6792000:   Batch Loss = 5.037343, Accuracy = 0.761999964714\n",
      "Performance on test set: Batch Loss = 5.10042619705, Accuracy = 0.745999991894\n",
      "Training epochs #6794000:   Batch Loss = 5.017836, Accuracy = 0.769000053406\n",
      "Performance on test set: Batch Loss = 5.09424543381, Accuracy = 0.743999958038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #6796000:   Batch Loss = 5.087595, Accuracy = 0.753000020981\n",
      "Performance on test set: Batch Loss = 5.00324630737, Accuracy = 0.778000056744\n",
      "Training epochs #6798000:   Batch Loss = 5.013964, Accuracy = 0.764000058174\n",
      "Performance on test set: Batch Loss = 5.00579595566, Accuracy = 0.76700001955\n",
      "Training epochs #6800000:   Batch Loss = 5.040160, Accuracy = 0.75\n",
      "Performance on test set: Batch Loss = 5.06791162491, Accuracy = 0.738000035286\n",
      "Training epochs #6802000:   Batch Loss = 5.008354, Accuracy = 0.764999985695\n",
      "Performance on test set: Batch Loss = 5.01836013794, Accuracy = 0.758000016212\n",
      "Training epochs #6804000:   Batch Loss = 4.982333, Accuracy = 0.775000035763\n",
      "Performance on test set: Batch Loss = 5.0385594368, Accuracy = 0.761000037193\n",
      "Training epochs #6806000:   Batch Loss = 5.071592, Accuracy = 0.726000010967\n",
      "Performance on test set: Batch Loss = 4.96887493134, Accuracy = 0.766000032425\n",
      "Training epochs #6808000:   Batch Loss = 5.045821, Accuracy = 0.739999949932\n",
      "Performance on test set: Batch Loss = 5.02511358261, Accuracy = 0.748000025749\n",
      "Training epochs #6810000:   Batch Loss = 5.025908, Accuracy = 0.755999982357\n",
      "Performance on test set: Batch Loss = 4.96413326263, Accuracy = 0.773000061512\n",
      "Training epochs #6812000:   Batch Loss = 5.015679, Accuracy = 0.754999995232\n",
      "Performance on test set: Batch Loss = 5.03333854675, Accuracy = 0.743999958038\n",
      "Training epochs #6814000:   Batch Loss = 5.017315, Accuracy = 0.743000030518\n",
      "Performance on test set: Batch Loss = 5.0223107338, Accuracy = 0.745000004768\n",
      "Training epochs #6816000:   Batch Loss = 5.023565, Accuracy = 0.736999988556\n",
      "Performance on test set: Batch Loss = 4.94077587128, Accuracy = 0.779000043869\n",
      "Training epochs #6818000:   Batch Loss = 5.021197, Accuracy = 0.735000014305\n",
      "Performance on test set: Batch Loss = 4.93611907959, Accuracy = 0.768999993801\n",
      "Training epochs #6820000:   Batch Loss = 4.972515, Accuracy = 0.761000037193\n",
      "Performance on test set: Batch Loss = 4.99957418442, Accuracy = 0.739000022411\n",
      "Training epochs #6822000:   Batch Loss = 4.999563, Accuracy = 0.743999958038\n",
      "Performance on test set: Batch Loss = 4.95305585861, Accuracy = 0.758999943733\n",
      "Training epochs #6824000:   Batch Loss = 4.935142, Accuracy = 0.752999961376\n",
      "Performance on test set: Batch Loss = 4.97786951065, Accuracy = 0.761999964714\n",
      "Training epochs #6826000:   Batch Loss = 5.013978, Accuracy = 0.740999996662\n",
      "Performance on test set: Batch Loss = 4.90977954865, Accuracy = 0.765999913216\n",
      "Training epochs #6828000:   Batch Loss = 4.974233, Accuracy = 0.740000009537\n",
      "Performance on test set: Batch Loss = 4.96418142319, Accuracy = 0.750000059605\n",
      "Training epochs #6830000:   Batch Loss = 4.946968, Accuracy = 0.756000101566\n",
      "Performance on test set: Batch Loss = 4.90560436249, Accuracy = 0.774999976158\n",
      "Training epochs #6832000:   Batch Loss = 4.933191, Accuracy = 0.755999982357\n",
      "Performance on test set: Batch Loss = 4.97763252258, Accuracy = 0.746999979019\n",
      "Training epochs #6834000:   Batch Loss = 4.932745, Accuracy = 0.759000062943\n",
      "Performance on test set: Batch Loss = 4.95835018158, Accuracy = 0.745000004768\n",
      "Training epochs #6836000:   Batch Loss = 4.909033, Accuracy = 0.76599997282\n",
      "Performance on test set: Batch Loss = 4.8847951889, Accuracy = 0.778999984264\n",
      "Training epochs #6838000:   Batch Loss = 4.915747, Accuracy = 0.758999943733\n",
      "Performance on test set: Batch Loss = 4.87596225739, Accuracy = 0.769000053406\n",
      "Training epochs #6840000:   Batch Loss = 4.838821, Accuracy = 0.775000035763\n",
      "Performance on test set: Batch Loss = 4.94189596176, Accuracy = 0.739000022411\n",
      "Training epochs #6842000:   Batch Loss = 4.892426, Accuracy = 0.772000014782\n",
      "Performance on test set: Batch Loss = 4.89725446701, Accuracy = 0.758999943733\n",
      "Training epochs #6844000:   Batch Loss = 4.917207, Accuracy = 0.76700001955\n",
      "Performance on test set: Batch Loss = 4.92205619812, Accuracy = 0.762000083923\n",
      "Training epochs #6846000:   Batch Loss = 4.878439, Accuracy = 0.770000040531\n",
      "Performance on test set: Batch Loss = 4.85537528992, Accuracy = 0.774000048637\n",
      "Training epochs #6848000:   Batch Loss = 4.912911, Accuracy = 0.750999987125\n",
      "Performance on test set: Batch Loss = 4.91113138199, Accuracy = 0.753999948502\n",
      "Training epochs #6850000:   Batch Loss = 4.894738, Accuracy = 0.764000058174\n",
      "Performance on test set: Batch Loss = 4.85085439682, Accuracy = 0.77899992466\n",
      "Training epochs #6852000:   Batch Loss = 4.896678, Accuracy = 0.748000025749\n",
      "Performance on test set: Batch Loss = 4.9183921814, Accuracy = 0.753999948502\n",
      "Training epochs #6854000:   Batch Loss = 4.884049, Accuracy = 0.761999964714\n",
      "Performance on test set: Batch Loss = 4.89164352417, Accuracy = 0.754000008106\n",
      "Training epochs #6856000:   Batch Loss = 4.846344, Accuracy = 0.769999980927\n",
      "Performance on test set: Batch Loss = 4.83172750473, Accuracy = 0.782999992371\n",
      "Training epochs #6858000:   Batch Loss = 4.825135, Accuracy = 0.787000000477\n",
      "Performance on test set: Batch Loss = 4.8166103363, Accuracy = 0.773999989033\n",
      "Training epochs #6860000:   Batch Loss = 4.821029, Accuracy = 0.773000001907\n",
      "Performance on test set: Batch Loss = 4.88132095337, Accuracy = 0.744000017643\n",
      "Training epochs #6862000:   Batch Loss = 4.770036, Accuracy = 0.794999957085\n",
      "Performance on test set: Batch Loss = 4.83959579468, Accuracy = 0.766999959946\n",
      "Training epochs #6864000:   Batch Loss = 4.849292, Accuracy = 0.754000067711\n",
      "Performance on test set: Batch Loss = 4.86801671982, Accuracy = 0.770000040531\n",
      "Training epochs #6866000:   Batch Loss = 4.833088, Accuracy = 0.766999959946\n",
      "Performance on test set: Batch Loss = 4.7986164093, Accuracy = 0.774999976158\n",
      "Training epochs #6868000:   Batch Loss = 4.794703, Accuracy = 0.777999937534\n",
      "Performance on test set: Batch Loss = 4.85190963745, Accuracy = 0.755000114441\n",
      "Training epochs #6870000:   Batch Loss = 4.890896, Accuracy = 0.748000025749\n",
      "Performance on test set: Batch Loss = 4.78915119171, Accuracy = 0.78100001812\n",
      "Training epochs #6872000:   Batch Loss = 4.773852, Accuracy = 0.780000090599\n",
      "Performance on test set: Batch Loss = 4.85557413101, Accuracy = 0.759000003338\n",
      "Training epochs #6874000:   Batch Loss = 4.760625, Accuracy = 0.782999992371\n",
      "Performance on test set: Batch Loss = 4.8187789917, Accuracy = 0.768000006676\n",
      "Training epochs #6876000:   Batch Loss = 4.835349, Accuracy = 0.774999976158\n",
      "Performance on test set: Batch Loss = 4.7725148201, Accuracy = 0.793000042439\n",
      "Training epochs #6878000:   Batch Loss = 4.762697, Accuracy = 0.783999979496\n",
      "Performance on test set: Batch Loss = 4.74973201752, Accuracy = 0.778999984264\n",
      "Training epochs #6880000:   Batch Loss = 4.800417, Accuracy = 0.767000079155\n",
      "Performance on test set: Batch Loss = 4.81283855438, Accuracy = 0.747000038624\n",
      "Training epochs #6882000:   Batch Loss = 4.734757, Accuracy = 0.780000090599\n",
      "Performance on test set: Batch Loss = 4.77408981323, Accuracy = 0.779000043869\n",
      "Training epochs #6884000:   Batch Loss = 4.749430, Accuracy = 0.791000068188\n",
      "Performance on test set: Batch Loss = 4.80319976807, Accuracy = 0.796000063419\n",
      "Training epochs #6886000:   Batch Loss = 4.803824, Accuracy = 0.776000022888\n",
      "Performance on test set: Batch Loss = 4.73190736771, Accuracy = 0.810000061989\n",
      "Training epochs #6888000:   Batch Loss = 4.800918, Accuracy = 0.814999997616\n",
      "Performance on test set: Batch Loss = 4.78054523468, Accuracy = 0.819999992847\n",
      "Training epochs #6890000:   Batch Loss = 4.781566, Accuracy = 0.832000017166\n",
      "Performance on test set: Batch Loss = 4.72403621674, Accuracy = 0.842000007629\n",
      "Training epochs #6892000:   Batch Loss = 4.739396, Accuracy = 0.828999996185\n",
      "Performance on test set: Batch Loss = 4.77854442596, Accuracy = 0.804000020027\n",
      "Training epochs #6894000:   Batch Loss = 4.741406, Accuracy = 0.817000031471\n",
      "Performance on test set: Batch Loss = 4.74100923538, Accuracy = 0.824000000954\n",
      "Training epochs #6896000:   Batch Loss = 4.755493, Accuracy = 0.80999994278\n",
      "Performance on test set: Batch Loss = 5.11435317993, Accuracy = 0.834999918938\n",
      "Training epochs #6898000:   Batch Loss = 6.372416, Accuracy = 0.0479999966919\n",
      "Performance on test set: Batch Loss = 6.86125516891, Accuracy = 0.0469999983907\n",
      "Training epochs #6900000:   Batch Loss = 6.965148, Accuracy = 0.0419999994338\n",
      "Performance on test set: Batch Loss = 6.76270675659, Accuracy = 0.0509999990463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #6902000:   Batch Loss = 6.439547, Accuracy = 0.0520000010729\n",
      "Performance on test set: Batch Loss = 6.09399032593, Accuracy = 0.679000020027\n",
      "Training epochs #6904000:   Batch Loss = 5.953555, Accuracy = 0.64099997282\n",
      "Performance on test set: Batch Loss = 5.88336181641, Accuracy = 0.638000011444\n",
      "Training epochs #6906000:   Batch Loss = 5.848443, Accuracy = 0.635999977589\n",
      "Performance on test set: Batch Loss = 5.72143554688, Accuracy = 0.667000055313\n",
      "Training epochs #6908000:   Batch Loss = 5.866129, Accuracy = 0.634000003338\n",
      "Performance on test set: Batch Loss = 5.88550662994, Accuracy = 0.634000003338\n",
      "Training epochs #6910000:   Batch Loss = 5.822239, Accuracy = 0.637999951839\n",
      "Performance on test set: Batch Loss = 5.73024463654, Accuracy = 0.661000013351\n",
      "Training epochs #6912000:   Batch Loss = 5.753389, Accuracy = 0.64200001955\n",
      "Performance on test set: Batch Loss = 5.80125665665, Accuracy = 0.621000051498\n",
      "Training epochs #6914000:   Batch Loss = 5.612668, Accuracy = 0.644999980927\n",
      "Performance on test set: Batch Loss = 5.59918260574, Accuracy = 0.637000024319\n",
      "Training epochs #6916000:   Batch Loss = 5.513273, Accuracy = 0.652000010014\n",
      "Performance on test set: Batch Loss = 5.42741775513, Accuracy = 0.656000077724\n",
      "Training epochs #6918000:   Batch Loss = 5.391983, Accuracy = 0.661000013351\n",
      "Performance on test set: Batch Loss = 5.35346317291, Accuracy = 0.657000005245\n",
      "Training epochs #6920000:   Batch Loss = 5.291675, Accuracy = 0.67199999094\n",
      "Performance on test set: Batch Loss = 5.38448572159, Accuracy = 0.629000008106\n",
      "Training epochs #6922000:   Batch Loss = 5.338881, Accuracy = 0.657000005245\n",
      "Performance on test set: Batch Loss = 5.33045339584, Accuracy = 0.658999979496\n",
      "Training epochs #6924000:   Batch Loss = 5.365479, Accuracy = 0.652999997139\n",
      "Performance on test set: Batch Loss = 5.37538719177, Accuracy = 0.6400000453\n",
      "Training epochs #6926000:   Batch Loss = 5.357403, Accuracy = 0.646000027657\n",
      "Performance on test set: Batch Loss = 5.32800197601, Accuracy = 0.669000029564\n",
      "Training epochs #6928000:   Batch Loss = 5.351833, Accuracy = 0.634999990463\n",
      "Performance on test set: Batch Loss = 5.36091709137, Accuracy = 0.634000003338\n",
      "Training epochs #6930000:   Batch Loss = 5.333071, Accuracy = 0.638999998569\n",
      "Performance on test set: Batch Loss = 5.28343772888, Accuracy = 0.661000013351\n",
      "Training epochs #6932000:   Batch Loss = 5.324904, Accuracy = 0.652000010014\n",
      "Performance on test set: Batch Loss = 5.3546090126, Accuracy = 0.621000051498\n",
      "Training epochs #6934000:   Batch Loss = 5.343639, Accuracy = 0.631999969482\n",
      "Performance on test set: Batch Loss = 5.33520698547, Accuracy = 0.636000037193\n",
      "Training epochs #6936000:   Batch Loss = 5.246784, Accuracy = 0.659999966621\n",
      "Performance on test set: Batch Loss = 5.24604320526, Accuracy = 0.65600001812\n",
      "Training epochs #6938000:   Batch Loss = 5.247847, Accuracy = 0.661000013351\n",
      "Performance on test set: Batch Loss = 5.26946115494, Accuracy = 0.657000005245\n",
      "Training epochs #6940000:   Batch Loss = 5.304591, Accuracy = 0.637000024319\n",
      "Performance on test set: Batch Loss = 5.35503339767, Accuracy = 0.626999974251\n",
      "Training epochs #6942000:   Batch Loss = 5.204064, Accuracy = 0.678999960423\n",
      "Performance on test set: Batch Loss = 5.27752590179, Accuracy = 0.657000005245\n",
      "Training epochs #6944000:   Batch Loss = 5.323660, Accuracy = 0.634000003338\n",
      "Performance on test set: Batch Loss = 5.31227970123, Accuracy = 0.638000011444\n",
      "Training epochs #6946000:   Batch Loss = 5.297974, Accuracy = 0.648999989033\n",
      "Performance on test set: Batch Loss = 5.24381542206, Accuracy = 0.666000008583\n",
      "Training epochs #6948000:   Batch Loss = 5.250999, Accuracy = 0.661999940872\n",
      "Performance on test set: Batch Loss = 5.31352472305, Accuracy = 0.633999943733\n",
      "Training epochs #6950000:   Batch Loss = 5.347187, Accuracy = 0.629999995232\n",
      "Performance on test set: Batch Loss = 5.23567390442, Accuracy = 0.661000013351\n",
      "Training epochs #6952000:   Batch Loss = 5.252374, Accuracy = 0.635999977589\n",
      "Performance on test set: Batch Loss = 5.33242368698, Accuracy = 0.621000051498\n",
      "Training epochs #6954000:   Batch Loss = 5.238983, Accuracy = 0.668999969959\n",
      "Performance on test set: Batch Loss = 5.31398248672, Accuracy = 0.636000037193\n",
      "Training epochs #6956000:   Batch Loss = 5.313882, Accuracy = 0.620000004768\n",
      "Performance on test set: Batch Loss = 5.23205375671, Accuracy = 0.65600001812\n",
      "Training epochs #6958000:   Batch Loss = 5.246067, Accuracy = 0.663999974728\n",
      "Performance on test set: Batch Loss = 5.24370861053, Accuracy = 0.657000005245\n",
      "Training epochs #6960000:   Batch Loss = 5.264778, Accuracy = 0.644999980927\n",
      "Performance on test set: Batch Loss = 5.31068706512, Accuracy = 0.627000033855\n",
      "Training epochs #6962000:   Batch Loss = 5.250199, Accuracy = 0.65700006485\n",
      "Performance on test set: Batch Loss = 5.24530363083, Accuracy = 0.657999992371\n",
      "Training epochs #6964000:   Batch Loss = 5.241318, Accuracy = 0.648999989033\n",
      "Performance on test set: Batch Loss = 5.28082084656, Accuracy = 0.638000011444\n",
      "Training epochs #6966000:   Batch Loss = 5.327565, Accuracy = 0.617999970913\n",
      "Performance on test set: Batch Loss = 5.21971702576, Accuracy = 0.666999995708\n",
      "Training epochs #6968000:   Batch Loss = 5.316351, Accuracy = 0.615000009537\n",
      "Performance on test set: Batch Loss = 5.28964614868, Accuracy = 0.634000062943\n",
      "Training epochs #6970000:   Batch Loss = 5.259463, Accuracy = 0.648000001907\n",
      "Performance on test set: Batch Loss = 5.21593952179, Accuracy = 0.661000013351\n",
      "Training epochs #6972000:   Batch Loss = 5.275305, Accuracy = 0.636999964714\n",
      "Performance on test set: Batch Loss = 5.31320047379, Accuracy = 0.621000051498\n",
      "Training epochs #6974000:   Batch Loss = 5.299046, Accuracy = 0.632000029087\n",
      "Performance on test set: Batch Loss = 5.29207277298, Accuracy = 0.637000024319\n",
      "Training epochs #6976000:   Batch Loss = 5.288675, Accuracy = 0.631999969482\n",
      "Performance on test set: Batch Loss = 5.21110439301, Accuracy = 0.655999958515\n",
      "Training epochs #6978000:   Batch Loss = 5.303998, Accuracy = 0.62700009346\n",
      "Performance on test set: Batch Loss = 5.22445487976, Accuracy = 0.657999992371\n",
      "Training epochs #6980000:   Batch Loss = 5.241888, Accuracy = 0.646000027657\n",
      "Performance on test set: Batch Loss = 5.29272842407, Accuracy = 0.627000033855\n",
      "Training epochs #6982000:   Batch Loss = 5.274172, Accuracy = 0.633000016212\n",
      "Performance on test set: Batch Loss = 5.22822141647, Accuracy = 0.659000039101\n",
      "Training epochs #6984000:   Batch Loss = 5.247486, Accuracy = 0.638999938965\n",
      "Performance on test set: Batch Loss = 5.26167297363, Accuracy = 0.638000011444\n",
      "Training epochs #6986000:   Batch Loss = 5.299629, Accuracy = 0.636000037193\n",
      "Performance on test set: Batch Loss = 5.20323085785, Accuracy = 0.667999982834\n",
      "Training epochs #6988000:   Batch Loss = 5.266056, Accuracy = 0.634000003338\n",
      "Performance on test set: Batch Loss = 5.27127933502, Accuracy = 0.634999990463\n",
      "Training epochs #6990000:   Batch Loss = 5.262505, Accuracy = 0.638000011444\n",
      "Performance on test set: Batch Loss = 5.19702863693, Accuracy = 0.661000013351\n",
      "Training epochs #6992000:   Batch Loss = 5.234310, Accuracy = 0.64200001955\n",
      "Performance on test set: Batch Loss = 5.29183149338, Accuracy = 0.623000025749\n",
      "Training epochs #6994000:   Batch Loss = 5.232454, Accuracy = 0.646999955177\n",
      "Performance on test set: Batch Loss = 5.27264928818, Accuracy = 0.637000083923\n",
      "Training epochs #6996000:   Batch Loss = 5.226151, Accuracy = 0.651000022888\n",
      "Performance on test set: Batch Loss = 5.18905258179, Accuracy = 0.65600001812\n",
      "Training epochs #6998000:   Batch Loss = 5.233862, Accuracy = 0.661000013351\n",
      "Performance on test set: Batch Loss = 5.20536565781, Accuracy = 0.657999992371\n",
      "Training epochs #7000000:   Batch Loss = 5.163944, Accuracy = 0.672000050545\n",
      "Performance on test set: Batch Loss = 5.27448558807, Accuracy = 0.626999974251\n",
      "Training epochs #7002000:   Batch Loss = 5.209081, Accuracy = 0.65600001812\n",
      "Performance on test set: Batch Loss = 5.20978736877, Accuracy = 0.658999979496\n",
      "Training epochs #7004000:   Batch Loss = 5.226840, Accuracy = 0.648000001907\n",
      "Performance on test set: Batch Loss = 5.24076652527, Accuracy = 0.638000011444\n",
      "Training epochs #7006000:   Batch Loss = 5.215056, Accuracy = 0.644999980927\n",
      "Performance on test set: Batch Loss = 5.18254947662, Accuracy = 0.668000042439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #7008000:   Batch Loss = 5.246720, Accuracy = 0.631000041962\n",
      "Performance on test set: Batch Loss = 5.25174331665, Accuracy = 0.634999990463\n",
      "Training epochs #7010000:   Batch Loss = 5.229863, Accuracy = 0.638000011444\n",
      "Performance on test set: Batch Loss = 5.17589044571, Accuracy = 0.661000013351\n",
      "Training epochs #7012000:   Batch Loss = 5.233308, Accuracy = 0.652999997139\n",
      "Performance on test set: Batch Loss = 5.26784420013, Accuracy = 0.623000025749\n",
      "Training epochs #7014000:   Batch Loss = 5.252733, Accuracy = 0.633000016212\n",
      "Performance on test set: Batch Loss = 5.25085258484, Accuracy = 0.637000024319\n",
      "Training epochs #7016000:   Batch Loss = 5.167574, Accuracy = 0.659999966621\n",
      "Performance on test set: Batch Loss = 5.16584968567, Accuracy = 0.65600001812\n",
      "Training epochs #7018000:   Batch Loss = 5.168111, Accuracy = 0.662000000477\n",
      "Performance on test set: Batch Loss = 5.18356704712, Accuracy = 0.657999992371\n",
      "Training epochs #7020000:   Batch Loss = 5.212657, Accuracy = 0.638000011444\n",
      "Performance on test set: Batch Loss = 5.25084209442, Accuracy = 0.627000033855\n",
      "Training epochs #7022000:   Batch Loss = 5.124910, Accuracy = 0.678999960423\n",
      "Performance on test set: Batch Loss = 5.18785381317, Accuracy = 0.659000039101\n",
      "Training epochs #7024000:   Batch Loss = 5.224428, Accuracy = 0.634000003338\n",
      "Performance on test set: Batch Loss = 5.21644496918, Accuracy = 0.638000011444\n",
      "Training epochs #7026000:   Batch Loss = 5.206844, Accuracy = 0.649999976158\n",
      "Performance on test set: Batch Loss = 5.15993356705, Accuracy = 0.668000042439\n",
      "Training epochs #7028000:   Batch Loss = 5.169222, Accuracy = 0.662000000477\n",
      "Performance on test set: Batch Loss = 5.22640800476, Accuracy = 0.634999990463\n",
      "Training epochs #7030000:   Batch Loss = 5.261655, Accuracy = 0.629000008106\n",
      "Performance on test set: Batch Loss = 5.15012645721, Accuracy = 0.661000013351\n",
      "Training epochs #7032000:   Batch Loss = 5.169789, Accuracy = 0.635999977589\n",
      "Performance on test set: Batch Loss = 5.24284982681, Accuracy = 0.623000025749\n",
      "Training epochs #7034000:   Batch Loss = 5.152355, Accuracy = 0.669999957085\n",
      "Performance on test set: Batch Loss = 5.22619438171, Accuracy = 0.636999964714\n",
      "Training epochs #7036000:   Batch Loss = 5.222449, Accuracy = 0.620999991894\n",
      "Performance on test set: Batch Loss = 5.13830471039, Accuracy = 0.65600001812\n",
      "Training epochs #7038000:   Batch Loss = 5.158471, Accuracy = 0.665000021458\n",
      "Performance on test set: Batch Loss = 5.15710639954, Accuracy = 0.658000051975\n",
      "Training epochs #7040000:   Batch Loss = 5.180734, Accuracy = 0.645999968052\n",
      "Performance on test set: Batch Loss = 5.22442865372, Accuracy = 0.626999974251\n",
      "Training epochs #7042000:   Batch Loss = 5.160295, Accuracy = 0.657999992371\n",
      "Performance on test set: Batch Loss = 5.16270685196, Accuracy = 0.658999979496\n",
      "Training epochs #7044000:   Batch Loss = 5.145913, Accuracy = 0.648999989033\n",
      "Performance on test set: Batch Loss = 5.18772554398, Accuracy = 0.638000011444\n",
      "Training epochs #7046000:   Batch Loss = 5.236637, Accuracy = 0.617999970913\n",
      "Performance on test set: Batch Loss = 5.13349246979, Accuracy = 0.668999969959\n",
      "Training epochs #7048000:   Batch Loss = 5.218286, Accuracy = 0.615999996662\n",
      "Performance on test set: Batch Loss = 5.19651794434, Accuracy = 0.634999990463\n",
      "Training epochs #7050000:   Batch Loss = 5.167738, Accuracy = 0.648000001907\n",
      "Performance on test set: Batch Loss = 5.12090826035, Accuracy = 0.661000013351\n",
      "Training epochs #7052000:   Batch Loss = 5.178093, Accuracy = 0.636999964714\n",
      "Performance on test set: Batch Loss = 5.20961666107, Accuracy = 0.624000012875\n",
      "Training epochs #7054000:   Batch Loss = 5.203628, Accuracy = 0.632999956608\n",
      "Performance on test set: Batch Loss = 5.19575595856, Accuracy = 0.638000011444\n",
      "Training epochs #7056000:   Batch Loss = 5.194660, Accuracy = 0.632000029087\n",
      "Performance on test set: Batch Loss = 5.10550069809, Accuracy = 0.657000005245\n",
      "Training epochs #7058000:   Batch Loss = 5.206112, Accuracy = 0.628000020981\n",
      "Performance on test set: Batch Loss = 5.1251707077, Accuracy = 0.657999992371\n",
      "Training epochs #7060000:   Batch Loss = 5.140461, Accuracy = 0.646000027657\n",
      "Performance on test set: Batch Loss = 5.19053125381, Accuracy = 0.626999974251\n",
      "Training epochs #7062000:   Batch Loss = 5.172076, Accuracy = 0.634000003338\n",
      "Performance on test set: Batch Loss = 5.13096189499, Accuracy = 0.658999979496\n",
      "Training epochs #7064000:   Batch Loss = 5.141788, Accuracy = 0.638999938965\n",
      "Performance on test set: Batch Loss = 5.15223121643, Accuracy = 0.638000011444\n",
      "Training epochs #7066000:   Batch Loss = 5.197615, Accuracy = 0.635999917984\n",
      "Performance on test set: Batch Loss = 5.09983444214, Accuracy = 0.669000029564\n",
      "Training epochs #7068000:   Batch Loss = 5.163171, Accuracy = 0.635000050068\n",
      "Performance on test set: Batch Loss = 5.16010522842, Accuracy = 0.635999917984\n",
      "Training epochs #7070000:   Batch Loss = 5.151848, Accuracy = 0.643000006676\n",
      "Performance on test set: Batch Loss = 5.08389186859, Accuracy = 0.665000021458\n",
      "Training epochs #7072000:   Batch Loss = 5.122034, Accuracy = 0.648999989033\n",
      "Performance on test set: Batch Loss = 5.17175722122, Accuracy = 0.627000033855\n",
      "Training epochs #7074000:   Batch Loss = 5.118392, Accuracy = 0.655000030994\n",
      "Performance on test set: Batch Loss = 5.16015815735, Accuracy = 0.642000079155\n",
      "Training epochs #7076000:   Batch Loss = 5.106891, Accuracy = 0.657000005245\n",
      "Performance on test set: Batch Loss = 5.06392335892, Accuracy = 0.666999995708\n",
      "Training epochs #7078000:   Batch Loss = 5.121955, Accuracy = 0.666999936104\n",
      "Performance on test set: Batch Loss = 5.08605861664, Accuracy = 0.657999992371\n",
      "Training epochs #7080000:   Batch Loss = 5.045831, Accuracy = 0.6740000844\n",
      "Performance on test set: Batch Loss = 5.15205335617, Accuracy = 0.634000003338\n",
      "Training epochs #7082000:   Batch Loss = 5.084404, Accuracy = 0.657999992371\n",
      "Performance on test set: Batch Loss = 5.09394931793, Accuracy = 0.659999966621\n",
      "Training epochs #7084000:   Batch Loss = 5.098676, Accuracy = 0.654000043869\n",
      "Performance on test set: Batch Loss = 5.1091299057, Accuracy = 0.641000032425\n",
      "Training epochs #7086000:   Batch Loss = 5.079683, Accuracy = 0.651000022888\n",
      "Performance on test set: Batch Loss = 5.05863714218, Accuracy = 0.768000006676\n",
      "Training epochs #7088000:   Batch Loss = 5.118832, Accuracy = 0.741000056267\n",
      "Performance on test set: Batch Loss = 5.11709403992, Accuracy = 0.75\n",
      "Training epochs #7090000:   Batch Loss = 5.091841, Accuracy = 0.759000003338\n",
      "Performance on test set: Batch Loss = 5.04078435898, Accuracy = 0.774999976158\n",
      "Training epochs #7092000:   Batch Loss = 5.110942, Accuracy = 0.740999996662\n",
      "Performance on test set: Batch Loss = 5.12512779236, Accuracy = 0.744000017643\n",
      "Training epochs #7094000:   Batch Loss = 5.108306, Accuracy = 0.753999948502\n",
      "Performance on test set: Batch Loss = 5.11840009689, Accuracy = 0.744000017643\n",
      "Training epochs #7096000:   Batch Loss = 5.029868, Accuracy = 0.763000011444\n",
      "Performance on test set: Batch Loss = 5.01787948608, Accuracy = 0.78100001812\n",
      "Training epochs #7098000:   Batch Loss = 5.025774, Accuracy = 0.773999989033\n",
      "Performance on test set: Batch Loss = 5.04045915604, Accuracy = 0.768000006676\n",
      "Training epochs #7100000:   Batch Loss = 5.064517, Accuracy = 0.760999977589\n",
      "Performance on test set: Batch Loss = 5.10603761673, Accuracy = 0.740000009537\n",
      "Training epochs #7102000:   Batch Loss = 4.980065, Accuracy = 0.788000047207\n",
      "Performance on test set: Batch Loss = 5.0502166748, Accuracy = 0.758999943733\n",
      "Training epochs #7104000:   Batch Loss = 5.077254, Accuracy = 0.747000038624\n",
      "Performance on test set: Batch Loss = 5.0611987114, Accuracy = 0.759999990463\n",
      "Training epochs #7106000:   Batch Loss = 5.061355, Accuracy = 0.755000054836\n",
      "Performance on test set: Batch Loss = 5.01472187042, Accuracy = 0.767000079155\n",
      "Training epochs #7108000:   Batch Loss = 5.020699, Accuracy = 0.76599997282\n",
      "Performance on test set: Batch Loss = 5.06863737106, Accuracy = 0.75\n",
      "Training epochs #7110000:   Batch Loss = 5.116772, Accuracy = 0.735000014305\n",
      "Performance on test set: Batch Loss = 4.99338912964, Accuracy = 0.774999976158\n",
      "Training epochs #7112000:   Batch Loss = 5.007970, Accuracy = 0.761999964714\n",
      "Performance on test set: Batch Loss = 5.0796122551, Accuracy = 0.745000004768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #7114000:   Batch Loss = 5.002916, Accuracy = 0.769999980927\n",
      "Performance on test set: Batch Loss = 5.07625293732, Accuracy = 0.743000030518\n",
      "Training epochs #7116000:   Batch Loss = 5.053610, Accuracy = 0.754000067711\n",
      "Performance on test set: Batch Loss = 4.97166490555, Accuracy = 0.78100001812\n",
      "Training epochs #7118000:   Batch Loss = 5.004371, Accuracy = 0.764999985695\n",
      "Performance on test set: Batch Loss = 4.99434328079, Accuracy = 0.768999934196\n",
      "Training epochs #7120000:   Batch Loss = 5.024785, Accuracy = 0.75\n",
      "Performance on test set: Batch Loss = 5.06348466873, Accuracy = 0.740000009537\n",
      "Training epochs #7122000:   Batch Loss = 5.001915, Accuracy = 0.764999985695\n",
      "Performance on test set: Batch Loss = 5.00986480713, Accuracy = 0.757999956608\n",
      "Training epochs #7124000:   Batch Loss = 4.968626, Accuracy = 0.776000022888\n",
      "Performance on test set: Batch Loss = 5.01701450348, Accuracy = 0.761000037193\n",
      "Training epochs #7126000:   Batch Loss = 5.074666, Accuracy = 0.726999998093\n",
      "Performance on test set: Batch Loss = 4.97411632538, Accuracy = 0.766999959946\n",
      "Training epochs #7128000:   Batch Loss = 5.047767, Accuracy = 0.740000009537\n",
      "Performance on test set: Batch Loss = 5.02504253387, Accuracy = 0.75\n",
      "Training epochs #7130000:   Batch Loss = 5.006507, Accuracy = 0.755999982357\n",
      "Performance on test set: Batch Loss = 4.95157766342, Accuracy = 0.774000048637\n",
      "Training epochs #7132000:   Batch Loss = 5.013226, Accuracy = 0.756000041962\n",
      "Performance on test set: Batch Loss = 5.03899478912, Accuracy = 0.745000004768\n",
      "Training epochs #7134000:   Batch Loss = 5.035551, Accuracy = 0.742000043392\n",
      "Performance on test set: Batch Loss = 5.03761863708, Accuracy = 0.742999970913\n",
      "Training epochs #7136000:   Batch Loss = 5.040215, Accuracy = 0.736000001431\n",
      "Performance on test set: Batch Loss = 4.93422842026, Accuracy = 0.779000043869\n",
      "Training epochs #7138000:   Batch Loss = 5.043246, Accuracy = 0.733999967575\n",
      "Performance on test set: Batch Loss = 4.95326852798, Accuracy = 0.768999993801\n",
      "Training epochs #7140000:   Batch Loss = 4.975746, Accuracy = 0.760999977589\n",
      "Performance on test set: Batch Loss = 5.02336978912, Accuracy = 0.739000022411\n",
      "Training epochs #7142000:   Batch Loss = 5.010934, Accuracy = 0.741999983788\n",
      "Performance on test set: Batch Loss = 4.97341442108, Accuracy = 0.757000029087\n",
      "Training epochs #7144000:   Batch Loss = 4.970109, Accuracy = 0.751999974251\n",
      "Performance on test set: Batch Loss = 4.97779893875, Accuracy = 0.761000037193\n",
      "Training epochs #7146000:   Batch Loss = 5.037940, Accuracy = 0.737999975681\n",
      "Performance on test set: Batch Loss = 4.9382610321, Accuracy = 0.764999985695\n",
      "Training epochs #7148000:   Batch Loss = 5.003552, Accuracy = 0.737999975681\n",
      "Performance on test set: Batch Loss = 4.98716449738, Accuracy = 0.747999966145\n",
      "Training epochs #7150000:   Batch Loss = 4.990200, Accuracy = 0.755000054836\n",
      "Performance on test set: Batch Loss = 4.91291999817, Accuracy = 0.772999942303\n",
      "Training epochs #7152000:   Batch Loss = 4.961371, Accuracy = 0.750999987125\n",
      "Performance on test set: Batch Loss = 5.00417661667, Accuracy = 0.744000017643\n",
      "Training epochs #7154000:   Batch Loss = 4.962918, Accuracy = 0.757000029087\n",
      "Performance on test set: Batch Loss = 5.00184059143, Accuracy = 0.743000030518\n",
      "Training epochs #7156000:   Batch Loss = 4.940852, Accuracy = 0.764999985695\n",
      "Performance on test set: Batch Loss = 4.8995552063, Accuracy = 0.778000056744\n",
      "Training epochs #7158000:   Batch Loss = 4.960512, Accuracy = 0.757000029087\n",
      "Performance on test set: Batch Loss = 4.91266822815, Accuracy = 0.76700001955\n",
      "Training epochs #7160000:   Batch Loss = 4.879219, Accuracy = 0.773999929428\n",
      "Performance on test set: Batch Loss = 4.98948764801, Accuracy = 0.738000035286\n",
      "Training epochs #7162000:   Batch Loss = 4.926450, Accuracy = 0.771000027657\n",
      "Performance on test set: Batch Loss = 4.93499803543, Accuracy = 0.757000029087\n",
      "Training epochs #7164000:   Batch Loss = 4.937019, Accuracy = 0.760999977589\n",
      "Performance on test set: Batch Loss = 4.93787193298, Accuracy = 0.761000037193\n",
      "Training epochs #7166000:   Batch Loss = 4.913608, Accuracy = 0.766000032425\n",
      "Performance on test set: Batch Loss = 4.89717245102, Accuracy = 0.76499992609\n",
      "Training epochs #7168000:   Batch Loss = 4.963073, Accuracy = 0.741000056267\n",
      "Performance on test set: Batch Loss = 4.94857597351, Accuracy = 0.750000059605\n",
      "Training epochs #7170000:   Batch Loss = 4.929319, Accuracy = 0.759000062943\n",
      "Performance on test set: Batch Loss = 4.87162065506, Accuracy = 0.774000048637\n",
      "Training epochs #7172000:   Batch Loss = 4.945301, Accuracy = 0.738999962807\n",
      "Performance on test set: Batch Loss = 4.96473360062, Accuracy = 0.746999979019\n",
      "Training epochs #7174000:   Batch Loss = 4.940462, Accuracy = 0.756000041962\n",
      "Performance on test set: Batch Loss = 4.95631790161, Accuracy = 0.743999958038\n",
      "Training epochs #7176000:   Batch Loss = 4.866228, Accuracy = 0.763999998569\n",
      "Performance on test set: Batch Loss = 4.8609213829, Accuracy = 0.779000043869\n",
      "Training epochs #7178000:   Batch Loss = 4.851810, Accuracy = 0.773000001907\n",
      "Performance on test set: Batch Loss = 4.86177110672, Accuracy = 0.768999993801\n",
      "Training epochs #7180000:   Batch Loss = 4.900947, Accuracy = 0.758000016212\n",
      "Performance on test set: Batch Loss = 4.94441127777, Accuracy = 0.739000022411\n",
      "Training epochs #7182000:   Batch Loss = 4.814688, Accuracy = 0.789000034332\n",
      "Performance on test set: Batch Loss = 4.88336706161, Accuracy = 0.758000016212\n",
      "Training epochs #7184000:   Batch Loss = 4.901648, Accuracy = 0.745000004768\n",
      "Performance on test set: Batch Loss = 4.88700342178, Accuracy = 0.761999964714\n",
      "Training epochs #7186000:   Batch Loss = 4.882427, Accuracy = 0.756999969482\n",
      "Performance on test set: Batch Loss = 4.84816169739, Accuracy = 0.772000074387\n",
      "Training epochs #7188000:   Batch Loss = 4.849480, Accuracy = 0.775000035763\n",
      "Performance on test set: Batch Loss = 4.89967441559, Accuracy = 0.754000008106\n",
      "Training epochs #7190000:   Batch Loss = 4.951111, Accuracy = 0.746000051498\n",
      "Performance on test set: Batch Loss = 4.81937408447, Accuracy = 0.77899992466\n",
      "Training epochs #7192000:   Batch Loss = 4.839175, Accuracy = 0.775000035763\n",
      "Performance on test set: Batch Loss = 4.91987800598, Accuracy = 0.754000008106\n",
      "Training epochs #7194000:   Batch Loss = 4.839576, Accuracy = 0.773000001907\n",
      "Performance on test set: Batch Loss = 4.90374135971, Accuracy = 0.753000020981\n",
      "Training epochs #7196000:   Batch Loss = 4.890849, Accuracy = 0.763000011444\n",
      "Performance on test set: Batch Loss = 4.81673192978, Accuracy = 0.783000051975\n",
      "Training epochs #7198000:   Batch Loss = 4.842117, Accuracy = 0.770999968052\n",
      "Performance on test set: Batch Loss = 4.8084859848, Accuracy = 0.774000048637\n",
      "Training epochs #7200000:   Batch Loss = 4.842709, Accuracy = 0.763000071049\n",
      "Performance on test set: Batch Loss = 4.8965678215, Accuracy = 0.741999983788\n",
      "Training epochs #7202000:   Batch Loss = 4.832911, Accuracy = 0.774000048637\n",
      "Performance on test set: Batch Loss = 4.83537435532, Accuracy = 0.76599997282\n",
      "Training epochs #7204000:   Batch Loss = 4.802571, Accuracy = 0.777999997139\n",
      "Performance on test set: Batch Loss = 4.84145402908, Accuracy = 0.768000006676\n",
      "Training epochs #7206000:   Batch Loss = 4.880679, Accuracy = 0.736000061035\n",
      "Performance on test set: Batch Loss = 4.80515432358, Accuracy = 0.773000061512\n",
      "Training epochs #7208000:   Batch Loss = 4.879994, Accuracy = 0.749000072479\n",
      "Performance on test set: Batch Loss = 4.85701417923, Accuracy = 0.754000008106\n",
      "Training epochs #7210000:   Batch Loss = 4.843246, Accuracy = 0.763999998569\n",
      "Performance on test set: Batch Loss = 4.7813744545, Accuracy = 0.779000043869\n",
      "Training epochs #7212000:   Batch Loss = 4.867784, Accuracy = 0.759000003338\n",
      "Performance on test set: Batch Loss = 4.8845744133, Accuracy = 0.753999948502\n",
      "Training epochs #7214000:   Batch Loss = 4.855364, Accuracy = 0.748000025749\n",
      "Performance on test set: Batch Loss = 4.86492919922, Accuracy = 0.753000020981\n",
      "Training epochs #7216000:   Batch Loss = 4.874121, Accuracy = 0.747000038624\n",
      "Performance on test set: Batch Loss = 4.78820514679, Accuracy = 0.782999992371\n",
      "Training epochs #7218000:   Batch Loss = 4.866505, Accuracy = 0.742999970913\n",
      "Performance on test set: Batch Loss = 4.7750749588, Accuracy = 0.773999989033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #7220000:   Batch Loss = 4.811257, Accuracy = 0.770000040531\n",
      "Performance on test set: Batch Loss = 4.86550712585, Accuracy = 0.741999983788\n",
      "Training epochs #7222000:   Batch Loss = 4.851506, Accuracy = 0.754999995232\n",
      "Performance on test set: Batch Loss = 4.8057141304, Accuracy = 0.76599997282\n",
      "Training epochs #7224000:   Batch Loss = 4.791969, Accuracy = 0.760000050068\n",
      "Performance on test set: Batch Loss = 4.81362199783, Accuracy = 0.768999993801\n",
      "Training epochs #7226000:   Batch Loss = 4.876730, Accuracy = 0.746000051498\n",
      "Performance on test set: Batch Loss = 4.77935695648, Accuracy = 0.773999989033\n",
      "Training epochs #7228000:   Batch Loss = 4.818702, Accuracy = 0.758000016212\n",
      "Performance on test set: Batch Loss = 4.83170175552, Accuracy = 0.756000041962\n",
      "Training epochs #7230000:   Batch Loss = 4.838474, Accuracy = 0.764999985695\n",
      "Performance on test set: Batch Loss = 4.75706195831, Accuracy = 0.77999997139\n",
      "Training epochs #7232000:   Batch Loss = 4.802512, Accuracy = 0.768999993801\n",
      "Performance on test set: Batch Loss = 4.86020612717, Accuracy = 0.754000067711\n",
      "Training epochs #7234000:   Batch Loss = 4.815620, Accuracy = 0.768000006676\n",
      "Performance on test set: Batch Loss = 4.84057283401, Accuracy = 0.753000020981\n",
      "Training epochs #7236000:   Batch Loss = 4.797488, Accuracy = 0.770999968052\n",
      "Performance on test set: Batch Loss = 4.76534366608, Accuracy = 0.782999992371\n",
      "Training epochs #7238000:   Batch Loss = 4.800067, Accuracy = 0.764999985695\n",
      "Performance on test set: Batch Loss = 4.75147438049, Accuracy = 0.773999989033\n",
      "Training epochs #7240000:   Batch Loss = 4.727479, Accuracy = 0.77999997139\n",
      "Performance on test set: Batch Loss = 4.84493160248, Accuracy = 0.741999983788\n",
      "Training epochs #7242000:   Batch Loss = 4.799831, Accuracy = 0.776999950409\n",
      "Performance on test set: Batch Loss = 4.78440666199, Accuracy = 0.766000032425\n",
      "Training epochs #7244000:   Batch Loss = 4.806840, Accuracy = 0.768000006676\n",
      "Performance on test set: Batch Loss = 4.79167413712, Accuracy = 0.768999993801\n",
      "Training epochs #7246000:   Batch Loss = 4.784223, Accuracy = 0.770000040531\n",
      "Performance on test set: Batch Loss = 4.7589392662, Accuracy = 0.773999929428\n",
      "Training epochs #7248000:   Batch Loss = 4.815041, Accuracy = 0.754999995232\n",
      "Performance on test set: Batch Loss = 4.81265640259, Accuracy = 0.756000041962\n",
      "Training epochs #7250000:   Batch Loss = 4.802454, Accuracy = 0.763999938965\n",
      "Performance on test set: Batch Loss = 4.74107837677, Accuracy = 0.780000030994\n",
      "Training epochs #7252000:   Batch Loss = 4.798315, Accuracy = 0.749000012875\n",
      "Performance on test set: Batch Loss = 4.8367934227, Accuracy = 0.754999995232\n",
      "Training epochs #7254000:   Batch Loss = 4.809896, Accuracy = 0.761999964714\n",
      "Performance on test set: Batch Loss = 4.81927776337, Accuracy = 0.754000067711\n",
      "Training epochs #7256000:   Batch Loss = 4.742123, Accuracy = 0.769999980927\n",
      "Performance on test set: Batch Loss = 4.74659156799, Accuracy = 0.783000051975\n",
      "Training epochs #7258000:   Batch Loss = 4.726252, Accuracy = 0.787000000477\n",
      "Performance on test set: Batch Loss = 4.72987747192, Accuracy = 0.774000048637\n",
      "Training epochs #7260000:   Batch Loss = 4.780845, Accuracy = 0.772000074387\n",
      "Performance on test set: Batch Loss = 4.82427024841, Accuracy = 0.741999983788\n",
      "Training epochs #7262000:   Batch Loss = 4.704867, Accuracy = 0.796000003815\n",
      "Performance on test set: Batch Loss = 4.76155471802, Accuracy = 0.767999947071\n",
      "Training epochs #7264000:   Batch Loss = 4.779047, Accuracy = 0.759000003338\n",
      "Performance on test set: Batch Loss = 4.77384519577, Accuracy = 0.768000006676\n",
      "Training epochs #7266000:   Batch Loss = 4.763547, Accuracy = 0.768999993801\n",
      "Performance on test set: Batch Loss = 4.74096155167, Accuracy = 0.774000048637\n",
      "Training epochs #7268000:   Batch Loss = 4.735993, Accuracy = 0.776000022888\n",
      "Performance on test set: Batch Loss = 4.79212903976, Accuracy = 0.756000041962\n",
      "Training epochs #7270000:   Batch Loss = 4.831478, Accuracy = 0.747000038624\n",
      "Performance on test set: Batch Loss = 4.71815347672, Accuracy = 0.77999997139\n",
      "Training epochs #7272000:   Batch Loss = 4.736382, Accuracy = 0.776000022888\n",
      "Performance on test set: Batch Loss = 4.81900024414, Accuracy = 0.754999995232\n",
      "Training epochs #7274000:   Batch Loss = 4.743218, Accuracy = 0.773999929428\n",
      "Performance on test set: Batch Loss = 4.80643987656, Accuracy = 0.754000067711\n",
      "Training epochs #7276000:   Batch Loss = 4.808155, Accuracy = 0.774999976158\n",
      "Performance on test set: Batch Loss = 4.73468637466, Accuracy = 0.791999995708\n",
      "Training epochs #7278000:   Batch Loss = 4.754877, Accuracy = 0.773000001907\n",
      "Performance on test set: Batch Loss = 4.71239519119, Accuracy = 0.775000035763\n",
      "Training epochs #7280000:   Batch Loss = 4.749248, Accuracy = 0.763999998569\n",
      "Performance on test set: Batch Loss = 4.79500865936, Accuracy = 0.741999983788\n",
      "Training epochs #7282000:   Batch Loss = 4.742674, Accuracy = 0.775000035763\n",
      "Performance on test set: Batch Loss = 4.73417282104, Accuracy = 0.767000079155\n",
      "Training epochs #7284000:   Batch Loss = 4.718232, Accuracy = 0.777999997139\n",
      "Performance on test set: Batch Loss = 4.75462865829, Accuracy = 0.76800006628\n",
      "Training epochs #7286000:   Batch Loss = 4.785504, Accuracy = 0.736000061035\n",
      "Performance on test set: Batch Loss = 4.71922159195, Accuracy = 0.773999989033\n",
      "Training epochs #7288000:   Batch Loss = 4.782026, Accuracy = 0.749000012875\n",
      "Performance on test set: Batch Loss = 4.77663993835, Accuracy = 0.754000008106\n",
      "Training epochs #7290000:   Batch Loss = 4.751530, Accuracy = 0.7650000453\n",
      "Performance on test set: Batch Loss = 4.69805145264, Accuracy = 0.780000030994\n",
      "Training epochs #7292000:   Batch Loss = 4.765368, Accuracy = 0.759000003338\n",
      "Performance on test set: Batch Loss = 4.79153490067, Accuracy = 0.756000041962\n",
      "Training epochs #7294000:   Batch Loss = 4.762180, Accuracy = 0.74899995327\n",
      "Performance on test set: Batch Loss = 4.77473831177, Accuracy = 0.754000067711\n",
      "Training epochs #7296000:   Batch Loss = 4.796131, Accuracy = 0.748000025749\n",
      "Performance on test set: Batch Loss = 4.71568346024, Accuracy = 0.783000051975\n",
      "Training epochs #7298000:   Batch Loss = 4.786088, Accuracy = 0.748000025749\n",
      "Performance on test set: Batch Loss = 4.70054244995, Accuracy = 0.777999997139\n",
      "Training epochs #7300000:   Batch Loss = 4.743647, Accuracy = 0.782999932766\n",
      "Performance on test set: Batch Loss = 4.78851747513, Accuracy = 0.742999970913\n",
      "Training epochs #7302000:   Batch Loss = 4.770078, Accuracy = 0.75\n",
      "Performance on test set: Batch Loss = 4.71162414551, Accuracy = 0.771000087261\n",
      "Training epochs #7304000:   Batch Loss = 4.816347, Accuracy = 0.762999951839\n",
      "Performance on test set: Batch Loss = 4.93069458008, Accuracy = 0.760999977589\n",
      "Training epochs #7306000:   Batch Loss = 5.125822, Accuracy = 0.740000009537\n",
      "Performance on test set: Batch Loss = 5.66069316864, Accuracy = 0.678000032902\n",
      "Training epochs #7308000:   Batch Loss = 5.885192, Accuracy = 0.633999943733\n",
      "Performance on test set: Batch Loss = 5.86090517044, Accuracy = 0.635999977589\n",
      "Training epochs #7310000:   Batch Loss = 5.822476, Accuracy = 0.638999998569\n",
      "Performance on test set: Batch Loss = 5.72450351715, Accuracy = 0.664000034332\n",
      "Training epochs #7312000:   Batch Loss = 5.752306, Accuracy = 0.643000006676\n",
      "Performance on test set: Batch Loss = 5.77243947983, Accuracy = 0.627999961376\n",
      "Training epochs #7314000:   Batch Loss = 5.583943, Accuracy = 0.648999989033\n",
      "Performance on test set: Batch Loss = 5.56830596924, Accuracy = 0.64099997282\n",
      "Training epochs #7316000:   Batch Loss = 5.499390, Accuracy = 0.652999937534\n",
      "Performance on test set: Batch Loss = 5.41367912292, Accuracy = 0.65700006485\n",
      "Training epochs #7318000:   Batch Loss = 5.339326, Accuracy = 0.661999940872\n",
      "Performance on test set: Batch Loss = 5.28088569641, Accuracy = 0.657999992371\n",
      "Training epochs #7320000:   Batch Loss = 5.174661, Accuracy = 0.675000011921\n",
      "Performance on test set: Batch Loss = 5.24840736389, Accuracy = 0.629000008106\n",
      "Training epochs #7322000:   Batch Loss = 5.164656, Accuracy = 0.657999992371\n",
      "Performance on test set: Batch Loss = 5.14685344696, Accuracy = 0.658999979496\n",
      "Training epochs #7324000:   Batch Loss = 5.151871, Accuracy = 0.653999984264\n",
      "Performance on test set: Batch Loss = 5.17810821533, Accuracy = 0.639999985695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #7326000:   Batch Loss = 5.170527, Accuracy = 0.647000074387\n",
      "Performance on test set: Batch Loss = 5.16437244415, Accuracy = 0.679000020027\n",
      "Training epochs #7328000:   Batch Loss = 5.227906, Accuracy = 0.643000006676\n",
      "Performance on test set: Batch Loss = 5.2490696907, Accuracy = 0.638000011444\n",
      "Training epochs #7330000:   Batch Loss = 5.213775, Accuracy = 0.643999993801\n",
      "Performance on test set: Batch Loss = 5.14675140381, Accuracy = 0.667999982834\n",
      "Training epochs #7332000:   Batch Loss = 5.216712, Accuracy = 0.65499997139\n",
      "Performance on test set: Batch Loss = 5.22042942047, Accuracy = 0.627000033855\n",
      "Training epochs #7334000:   Batch Loss = 5.196549, Accuracy = 0.635999977589\n",
      "Performance on test set: Batch Loss = 5.20383930206, Accuracy = 0.64099997282\n",
      "Training epochs #7336000:   Batch Loss = 5.116317, Accuracy = 0.665000021458\n",
      "Performance on test set: Batch Loss = 5.10123586655, Accuracy = 0.657000005245\n",
      "Training epochs #7338000:   Batch Loss = 5.102847, Accuracy = 0.662000000477\n",
      "Performance on test set: Batch Loss = 5.12086582184, Accuracy = 0.657999992371\n",
      "Training epochs #7340000:   Batch Loss = 5.138509, Accuracy = 0.639999985695\n",
      "Performance on test set: Batch Loss = 5.1839838028, Accuracy = 0.629000067711\n",
      "Training epochs #7342000:   Batch Loss = 5.044133, Accuracy = 0.679000020027\n",
      "Performance on test set: Batch Loss = 5.11096954346, Accuracy = 0.658999979496\n",
      "Training epochs #7344000:   Batch Loss = 5.145154, Accuracy = 0.634000062943\n",
      "Performance on test set: Batch Loss = 5.14363002777, Accuracy = 0.639999985695\n",
      "Training epochs #7346000:   Batch Loss = 5.126567, Accuracy = 0.652000010014\n",
      "Performance on test set: Batch Loss = 5.08741474152, Accuracy = 0.668999969959\n",
      "Training epochs #7348000:   Batch Loss = 5.097208, Accuracy = 0.665000021458\n",
      "Performance on test set: Batch Loss = 5.1504445076, Accuracy = 0.634999990463\n",
      "Training epochs #7350000:   Batch Loss = 5.177810, Accuracy = 0.631999969482\n",
      "Performance on test set: Batch Loss = 5.06770706177, Accuracy = 0.663999974728\n",
      "Training epochs #7352000:   Batch Loss = 5.085912, Accuracy = 0.638999938965\n",
      "Performance on test set: Batch Loss = 5.15532875061, Accuracy = 0.625999927521\n",
      "Training epochs #7354000:   Batch Loss = 5.063286, Accuracy = 0.671000003815\n",
      "Performance on test set: Batch Loss = 5.13558769226, Accuracy = 0.64099997282\n",
      "Training epochs #7356000:   Batch Loss = 5.134393, Accuracy = 0.62399995327\n",
      "Performance on test set: Batch Loss = 5.04741573334, Accuracy = 0.65700006485\n",
      "Training epochs #7358000:   Batch Loss = 5.065120, Accuracy = 0.666999995708\n",
      "Performance on test set: Batch Loss = 5.06649637222, Accuracy = 0.657999992371\n",
      "Training epochs #7360000:   Batch Loss = 5.090302, Accuracy = 0.648000001907\n",
      "Performance on test set: Batch Loss = 5.13310098648, Accuracy = 0.628999948502\n",
      "Training epochs #7362000:   Batch Loss = 5.067942, Accuracy = 0.659999966621\n",
      "Performance on test set: Batch Loss = 5.07105302811, Accuracy = 0.660000026226\n",
      "Training epochs #7364000:   Batch Loss = 5.052113, Accuracy = 0.650999963284\n",
      "Performance on test set: Batch Loss = 5.09899997711, Accuracy = 0.639999985695\n",
      "Training epochs #7366000:   Batch Loss = 5.143364, Accuracy = 0.620999932289\n",
      "Performance on test set: Batch Loss = 5.04095888138, Accuracy = 0.671000003815\n",
      "Training epochs #7368000:   Batch Loss = 5.119478, Accuracy = 0.622999966145\n",
      "Performance on test set: Batch Loss = 5.09746265411, Accuracy = 0.638000011444\n",
      "Training epochs #7370000:   Batch Loss = 5.070837, Accuracy = 0.652999997139\n",
      "Performance on test set: Batch Loss = 5.02301597595, Accuracy = 0.667999982834\n",
      "Training epochs #7372000:   Batch Loss = 5.074270, Accuracy = 0.644999980927\n",
      "Performance on test set: Batch Loss = 5.1024107933, Accuracy = 0.629000008106\n",
      "Training epochs #7374000:   Batch Loss = 5.096154, Accuracy = 0.637000024319\n",
      "Performance on test set: Batch Loss = 5.08879566193, Accuracy = 0.64200001955\n",
      "Training epochs #7376000:   Batch Loss = 5.086111, Accuracy = 0.633000016212\n",
      "Performance on test set: Batch Loss = 4.99461746216, Accuracy = 0.657999992371\n",
      "Training epochs #7378000:   Batch Loss = 5.092637, Accuracy = 0.637000024319\n",
      "Performance on test set: Batch Loss = 5.00926017761, Accuracy = 0.658999919891\n",
      "Training epochs #7380000:   Batch Loss = 5.022600, Accuracy = 0.650999963284\n",
      "Performance on test set: Batch Loss = 5.06430721283, Accuracy = 0.636000037193\n",
      "Training epochs #7382000:   Batch Loss = 5.039003, Accuracy = 0.64099997282\n",
      "Performance on test set: Batch Loss = 4.98827695847, Accuracy = 0.759999990463\n",
      "Training epochs #7384000:   Batch Loss = 4.961236, Accuracy = 0.758000016212\n",
      "Performance on test set: Batch Loss = 4.95416545868, Accuracy = 0.763000071049\n",
      "Training epochs #7386000:   Batch Loss = 5.006641, Accuracy = 0.740000009537\n",
      "Performance on test set: Batch Loss = 4.9163351059, Accuracy = 0.768000006676\n",
      "Training epochs #7388000:   Batch Loss = 4.935570, Accuracy = 0.741000056267\n",
      "Performance on test set: Batch Loss = 4.91166591644, Accuracy = 0.749000012875\n",
      "Training epochs #7390000:   Batch Loss = 4.922316, Accuracy = 0.756000041962\n",
      "Performance on test set: Batch Loss = 4.83251667023, Accuracy = 0.777000010014\n",
      "Training epochs #7392000:   Batch Loss = 4.896935, Accuracy = 0.753000080585\n",
      "Performance on test set: Batch Loss = 4.92042255402, Accuracy = 0.750999987125\n",
      "Training epochs #7394000:   Batch Loss = 4.874792, Accuracy = 0.758999943733\n",
      "Performance on test set: Batch Loss = 4.92579078674, Accuracy = 0.748000025749\n",
      "Training epochs #7396000:   Batch Loss = 4.845062, Accuracy = 0.76599997282\n",
      "Performance on test set: Batch Loss = 4.79966831207, Accuracy = 0.779000043869\n",
      "Training epochs #7398000:   Batch Loss = 4.855226, Accuracy = 0.759000062943\n",
      "Performance on test set: Batch Loss = 4.80738687515, Accuracy = 0.767999947071\n",
      "Training epochs #7400000:   Batch Loss = 4.767169, Accuracy = 0.776999950409\n",
      "Performance on test set: Batch Loss = 4.87090110779, Accuracy = 0.741000056267\n",
      "Training epochs #7402000:   Batch Loss = 4.819088, Accuracy = 0.773999989033\n",
      "Performance on test set: Batch Loss = 4.81147527695, Accuracy = 0.759999990463\n",
      "Training epochs #7404000:   Batch Loss = 4.814270, Accuracy = 0.767999947071\n",
      "Performance on test set: Batch Loss = 4.8174571991, Accuracy = 0.764000058174\n",
      "Training epochs #7406000:   Batch Loss = 4.786739, Accuracy = 0.769000053406\n",
      "Performance on test set: Batch Loss = 4.76566410065, Accuracy = 0.768000006676\n",
      "Training epochs #7408000:   Batch Loss = 4.809637, Accuracy = 0.745999991894\n",
      "Performance on test set: Batch Loss = 4.80665636063, Accuracy = 0.753999948502\n",
      "Training epochs #7410000:   Batch Loss = 4.791779, Accuracy = 0.766000032425\n",
      "Performance on test set: Batch Loss = 4.73016309738, Accuracy = 0.782000005245\n",
      "Training epochs #7412000:   Batch Loss = 4.793328, Accuracy = 0.75100004673\n",
      "Performance on test set: Batch Loss = 4.82442092896, Accuracy = 0.759000003338\n",
      "Training epochs #7414000:   Batch Loss = 4.799015, Accuracy = 0.76599997282\n",
      "Performance on test set: Batch Loss = 4.81300020218, Accuracy = 0.758000016212\n",
      "Training epochs #7416000:   Batch Loss = 4.714660, Accuracy = 0.775000035763\n",
      "Performance on test set: Batch Loss = 4.71672725677, Accuracy = 0.784000039101\n",
      "Training epochs #7418000:   Batch Loss = 4.709214, Accuracy = 0.787000000477\n",
      "Performance on test set: Batch Loss = 4.71121501923, Accuracy = 0.774999976158\n",
      "Training epochs #7420000:   Batch Loss = 4.753191, Accuracy = 0.773000061512\n",
      "Performance on test set: Batch Loss = 4.79371786118, Accuracy = 0.744000017643\n",
      "Training epochs #7422000:   Batch Loss = 4.672560, Accuracy = 0.795000016689\n",
      "Performance on test set: Batch Loss = 4.73786735535, Accuracy = 0.767000079155\n",
      "Training epochs #7424000:   Batch Loss = 4.757296, Accuracy = 0.754000008106\n",
      "Performance on test set: Batch Loss = 4.75005483627, Accuracy = 0.770000040531\n",
      "Training epochs #7426000:   Batch Loss = 4.737672, Accuracy = 0.768000006676\n",
      "Performance on test set: Batch Loss = 4.70799684525, Accuracy = 0.776000022888\n",
      "Training epochs #7428000:   Batch Loss = 4.703385, Accuracy = 0.778999984264\n",
      "Performance on test set: Batch Loss = 4.75186920166, Accuracy = 0.754999995232\n",
      "Training epochs #7430000:   Batch Loss = 4.797090, Accuracy = 0.748000025749\n",
      "Performance on test set: Batch Loss = 4.67910051346, Accuracy = 0.782999992371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #7432000:   Batch Loss = 4.694532, Accuracy = 0.779000043869\n",
      "Performance on test set: Batch Loss = 4.77362442017, Accuracy = 0.759000062943\n",
      "Training epochs #7434000:   Batch Loss = 4.696833, Accuracy = 0.774999976158\n",
      "Performance on test set: Batch Loss = 4.75924062729, Accuracy = 0.758000016212\n",
      "Training epochs #7436000:   Batch Loss = 4.737223, Accuracy = 0.76800006628\n",
      "Performance on test set: Batch Loss = 4.67320346832, Accuracy = 0.784000039101\n",
      "Training epochs #7438000:   Batch Loss = 4.695737, Accuracy = 0.773999989033\n",
      "Performance on test set: Batch Loss = 4.66166639328, Accuracy = 0.775000035763\n",
      "Training epochs #7440000:   Batch Loss = 4.705406, Accuracy = 0.766999959946\n",
      "Performance on test set: Batch Loss = 4.74890041351, Accuracy = 0.744000017643\n",
      "Training epochs #7442000:   Batch Loss = 4.687757, Accuracy = 0.776000082493\n",
      "Performance on test set: Batch Loss = 4.69691324234, Accuracy = 0.76700001955\n",
      "Training epochs #7444000:   Batch Loss = 4.658992, Accuracy = 0.77999997139\n",
      "Performance on test set: Batch Loss = 4.7078332901, Accuracy = 0.769999980927\n",
      "Training epochs #7446000:   Batch Loss = 4.734386, Accuracy = 0.740000069141\n",
      "Performance on test set: Batch Loss = 4.6624622345, Accuracy = 0.789000034332\n",
      "Training epochs #7448000:   Batch Loss = 4.725661, Accuracy = 0.787000000477\n",
      "Performance on test set: Batch Loss = 4.70987701416, Accuracy = 0.794000029564\n",
      "Training epochs #7450000:   Batch Loss = 4.691849, Accuracy = 0.802000045776\n",
      "Performance on test set: Batch Loss = 4.64798927307, Accuracy = 0.819000065327\n",
      "Training epochs #7452000:   Batch Loss = 4.702060, Accuracy = 0.797999978065\n",
      "Performance on test set: Batch Loss = 4.72698163986, Accuracy = 0.789999961853\n",
      "Training epochs #7454000:   Batch Loss = 4.696677, Accuracy = 0.797000050545\n",
      "Performance on test set: Batch Loss = 4.71334981918, Accuracy = 0.796999931335\n",
      "Training epochs #7456000:   Batch Loss = 4.717018, Accuracy = 0.793999910355\n",
      "Performance on test set: Batch Loss = 4.63004255295, Accuracy = 0.804999947548\n",
      "Training epochs #7458000:   Batch Loss = 4.708943, Accuracy = 0.78100001812\n",
      "Performance on test set: Batch Loss = 4.62824964523, Accuracy = 0.817999958992\n",
      "Training epochs #7460000:   Batch Loss = 4.648509, Accuracy = 0.808000028133\n",
      "Performance on test set: Batch Loss = 4.70094680786, Accuracy = 0.786000013351\n",
      "Training epochs #7462000:   Batch Loss = 4.752927, Accuracy = 0.762000024319\n",
      "Performance on test set: Batch Loss = 4.6565861702, Accuracy = 0.810999929905\n",
      "Training epochs #7464000:   Batch Loss = 4.625849, Accuracy = 0.81400001049\n",
      "Performance on test set: Batch Loss = 5.83078289032, Accuracy = 0.192000001669\n",
      "Training epochs #7466000:   Batch Loss = 5.627084, Accuracy = 0.143000006676\n",
      "Performance on test set: Batch Loss = 4.68795919418, Accuracy = 0.768000006676\n",
      "Training epochs #7468000:   Batch Loss = 4.853484, Accuracy = 0.740000069141\n",
      "Performance on test set: Batch Loss = 4.950548172, Accuracy = 0.750999927521\n",
      "Training epochs #7470000:   Batch Loss = 4.929470, Accuracy = 0.755999982357\n",
      "Performance on test set: Batch Loss = 4.80086946487, Accuracy = 0.777999997139\n",
      "Training epochs #7472000:   Batch Loss = 4.856899, Accuracy = 0.753000020981\n",
      "Performance on test set: Batch Loss = 4.84957122803, Accuracy = 0.74899995327\n",
      "Training epochs #7474000:   Batch Loss = 4.772898, Accuracy = 0.763000011444\n",
      "Performance on test set: Batch Loss = 4.8103222847, Accuracy = 0.74899995327\n",
      "Training epochs #7476000:   Batch Loss = 4.779031, Accuracy = 0.773999989033\n",
      "Performance on test set: Batch Loss = 4.77378988266, Accuracy = 0.791000068188\n",
      "Training epochs #7478000:   Batch Loss = 4.787599, Accuracy = 0.76599997282\n",
      "Performance on test set: Batch Loss = 4.72519111633, Accuracy = 0.844999909401\n",
      "Training epochs #7480000:   Batch Loss = 4.688693, Accuracy = 0.828999996185\n",
      "Performance on test set: Batch Loss = 4.73455476761, Accuracy = 0.793000042439\n",
      "Training epochs #7482000:   Batch Loss = 4.679760, Accuracy = 0.805999994278\n",
      "Performance on test set: Batch Loss = 5.50865650177, Accuracy = 0.130999997258\n",
      "Training epochs #7484000:   Batch Loss = 5.346310, Accuracy = 0.775999963284\n",
      "Performance on test set: Batch Loss = 5.20017433167, Accuracy = 0.770000040531\n",
      "Training epochs #7486000:   Batch Loss = 5.085979, Accuracy = 0.769000053406\n",
      "Performance on test set: Batch Loss = 5.0322766304, Accuracy = 0.768000006676\n",
      "Training epochs #7488000:   Batch Loss = 5.135526, Accuracy = 0.741999983788\n",
      "Performance on test set: Batch Loss = 5.14921188354, Accuracy = 0.750000059605\n",
      "Training epochs #7490000:   Batch Loss = 5.123544, Accuracy = 0.759999990463\n",
      "Performance on test set: Batch Loss = 5.0681643486, Accuracy = 0.774999916553\n",
      "Training epochs #7492000:   Batch Loss = 5.182473, Accuracy = 0.740999996662\n",
      "Performance on test set: Batch Loss = 5.18844509125, Accuracy = 0.627000033855\n",
      "Training epochs #7494000:   Batch Loss = 5.107690, Accuracy = 0.753999948502\n",
      "Performance on test set: Batch Loss = 5.02028369904, Accuracy = 0.743000030518\n",
      "Training epochs #7496000:   Batch Loss = 4.881534, Accuracy = 0.763000011444\n",
      "Performance on test set: Batch Loss = 4.82393932343, Accuracy = 0.78100001812\n",
      "Training epochs #7498000:   Batch Loss = 4.829327, Accuracy = 0.773000001907\n",
      "Performance on test set: Batch Loss = 4.84145355225, Accuracy = 0.767999947071\n",
      "Training epochs #7500000:   Batch Loss = 4.864699, Accuracy = 0.759999990463\n",
      "Performance on test set: Batch Loss = 4.92570972443, Accuracy = 0.742000043392\n",
      "Training epochs #7502000:   Batch Loss = 4.813173, Accuracy = 0.791000008583\n",
      "Performance on test set: Batch Loss = 4.8588013649, Accuracy = 0.760000050068\n",
      "Training epochs #7504000:   Batch Loss = 4.858234, Accuracy = 0.745000004768\n",
      "Performance on test set: Batch Loss = 4.84478759766, Accuracy = 0.763999938965\n",
      "Training epochs #7506000:   Batch Loss = 4.846437, Accuracy = 0.758000016212\n",
      "Performance on test set: Batch Loss = 4.79736661911, Accuracy = 0.769999980927\n",
      "Training epochs #7508000:   Batch Loss = 4.802155, Accuracy = 0.770000040531\n",
      "Performance on test set: Batch Loss = 4.84877252579, Accuracy = 0.750999927521\n",
      "Training epochs #7510000:   Batch Loss = 4.886999, Accuracy = 0.739000082016\n",
      "Performance on test set: Batch Loss = 4.75792837143, Accuracy = 0.780000030994\n",
      "Training epochs #7512000:   Batch Loss = 4.771043, Accuracy = 0.764999985695\n",
      "Performance on test set: Batch Loss = 4.82547426224, Accuracy = 0.749999940395\n",
      "Training epochs #7514000:   Batch Loss = 4.756986, Accuracy = 0.769999980927\n",
      "Performance on test set: Batch Loss = 4.8251786232, Accuracy = 0.74899995327\n",
      "Training epochs #7516000:   Batch Loss = 4.798005, Accuracy = 0.758000016212\n",
      "Performance on test set: Batch Loss = 4.7281537056, Accuracy = 0.780000090599\n",
      "Training epochs #7518000:   Batch Loss = 4.743237, Accuracy = 0.766000032425\n",
      "Performance on test set: Batch Loss = 4.71897792816, Accuracy = 0.768999993801\n",
      "Training epochs #7520000:   Batch Loss = 4.765342, Accuracy = 0.754000008106\n",
      "Performance on test set: Batch Loss = 4.78578519821, Accuracy = 0.743000030518\n",
      "Training epochs #7522000:   Batch Loss = 4.732657, Accuracy = 0.76700001955\n",
      "Performance on test set: Batch Loss = 4.74548006058, Accuracy = 0.758999943733\n",
      "Training epochs #7524000:   Batch Loss = 4.689258, Accuracy = 0.776999950409\n",
      "Performance on test set: Batch Loss = 4.73849868774, Accuracy = 0.763000011444\n",
      "Training epochs #7526000:   Batch Loss = 4.767457, Accuracy = 0.728000044823\n",
      "Performance on test set: Batch Loss = 4.69058895111, Accuracy = 0.768000006676\n",
      "Training epochs #7528000:   Batch Loss = 4.741951, Accuracy = 0.741999983788\n",
      "Performance on test set: Batch Loss = 4.72390031815, Accuracy = 0.746999979019\n",
      "Training epochs #7530000:   Batch Loss = 4.701584, Accuracy = 0.757999956608\n",
      "Performance on test set: Batch Loss = 4.65266942978, Accuracy = 0.772000014782\n",
      "Training epochs #7532000:   Batch Loss = 4.681735, Accuracy = 0.755999982357\n",
      "Performance on test set: Batch Loss = 4.71245193481, Accuracy = 0.745000004768\n",
      "Training epochs #7534000:   Batch Loss = 4.681063, Accuracy = 0.745000004768\n",
      "Performance on test set: Batch Loss = 4.68812990189, Accuracy = 0.762000024319\n",
      "Training epochs #7536000:   Batch Loss = 4.689899, Accuracy = 0.752999961376\n",
      "Performance on test set: Batch Loss = 4.60523319244, Accuracy = 0.793999969959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #7538000:   Batch Loss = 4.669987, Accuracy = 0.754999995232\n",
      "Performance on test set: Batch Loss = 4.5823469162, Accuracy = 0.78100001812\n",
      "Training epochs #7540000:   Batch Loss = 4.617056, Accuracy = 0.774000048637\n",
      "Performance on test set: Batch Loss = 4.64809465408, Accuracy = 0.759000003338\n",
      "Training epochs #7542000:   Batch Loss = 4.632656, Accuracy = 0.764000058174\n",
      "Performance on test set: Batch Loss = 4.59956407547, Accuracy = 0.766999959946\n",
      "Training epochs #7544000:   Batch Loss = 4.565088, Accuracy = 0.773000061512\n",
      "Performance on test set: Batch Loss = 4.60133838654, Accuracy = 0.79699999094\n",
      "Training epochs #7546000:   Batch Loss = 4.645215, Accuracy = 0.791999995708\n",
      "Performance on test set: Batch Loss = 4.54420471191, Accuracy = 0.81400001049\n",
      "Training epochs #7548000:   Batch Loss = 4.591787, Accuracy = 0.782000005245\n",
      "Performance on test set: Batch Loss = 4.58322668076, Accuracy = 0.802000045776\n",
      "Training epochs #7550000:   Batch Loss = 4.545873, Accuracy = 0.833999991417\n",
      "Performance on test set: Batch Loss = 4.52124071121, Accuracy = 0.826000034809\n",
      "Training epochs #7552000:   Batch Loss = 4.542751, Accuracy = 0.81400001049\n",
      "Performance on test set: Batch Loss = 4.58155345917, Accuracy = 0.800999999046\n",
      "Training epochs #7554000:   Batch Loss = 4.534122, Accuracy = 0.819000065327\n",
      "Performance on test set: Batch Loss = 4.55730009079, Accuracy = 0.81300008297\n",
      "Training epochs #7556000:   Batch Loss = 4.526034, Accuracy = 0.822000026703\n",
      "Performance on test set: Batch Loss = 4.49687242508, Accuracy = 0.835000038147\n",
      "Training epochs #7558000:   Batch Loss = 4.519164, Accuracy = 0.822999954224\n",
      "Performance on test set: Batch Loss = 4.47160768509, Accuracy = 0.842999994755\n",
      "Training epochs #7560000:   Batch Loss = 4.446017, Accuracy = 0.845000088215\n",
      "Performance on test set: Batch Loss = 4.53073596954, Accuracy = 0.803999960423\n",
      "Training epochs #7562000:   Batch Loss = 4.506049, Accuracy = 0.824000060558\n",
      "Performance on test set: Batch Loss = 4.53277683258, Accuracy = 0.837000012398\n",
      "Training epochs #7564000:   Batch Loss = 4.677062, Accuracy = 0.787000060081\n",
      "Performance on test set: Batch Loss = 4.74488210678, Accuracy = 0.782000005245\n",
      "Training epochs #7566000:   Batch Loss = 4.715658, Accuracy = 0.788000047207\n",
      "Performance on test set: Batch Loss = 4.64921236038, Accuracy = 0.788999974728\n",
      "Training epochs #7568000:   Batch Loss = 4.591868, Accuracy = 0.768999993801\n",
      "Performance on test set: Batch Loss = 5.02754449844, Accuracy = 0.806999981403\n",
      "Training epochs #7570000:   Batch Loss = 4.932008, Accuracy = 0.804000020027\n",
      "Performance on test set: Batch Loss = 4.53317832947, Accuracy = 0.784999966621\n",
      "Training epochs #7572000:   Batch Loss = 4.819811, Accuracy = 0.761000037193\n",
      "Performance on test set: Batch Loss = 4.81579494476, Accuracy = 0.754999995232\n",
      "Training epochs #7574000:   Batch Loss = 4.823176, Accuracy = 0.766000032425\n",
      "Performance on test set: Batch Loss = 4.87919902802, Accuracy = 0.768999993801\n",
      "Training epochs #7576000:   Batch Loss = 4.777801, Accuracy = 0.78200006485\n",
      "Performance on test set: Batch Loss = 4.70636463165, Accuracy = 0.802999973297\n",
      "Training epochs #7578000:   Batch Loss = 4.763508, Accuracy = 0.792000055313\n",
      "Performance on test set: Batch Loss = 4.75390148163, Accuracy = 0.783999979496\n",
      "Training epochs #7580000:   Batch Loss = 4.741614, Accuracy = 0.782999992371\n",
      "Performance on test set: Batch Loss = 4.7270116806, Accuracy = 0.756000041962\n",
      "Training epochs #7582000:   Batch Loss = 4.586088, Accuracy = 0.80999994278\n",
      "Performance on test set: Batch Loss = 4.58037900925, Accuracy = 0.768999993801\n",
      "Training epochs #7584000:   Batch Loss = 4.670357, Accuracy = 0.762999951839\n",
      "Performance on test set: Batch Loss = 4.63040924072, Accuracy = 0.773000001907\n",
      "Training epochs #7586000:   Batch Loss = 4.745604, Accuracy = 0.783000051975\n",
      "Performance on test set: Batch Loss = 4.65726613998, Accuracy = 0.793999969959\n",
      "Training epochs #7588000:   Batch Loss = 4.688457, Accuracy = 0.777000069618\n",
      "Performance on test set: Batch Loss = 4.73134326935, Accuracy = 0.759000003338\n",
      "Training epochs #7590000:   Batch Loss = 4.816495, Accuracy = 0.746000051498\n",
      "Performance on test set: Batch Loss = 4.68138313293, Accuracy = 0.792000055313\n",
      "Training epochs #7592000:   Batch Loss = 4.734769, Accuracy = 0.774999976158\n",
      "Performance on test set: Batch Loss = 4.72800540924, Accuracy = 0.755999982357\n",
      "Training epochs #7594000:   Batch Loss = 4.638333, Accuracy = 0.783999979496\n",
      "Performance on test set: Batch Loss = 4.74269390106, Accuracy = 0.754000008106\n",
      "Training epochs #7596000:   Batch Loss = 4.713706, Accuracy = 0.770000040531\n",
      "Performance on test set: Batch Loss = 4.63045835495, Accuracy = 0.794000029564\n",
      "Training epochs #7598000:   Batch Loss = 4.646869, Accuracy = 0.775999963284\n",
      "Performance on test set: Batch Loss = 4.66240549088, Accuracy = 0.775999963284\n",
      "Training epochs #7600000:   Batch Loss = 4.726500, Accuracy = 0.763000011444\n",
      "Performance on test set: Batch Loss = 4.71061849594, Accuracy = 0.754000067711\n",
      "Training epochs #7602000:   Batch Loss = 4.644025, Accuracy = 0.774999976158\n",
      "Performance on test set: Batch Loss = 4.68452262878, Accuracy = 0.768000006676\n",
      "Training epochs #7604000:   Batch Loss = 4.609666, Accuracy = 0.788000047207\n",
      "Performance on test set: Batch Loss = 4.69010877609, Accuracy = 0.772000014782\n",
      "Training epochs #7606000:   Batch Loss = 4.699092, Accuracy = 0.743999958038\n",
      "Performance on test set: Batch Loss = 4.60450410843, Accuracy = 0.778999984264\n",
      "Training epochs #7608000:   Batch Loss = 4.671349, Accuracy = 0.756000041962\n",
      "Performance on test set: Batch Loss = 4.6365237236, Accuracy = 0.763000071049\n",
      "Training epochs #7610000:   Batch Loss = 4.643782, Accuracy = 0.774000048637\n",
      "Performance on test set: Batch Loss = 4.56921100616, Accuracy = 0.791000008583\n",
      "Training epochs #7612000:   Batch Loss = 4.600361, Accuracy = 0.764999985695\n",
      "Performance on test set: Batch Loss = 4.60776567459, Accuracy = 0.759000003338\n",
      "Training epochs #7614000:   Batch Loss = 4.629725, Accuracy = 0.752000033855\n",
      "Performance on test set: Batch Loss = 4.61756801605, Accuracy = 0.754000008106\n",
      "Training epochs #7616000:   Batch Loss = 4.630690, Accuracy = 0.747999966145\n",
      "Performance on test set: Batch Loss = 4.54300117493, Accuracy = 0.791000008583\n",
      "Training epochs #7618000:   Batch Loss = 4.637534, Accuracy = 0.749000012875\n",
      "Performance on test set: Batch Loss = 4.55758571625, Accuracy = 0.773999989033\n",
      "Training epochs #7620000:   Batch Loss = 4.590476, Accuracy = 0.777999997139\n",
      "Performance on test set: Batch Loss = 4.5943145752, Accuracy = 0.753999948502\n",
      "Training epochs #7622000:   Batch Loss = 4.603309, Accuracy = 0.764000058174\n",
      "Performance on test set: Batch Loss = 4.57322311401, Accuracy = 0.772000074387\n",
      "Training epochs #7624000:   Batch Loss = 4.548850, Accuracy = 0.771000027657\n",
      "Performance on test set: Batch Loss = 4.59473609924, Accuracy = 0.788999974728\n",
      "Training epochs #7626000:   Batch Loss = 4.602355, Accuracy = 0.773000121117\n",
      "Performance on test set: Batch Loss = 4.51583957672, Accuracy = 0.799000024796\n",
      "Training epochs #7628000:   Batch Loss = 4.592528, Accuracy = 0.776000022888\n",
      "Performance on test set: Batch Loss = 4.54540634155, Accuracy = 0.783999979496\n",
      "Training epochs #7630000:   Batch Loss = 4.527759, Accuracy = 0.791000008583\n",
      "Performance on test set: Batch Loss = 4.49459981918, Accuracy = 0.791000068188\n",
      "Training epochs #7632000:   Batch Loss = 4.526563, Accuracy = 0.783000051975\n",
      "Performance on test set: Batch Loss = 4.53043699265, Accuracy = 0.777999997139\n",
      "Training epochs #7634000:   Batch Loss = 4.498442, Accuracy = 0.784000039101\n",
      "Performance on test set: Batch Loss = 4.52971124649, Accuracy = 0.799000024796\n",
      "Training epochs #7636000:   Batch Loss = 4.488876, Accuracy = 0.787999987602\n",
      "Performance on test set: Batch Loss = 4.46276092529, Accuracy = 0.825999975204\n",
      "Training epochs #7638000:   Batch Loss = 4.490283, Accuracy = 0.81099998951\n",
      "Performance on test set: Batch Loss = 4.46185970306, Accuracy = 0.786000013351\n",
      "Training epochs #7640000:   Batch Loss = 4.428740, Accuracy = 0.818000018597\n",
      "Performance on test set: Batch Loss = 4.50016593933, Accuracy = 0.796000063419\n",
      "Training epochs #7642000:   Batch Loss = 4.466238, Accuracy = 0.825000047684\n",
      "Performance on test set: Batch Loss = 4.47069358826, Accuracy = 0.811999976635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #7644000:   Batch Loss = 4.493610, Accuracy = 0.790000021458\n",
      "Performance on test set: Batch Loss = 4.49621200562, Accuracy = 0.782999992371\n",
      "Training epochs #7646000:   Batch Loss = 4.427670, Accuracy = 0.820999979973\n",
      "Performance on test set: Batch Loss = 4.4212436676, Accuracy = 0.822000086308\n",
      "Training epochs #7648000:   Batch Loss = 4.461497, Accuracy = 0.792999982834\n",
      "Performance on test set: Batch Loss = 4.45290184021, Accuracy = 0.803999960423\n",
      "Training epochs #7650000:   Batch Loss = 4.439020, Accuracy = 0.852000057697\n",
      "Performance on test set: Batch Loss = 4.40200567245, Accuracy = 0.852999985218\n",
      "Training epochs #7652000:   Batch Loss = 4.441748, Accuracy = 0.854000031948\n",
      "Performance on test set: Batch Loss = 4.45147275925, Accuracy = 0.859000086784\n",
      "Training epochs #7654000:   Batch Loss = 4.435951, Accuracy = 0.832000017166\n",
      "Performance on test set: Batch Loss = 4.43980360031, Accuracy = 0.828999996185\n",
      "Training epochs #7656000:   Batch Loss = 4.394161, Accuracy = 0.864000022411\n",
      "Performance on test set: Batch Loss = 4.39628982544, Accuracy = 0.855000019073\n",
      "Training epochs #7658000:   Batch Loss = 4.403670, Accuracy = 0.855000078678\n",
      "Performance on test set: Batch Loss = 4.37759017944, Accuracy = 0.869000017643\n",
      "Training epochs #7660000:   Batch Loss = 4.389844, Accuracy = 0.837999999523\n",
      "Performance on test set: Batch Loss = 4.42076873779, Accuracy = 0.842000007629\n",
      "Training epochs #7662000:   Batch Loss = 4.342156, Accuracy = 0.854000031948\n",
      "Performance on test set: Batch Loss = 4.40084648132, Accuracy = 0.860000014305\n",
      "Training epochs #7664000:   Batch Loss = 4.413750, Accuracy = 0.84500002861\n",
      "Performance on test set: Batch Loss = 4.45993232727, Accuracy = 0.844000041485\n",
      "Training epochs #7666000:   Batch Loss = 4.419711, Accuracy = 0.820999979973\n",
      "Performance on test set: Batch Loss = 4.36848449707, Accuracy = 0.846000015736\n",
      "Training epochs #7668000:   Batch Loss = 4.365967, Accuracy = 0.82900005579\n",
      "Performance on test set: Batch Loss = 4.39558982849, Accuracy = 0.851999938488\n",
      "Training epochs #7670000:   Batch Loss = 4.453057, Accuracy = 0.81099998951\n",
      "Performance on test set: Batch Loss = 4.49132585526, Accuracy = 0.828000068665\n",
      "Training epochs #7672000:   Batch Loss = 5.676417, Accuracy = 0.217000007629\n",
      "Performance on test set: Batch Loss = 5.6956987381, Accuracy = 0.163000002503\n",
      "Training epochs #7674000:   Batch Loss = 5.412741, Accuracy = 0.77999997139\n",
      "Performance on test set: Batch Loss = 5.15968799591, Accuracy = 0.746999979019\n",
      "Training epochs #7676000:   Batch Loss = 5.074930, Accuracy = 0.759000003338\n",
      "Performance on test set: Batch Loss = 4.88877534866, Accuracy = 0.787000060081\n",
      "Training epochs #7678000:   Batch Loss = 4.974292, Accuracy = 0.763999998569\n",
      "Performance on test set: Batch Loss = 4.88801240921, Accuracy = 0.776000022888\n",
      "Training epochs #7680000:   Batch Loss = 5.649185, Accuracy = 0.15000000596\n",
      "Performance on test set: Batch Loss = 5.1817574501, Accuracy = 0.740000009537\n",
      "Training epochs #7682000:   Batch Loss = 5.636624, Accuracy = 0.658999979496\n",
      "Performance on test set: Batch Loss = 5.72178936005, Accuracy = 0.663000047207\n",
      "Training epochs #7684000:   Batch Loss = 5.831608, Accuracy = 0.649999976158\n",
      "Performance on test set: Batch Loss = 5.87161445618, Accuracy = 0.638999998569\n",
      "Training epochs #7686000:   Batch Loss = 5.856229, Accuracy = 0.620000004768\n",
      "Performance on test set: Batch Loss = 5.54709243774, Accuracy = 0.668000042439\n",
      "Training epochs #7688000:   Batch Loss = 5.661377, Accuracy = 0.616999983788\n",
      "Performance on test set: Batch Loss = 5.43874645233, Accuracy = 0.638000011444\n",
      "Training epochs #7690000:   Batch Loss = 5.296140, Accuracy = 0.651999950409\n",
      "Performance on test set: Batch Loss = 5.18329763412, Accuracy = 0.660000026226\n",
      "Training epochs #7692000:   Batch Loss = 5.234973, Accuracy = 0.64099997282\n",
      "Performance on test set: Batch Loss = 5.2859287262, Accuracy = 0.624999940395\n",
      "Training epochs #7694000:   Batch Loss = 5.279616, Accuracy = 0.633999943733\n",
      "Performance on test set: Batch Loss = 5.30669689178, Accuracy = 0.638999998569\n",
      "Training epochs #7696000:   Batch Loss = 5.309454, Accuracy = 0.634999990463\n",
      "Performance on test set: Batch Loss = 5.26321649551, Accuracy = 0.661000013351\n",
      "Training epochs #7698000:   Batch Loss = 5.272145, Accuracy = 0.628000020981\n",
      "Performance on test set: Batch Loss = 5.18355226517, Accuracy = 0.658999979496\n",
      "Training epochs #7700000:   Batch Loss = 5.156506, Accuracy = 0.648000001907\n",
      "Performance on test set: Batch Loss = 5.1496758461, Accuracy = 0.625999987125\n",
      "Training epochs #7702000:   Batch Loss = 5.120569, Accuracy = 0.633000016212\n",
      "Performance on test set: Batch Loss = 5.05504798889, Accuracy = 0.659999966621\n",
      "Training epochs #7704000:   Batch Loss = 5.075368, Accuracy = 0.639999985695\n",
      "Performance on test set: Batch Loss = 5.09962034225, Accuracy = 0.638999998569\n",
      "Training epochs #7706000:   Batch Loss = 5.134098, Accuracy = 0.637000024319\n",
      "Performance on test set: Batch Loss = 5.04982280731, Accuracy = 0.667999982834\n",
      "Training epochs #7708000:   Batch Loss = 5.136973, Accuracy = 0.637000024319\n",
      "Performance on test set: Batch Loss = 5.14498281479, Accuracy = 0.634999990463\n",
      "Training epochs #7710000:   Batch Loss = 5.122544, Accuracy = 0.637999951839\n",
      "Performance on test set: Batch Loss = 5.05526733398, Accuracy = 0.660000026226\n",
      "Training epochs #7712000:   Batch Loss = 5.082584, Accuracy = 0.643999993801\n",
      "Performance on test set: Batch Loss = 5.137758255, Accuracy = 0.625000059605\n",
      "Training epochs #7714000:   Batch Loss = 5.073188, Accuracy = 0.646999955177\n",
      "Performance on test set: Batch Loss = 5.11858940125, Accuracy = 0.637999951839\n",
      "Training epochs #7716000:   Batch Loss = 5.058911, Accuracy = 0.651000082493\n",
      "Performance on test set: Batch Loss = 5.01215362549, Accuracy = 0.657999992371\n",
      "Training epochs #7718000:   Batch Loss = 5.078609, Accuracy = 0.662000060081\n",
      "Performance on test set: Batch Loss = 5.0443687439, Accuracy = 0.657999992371\n",
      "Training epochs #7720000:   Batch Loss = 5.000591, Accuracy = 0.675000011921\n",
      "Performance on test set: Batch Loss = 5.10366249084, Accuracy = 0.628000020981\n",
      "Training epochs #7722000:   Batch Loss = 5.041379, Accuracy = 0.65600001812\n",
      "Performance on test set: Batch Loss = 5.03105926514, Accuracy = 0.659999966621\n",
      "Training epochs #7724000:   Batch Loss = 5.053334, Accuracy = 0.650000035763\n",
      "Performance on test set: Batch Loss = 5.06073522568, Accuracy = 0.638999938965\n",
      "Training epochs #7726000:   Batch Loss = 5.033185, Accuracy = 0.646999955177\n",
      "Performance on test set: Batch Loss = 5.00369501114, Accuracy = 0.667999982834\n",
      "Training epochs #7728000:   Batch Loss = 5.071662, Accuracy = 0.631000041962\n",
      "Performance on test set: Batch Loss = 5.07272338867, Accuracy = 0.637999951839\n",
      "Training epochs #7730000:   Batch Loss = 5.049123, Accuracy = 0.638999998569\n",
      "Performance on test set: Batch Loss = 4.99887228012, Accuracy = 0.661000013351\n",
      "Training epochs #7732000:   Batch Loss = 5.042937, Accuracy = 0.654000043869\n",
      "Performance on test set: Batch Loss = 5.08988285065, Accuracy = 0.625\n",
      "Training epochs #7734000:   Batch Loss = 5.070623, Accuracy = 0.636000037193\n",
      "Performance on test set: Batch Loss = 5.06583356857, Accuracy = 0.638000011444\n",
      "Training epochs #7736000:   Batch Loss = 4.977100, Accuracy = 0.663999974728\n",
      "Performance on test set: Batch Loss = 4.98014450073, Accuracy = 0.659000039101\n",
      "Training epochs #7738000:   Batch Loss = 4.983525, Accuracy = 0.665000021458\n",
      "Performance on test set: Batch Loss = 4.99249458313, Accuracy = 0.658000051975\n",
      "Training epochs #7740000:   Batch Loss = 5.019833, Accuracy = 0.637999951839\n",
      "Performance on test set: Batch Loss = 5.05402755737, Accuracy = 0.629000008106\n",
      "Training epochs #7742000:   Batch Loss = 4.935474, Accuracy = 0.680000007153\n",
      "Performance on test set: Batch Loss = 4.98663806915, Accuracy = 0.660000026226\n",
      "Training epochs #7744000:   Batch Loss = 5.025975, Accuracy = 0.636999964714\n",
      "Performance on test set: Batch Loss = 5.02667188644, Accuracy = 0.638000011444\n",
      "Training epochs #7746000:   Batch Loss = 5.012306, Accuracy = 0.649999976158\n",
      "Performance on test set: Batch Loss = 4.96744441986, Accuracy = 0.667999982834\n",
      "Training epochs #7748000:   Batch Loss = 4.975652, Accuracy = 0.662000000477\n",
      "Performance on test set: Batch Loss = 5.02630090714, Accuracy = 0.637999951839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #7750000:   Batch Loss = 5.053510, Accuracy = 0.629000008106\n",
      "Performance on test set: Batch Loss = 4.95489025116, Accuracy = 0.662000000477\n",
      "Training epochs #7752000:   Batch Loss = 4.965068, Accuracy = 0.636999964714\n",
      "Performance on test set: Batch Loss = 5.04083490372, Accuracy = 0.624000012875\n",
      "Training epochs #7754000:   Batch Loss = 4.944129, Accuracy = 0.671000003815\n",
      "Performance on test set: Batch Loss = 5.01913928986, Accuracy = 0.638000011444\n",
      "Training epochs #7756000:   Batch Loss = 5.012714, Accuracy = 0.621000051498\n",
      "Performance on test set: Batch Loss = 4.9265127182, Accuracy = 0.659000039101\n",
      "Training epochs #7758000:   Batch Loss = 4.949274, Accuracy = 0.666000008583\n",
      "Performance on test set: Batch Loss = 4.94349336624, Accuracy = 0.657999992371\n",
      "Training epochs #7760000:   Batch Loss = 4.968117, Accuracy = 0.646000027657\n",
      "Performance on test set: Batch Loss = 5.00545215607, Accuracy = 0.629999995232\n",
      "Training epochs #7762000:   Batch Loss = 4.945327, Accuracy = 0.660000085831\n",
      "Performance on test set: Batch Loss = 4.94338941574, Accuracy = 0.660999953747\n",
      "Training epochs #7764000:   Batch Loss = 4.932165, Accuracy = 0.650999963284\n",
      "Performance on test set: Batch Loss = 4.97849559784, Accuracy = 0.638000011444\n",
      "Training epochs #7766000:   Batch Loss = 5.013951, Accuracy = 0.618999958038\n",
      "Performance on test set: Batch Loss = 4.91341781616, Accuracy = 0.670000016689\n",
      "Training epochs #7768000:   Batch Loss = 4.989134, Accuracy = 0.615999996662\n",
      "Performance on test set: Batch Loss = 4.96188640594, Accuracy = 0.639999985695\n",
      "Training epochs #7770000:   Batch Loss = 4.926577, Accuracy = 0.651999950409\n",
      "Performance on test set: Batch Loss = 4.86844539642, Accuracy = 0.664000034332\n",
      "Training epochs #7772000:   Batch Loss = 4.876750, Accuracy = 0.661000013351\n",
      "Performance on test set: Batch Loss = 4.92368936539, Accuracy = 0.650000035763\n",
      "Training epochs #7774000:   Batch Loss = 4.921419, Accuracy = 0.653999984264\n",
      "Performance on test set: Batch Loss = 4.89459323883, Accuracy = 0.658999979496\n",
      "Training epochs #7776000:   Batch Loss = 4.900954, Accuracy = 0.650000035763\n",
      "Performance on test set: Batch Loss = 4.8222694397, Accuracy = 0.67099994421\n",
      "Training epochs #7778000:   Batch Loss = 4.903666, Accuracy = 0.649999976158\n",
      "Performance on test set: Batch Loss = 4.83133459091, Accuracy = 0.671999931335\n",
      "Training epochs #7780000:   Batch Loss = 4.838281, Accuracy = 0.659000039101\n",
      "Performance on test set: Batch Loss = 4.87532758713, Accuracy = 0.64099997282\n",
      "Training epochs #7782000:   Batch Loss = 4.862060, Accuracy = 0.649000048637\n",
      "Performance on test set: Batch Loss = 4.82607746124, Accuracy = 0.672999978065\n",
      "Training epochs #7784000:   Batch Loss = 4.832576, Accuracy = 0.659999966621\n",
      "Performance on test set: Batch Loss = 4.85377264023, Accuracy = 0.646999955177\n",
      "Training epochs #7786000:   Batch Loss = 4.862278, Accuracy = 0.643000006676\n",
      "Performance on test set: Batch Loss = 4.7696428299, Accuracy = 0.689000070095\n",
      "Training epochs #7788000:   Batch Loss = 4.846546, Accuracy = 0.743999958038\n",
      "Performance on test set: Batch Loss = 4.81942415237, Accuracy = 0.763000011444\n",
      "Training epochs #7790000:   Batch Loss = 4.767305, Accuracy = 0.764999985695\n",
      "Performance on test set: Batch Loss = 4.75233030319, Accuracy = 0.790000021458\n",
      "Training epochs #7792000:   Batch Loss = 4.751338, Accuracy = 0.76700001955\n",
      "Performance on test set: Batch Loss = 4.78219795227, Accuracy = 0.75\n",
      "Training epochs #7794000:   Batch Loss = 4.728930, Accuracy = 0.766999959946\n",
      "Performance on test set: Batch Loss = 4.76704835892, Accuracy = 0.763999938965\n",
      "Training epochs #7796000:   Batch Loss = 4.715042, Accuracy = 0.788999974728\n",
      "Performance on test set: Batch Loss = 4.67030906677, Accuracy = 0.798999965191\n",
      "Training epochs #7798000:   Batch Loss = 4.727034, Accuracy = 0.773999929428\n",
      "Performance on test set: Batch Loss = 4.69757223129, Accuracy = 0.782000005245\n",
      "Training epochs #7800000:   Batch Loss = 4.647275, Accuracy = 0.796000063419\n",
      "Performance on test set: Batch Loss = 4.71419334412, Accuracy = 0.754999995232\n",
      "Training epochs #7802000:   Batch Loss = 4.639449, Accuracy = 0.787000000477\n",
      "Performance on test set: Batch Loss = 4.66776132584, Accuracy = 0.773000001907\n",
      "Training epochs #7804000:   Batch Loss = 4.653779, Accuracy = 0.78100001812\n",
      "Performance on test set: Batch Loss = 4.71558189392, Accuracy = 0.774000048637\n",
      "Training epochs #7806000:   Batch Loss = 4.651484, Accuracy = 0.786000013351\n",
      "Performance on test set: Batch Loss = 4.74336099625, Accuracy = 0.766000032425\n",
      "Training epochs #7808000:   Batch Loss = 4.866645, Accuracy = 0.754000008106\n",
      "Performance on test set: Batch Loss = 4.8685760498, Accuracy = 0.762000024319\n",
      "Training epochs #7810000:   Batch Loss = 4.818844, Accuracy = 0.762000024319\n",
      "Performance on test set: Batch Loss = 4.73702764511, Accuracy = 0.773999989033\n",
      "Training epochs #7812000:   Batch Loss = 4.817963, Accuracy = 0.741999983788\n",
      "Performance on test set: Batch Loss = 4.83831739426, Accuracy = 0.747000038624\n",
      "Training epochs #7814000:   Batch Loss = 4.801738, Accuracy = 0.760999977589\n",
      "Performance on test set: Batch Loss = 4.82083511353, Accuracy = 0.746999979019\n",
      "Training epochs #7816000:   Batch Loss = 4.690824, Accuracy = 0.769999980927\n",
      "Performance on test set: Batch Loss = 4.68248176575, Accuracy = 0.787000060081\n",
      "Training epochs #7818000:   Batch Loss = 4.675652, Accuracy = 0.790000021458\n",
      "Performance on test set: Batch Loss = 4.67987251282, Accuracy = 0.786000013351\n",
      "Training epochs #7820000:   Batch Loss = 4.712711, Accuracy = 0.775999903679\n",
      "Performance on test set: Batch Loss = 4.74404907227, Accuracy = 0.75\n",
      "Training epochs #7822000:   Batch Loss = 4.613012, Accuracy = 0.79699999094\n",
      "Performance on test set: Batch Loss = 4.67391061783, Accuracy = 0.773000061512\n",
      "Training epochs #7824000:   Batch Loss = 4.671438, Accuracy = 0.759000003338\n",
      "Performance on test set: Batch Loss = 4.6538977623, Accuracy = 0.791999936104\n",
      "Training epochs #7826000:   Batch Loss = 4.670822, Accuracy = 0.807000041008\n",
      "Performance on test set: Batch Loss = 4.61645317078, Accuracy = 0.797000050545\n",
      "Training epochs #7828000:   Batch Loss = 4.598971, Accuracy = 0.787000000477\n",
      "Performance on test set: Batch Loss = 4.64661026001, Accuracy = 0.763999998569\n",
      "Training epochs #7830000:   Batch Loss = 4.703153, Accuracy = 0.75\n",
      "Performance on test set: Batch Loss = 4.57139635086, Accuracy = 0.786999940872\n",
      "Training epochs #7832000:   Batch Loss = 4.559643, Accuracy = 0.81200003624\n",
      "Performance on test set: Batch Loss = 4.67138433456, Accuracy = 0.77999997139\n",
      "Training epochs #7834000:   Batch Loss = 4.582069, Accuracy = 0.817000031471\n",
      "Performance on test set: Batch Loss = 4.62015295029, Accuracy = 0.793000042439\n",
      "Training epochs #7836000:   Batch Loss = 4.604756, Accuracy = 0.776000022888\n",
      "Performance on test set: Batch Loss = 4.52717208862, Accuracy = 0.797000050545\n",
      "Training epochs #7838000:   Batch Loss = 4.517665, Accuracy = 0.7990000844\n",
      "Performance on test set: Batch Loss = 4.4362206459, Accuracy = 0.836000025272\n",
      "Training epochs #7840000:   Batch Loss = 4.489769, Accuracy = 0.820999979973\n",
      "Performance on test set: Batch Loss = 4.49665117264, Accuracy = 0.808000028133\n",
      "Training epochs #7842000:   Batch Loss = 4.432812, Accuracy = 0.828999996185\n",
      "Performance on test set: Batch Loss = 4.45704698563, Accuracy = 0.833000063896\n",
      "Training epochs #7844000:   Batch Loss = 4.423324, Accuracy = 0.835000038147\n",
      "Performance on test set: Batch Loss = 4.43927001953, Accuracy = 0.836000025272\n",
      "Training epochs #7846000:   Batch Loss = 4.413323, Accuracy = 0.817999958992\n",
      "Performance on test set: Batch Loss = 4.368060112, Accuracy = 0.865999996662\n",
      "Training epochs #7848000:   Batch Loss = 4.472714, Accuracy = 0.814999997616\n",
      "Performance on test set: Batch Loss = 4.40215921402, Accuracy = 0.815999984741\n",
      "Training epochs #7850000:   Batch Loss = 4.420671, Accuracy = 0.802000045776\n",
      "Performance on test set: Batch Loss = 4.40878677368, Accuracy = 0.830999970436\n",
      "Training epochs #7852000:   Batch Loss = 4.366982, Accuracy = 0.837000072002\n",
      "Performance on test set: Batch Loss = 4.42044115067, Accuracy = 0.805000007153\n",
      "Training epochs #7854000:   Batch Loss = 4.383266, Accuracy = 0.822999954224\n",
      "Performance on test set: Batch Loss = 4.45961999893, Accuracy = 0.808000028133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #7856000:   Batch Loss = 4.470645, Accuracy = 0.813000023365\n",
      "Performance on test set: Batch Loss = 5.14073753357, Accuracy = 0.180000007153\n",
      "Training epochs #7858000:   Batch Loss = 5.136876, Accuracy = 0.762999951839\n",
      "Performance on test set: Batch Loss = 5.01653957367, Accuracy = 0.795000076294\n",
      "Training epochs #7860000:   Batch Loss = 5.075591, Accuracy = 0.762999951839\n",
      "Performance on test set: Batch Loss = 5.05348396301, Accuracy = 0.740000069141\n",
      "Training epochs #7862000:   Batch Loss = 5.000288, Accuracy = 0.745000064373\n",
      "Performance on test set: Batch Loss = 4.95784711838, Accuracy = 0.664000034332\n",
      "Training epochs #7864000:   Batch Loss = 4.964534, Accuracy = 0.641999959946\n",
      "Performance on test set: Batch Loss = 5.07039880753, Accuracy = 0.638999998569\n",
      "Training epochs #7866000:   Batch Loss = 5.135765, Accuracy = 0.638999998569\n",
      "Performance on test set: Batch Loss = 4.94369506836, Accuracy = 0.669000029564\n",
      "Training epochs #7868000:   Batch Loss = 4.915917, Accuracy = 0.65600001812\n",
      "Performance on test set: Batch Loss = 4.8320775032, Accuracy = 0.659999966621\n",
      "Training epochs #7870000:   Batch Loss = 4.725465, Accuracy = 0.666999995708\n",
      "Performance on test set: Batch Loss = 4.63879013062, Accuracy = 0.678000032902\n",
      "Training epochs #7872000:   Batch Loss = 4.642069, Accuracy = 0.78100001812\n",
      "Performance on test set: Batch Loss = 7.31830692291, Accuracy = 0.177000001073\n",
      "Training epochs #7874000:   Batch Loss = 4.652878, Accuracy = 0.78200006485\n",
      "Performance on test set: Batch Loss = 4.71390199661, Accuracy = 0.763000011444\n",
      "Training epochs #7876000:   Batch Loss = 4.743356, Accuracy = 0.782000005245\n",
      "Performance on test set: Batch Loss = 4.70678949356, Accuracy = 0.799999952316\n",
      "Training epochs #7878000:   Batch Loss = 4.801060, Accuracy = 0.777999997139\n",
      "Performance on test set: Batch Loss = 4.71888971329, Accuracy = 0.784999966621\n",
      "Training epochs #7880000:   Batch Loss = 4.657765, Accuracy = 0.791999936104\n",
      "Performance on test set: Batch Loss = 4.76797962189, Accuracy = 0.753000080585\n",
      "Training epochs #7882000:   Batch Loss = 4.655551, Accuracy = 0.786000013351\n",
      "Performance on test set: Batch Loss = 4.69665288925, Accuracy = 0.774000048637\n",
      "Training epochs #7884000:   Batch Loss = 4.690425, Accuracy = 0.779000043869\n",
      "Performance on test set: Batch Loss = 4.73647785187, Accuracy = 0.774999976158\n",
      "Training epochs #7886000:   Batch Loss = 4.641281, Accuracy = 0.784999966621\n",
      "Performance on test set: Batch Loss = 4.62622356415, Accuracy = 0.789000034332\n",
      "Training epochs #7888000:   Batch Loss = 4.710195, Accuracy = 0.761000037193\n",
      "Performance on test set: Batch Loss = 4.70437812805, Accuracy = 0.761999964714\n",
      "Training epochs #7890000:   Batch Loss = 4.691509, Accuracy = 0.776000022888\n",
      "Performance on test set: Batch Loss = 4.64055395126, Accuracy = 0.783999919891\n",
      "Training epochs #7892000:   Batch Loss = 4.699138, Accuracy = 0.75100004673\n",
      "Performance on test set: Batch Loss = 4.68035507202, Accuracy = 0.767999947071\n",
      "Training epochs #7894000:   Batch Loss = 4.618120, Accuracy = 0.778999984264\n",
      "Performance on test set: Batch Loss = 4.59957361221, Accuracy = 0.763000011444\n",
      "Training epochs #7896000:   Batch Loss = 4.549455, Accuracy = 0.785000026226\n",
      "Performance on test set: Batch Loss = 4.50168323517, Accuracy = 0.81400001049\n",
      "Training epochs #7898000:   Batch Loss = 4.684477, Accuracy = 0.792999982834\n",
      "Performance on test set: Batch Loss = 4.65455961227, Accuracy = 0.791000008583\n",
      "Training epochs #7900000:   Batch Loss = 4.658577, Accuracy = 0.770000040531\n",
      "Performance on test set: Batch Loss = 4.63598537445, Accuracy = 0.752000033855\n",
      "Training epochs #7902000:   Batch Loss = 4.512371, Accuracy = 0.804000020027\n",
      "Performance on test set: Batch Loss = 4.53246545792, Accuracy = 0.768999993801\n",
      "Training epochs #7904000:   Batch Loss = 4.496756, Accuracy = 0.762000024319\n",
      "Performance on test set: Batch Loss = 4.48202610016, Accuracy = 0.766999959946\n",
      "Training epochs #7906000:   Batch Loss = 4.475574, Accuracy = 0.761000037193\n",
      "Performance on test set: Batch Loss = 4.48449516296, Accuracy = 0.771000027657\n",
      "Training epochs #7908000:   Batch Loss = 4.434125, Accuracy = 0.780000030994\n",
      "Performance on test set: Batch Loss = 4.48544120789, Accuracy = 0.774000048637\n",
      "Training epochs #7910000:   Batch Loss = 4.467262, Accuracy = 0.753999948502\n",
      "Performance on test set: Batch Loss = 4.36870193481, Accuracy = 0.778999984264\n",
      "Training epochs #7912000:   Batch Loss = 4.358910, Accuracy = 0.783999919891\n",
      "Performance on test set: Batch Loss = 4.42042732239, Accuracy = 0.758000016212\n",
      "Training epochs #7914000:   Batch Loss = 4.330218, Accuracy = 0.803000032902\n",
      "Performance on test set: Batch Loss = 4.36976575851, Accuracy = 0.804000020027\n",
      "Training epochs #7916000:   Batch Loss = 4.369193, Accuracy = 0.813000023365\n",
      "Performance on test set: Batch Loss = 4.32153511047, Accuracy = 0.832000017166\n",
      "Training epochs #7918000:   Batch Loss = 4.313347, Accuracy = 0.828000068665\n",
      "Performance on test set: Batch Loss = 4.31049251556, Accuracy = 0.835000038147\n",
      "Training epochs #7920000:   Batch Loss = 4.353633, Accuracy = 0.825000047684\n",
      "Performance on test set: Batch Loss = 4.35229825974, Accuracy = 0.793000042439\n",
      "Training epochs #7922000:   Batch Loss = 4.285905, Accuracy = 0.824000060558\n",
      "Performance on test set: Batch Loss = 4.33186531067, Accuracy = 0.819000005722\n",
      "Training epochs #7924000:   Batch Loss = 4.316374, Accuracy = 0.827000021935\n",
      "Performance on test set: Batch Loss = 4.3521900177, Accuracy = 0.828999996185\n",
      "Training epochs #7926000:   Batch Loss = 4.329499, Accuracy = 0.808000028133\n",
      "Performance on test set: Batch Loss = 4.30060052872, Accuracy = 0.814000070095\n",
      "Training epochs #7928000:   Batch Loss = 4.360866, Accuracy = 0.804000020027\n",
      "Performance on test set: Batch Loss = 4.34085130692, Accuracy = 0.817000031471\n",
      "Training epochs #7930000:   Batch Loss = 4.360006, Accuracy = 0.819000005722\n",
      "Performance on test set: Batch Loss = 4.2960562706, Accuracy = 0.853000044823\n",
      "Training epochs #7932000:   Batch Loss = 4.320677, Accuracy = 0.856000006199\n",
      "Performance on test set: Batch Loss = 4.36750411987, Accuracy = 0.827000021935\n",
      "Training epochs #7934000:   Batch Loss = 4.322377, Accuracy = 0.854000031948\n",
      "Performance on test set: Batch Loss = 4.32916736603, Accuracy = 0.839999973774\n",
      "Training epochs #7936000:   Batch Loss = 4.350685, Accuracy = 0.842000007629\n",
      "Performance on test set: Batch Loss = 4.30433559418, Accuracy = 0.851999998093\n",
      "Training epochs #7938000:   Batch Loss = 4.359082, Accuracy = 0.836000025272\n",
      "Performance on test set: Batch Loss = 4.27429676056, Accuracy = 0.865999996662\n",
      "Training epochs #7940000:   Batch Loss = 4.315248, Accuracy = 0.851000010967\n",
      "Performance on test set: Batch Loss = 4.32374286652, Accuracy = 0.844000041485\n",
      "Training epochs #7942000:   Batch Loss = 4.350029, Accuracy = 0.835999965668\n",
      "Performance on test set: Batch Loss = 4.30617141724, Accuracy = 0.857000112534\n",
      "Training epochs #7944000:   Batch Loss = 4.280614, Accuracy = 0.861000001431\n",
      "Performance on test set: Batch Loss = 4.33633041382, Accuracy = 0.853000044823\n",
      "Training epochs #7946000:   Batch Loss = 4.367839, Accuracy = 0.825999975204\n",
      "Performance on test set: Batch Loss = 4.57215452194, Accuracy = 0.809000015259\n",
      "Training epochs #7948000:   Batch Loss = 6.328656, Accuracy = 0.169999986887\n",
      "Performance on test set: Batch Loss = 7.15101909637, Accuracy = 0.0860000029206\n",
      "Training epochs #7950000:   Batch Loss = 7.163573, Accuracy = 0.0580000020564\n",
      "Performance on test set: Batch Loss = 6.90357685089, Accuracy = 0.0670000016689\n",
      "Training epochs #7952000:   Batch Loss = 6.911143, Accuracy = 0.641999959946\n",
      "Performance on test set: Batch Loss = 7.25414276123, Accuracy = 0.624000012875\n",
      "Training epochs #7954000:   Batch Loss = 6.437697, Accuracy = 0.644999921322\n",
      "Performance on test set: Batch Loss = 6.32354164124, Accuracy = 0.636000037193\n",
      "Training epochs #7956000:   Batch Loss = 6.328770, Accuracy = 0.650999963284\n",
      "Performance on test set: Batch Loss = 6.19838809967, Accuracy = 0.65600001812\n",
      "Training epochs #7958000:   Batch Loss = 6.008039, Accuracy = 0.660000026226\n",
      "Performance on test set: Batch Loss = 5.98014163971, Accuracy = 0.656999945641\n",
      "Training epochs #7960000:   Batch Loss = 5.806091, Accuracy = 0.671000003815\n",
      "Performance on test set: Batch Loss = 5.92392253876, Accuracy = 0.625000059605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #7962000:   Batch Loss = 5.743430, Accuracy = 0.65499997139\n",
      "Performance on test set: Batch Loss = 5.57839870453, Accuracy = 0.657000005245\n",
      "Training epochs #7964000:   Batch Loss = 5.546608, Accuracy = 0.648000001907\n",
      "Performance on test set: Batch Loss = 5.54580307007, Accuracy = 0.636999964714\n",
      "Training epochs #7966000:   Batch Loss = 5.521396, Accuracy = 0.644999980927\n",
      "Performance on test set: Batch Loss = 5.4007062912, Accuracy = 0.666000008583\n",
      "Training epochs #7968000:   Batch Loss = 5.489649, Accuracy = 0.629999995232\n",
      "Performance on test set: Batch Loss = 5.49661111832, Accuracy = 0.631999969482\n",
      "Training epochs #7970000:   Batch Loss = 5.454928, Accuracy = 0.636999964714\n",
      "Performance on test set: Batch Loss = 5.31521224976, Accuracy = 0.660000026226\n",
      "Training epochs #7972000:   Batch Loss = 5.258943, Accuracy = 0.651999950409\n",
      "Performance on test set: Batch Loss = 5.36824989319, Accuracy = 0.620000004768\n",
      "Training epochs #7974000:   Batch Loss = 5.270001, Accuracy = 0.632000029087\n",
      "Performance on test set: Batch Loss = 5.21504354477, Accuracy = 0.635999977589\n",
      "Training epochs #7976000:   Batch Loss = 5.078164, Accuracy = 0.659999966621\n",
      "Performance on test set: Batch Loss = 5.11488342285, Accuracy = 0.65600001812\n",
      "Training epochs #7978000:   Batch Loss = 5.070204, Accuracy = 0.661000013351\n",
      "Performance on test set: Batch Loss = 5.05301761627, Accuracy = 0.656999945641\n",
      "Training epochs #7980000:   Batch Loss = 5.099734, Accuracy = 0.636999964714\n",
      "Performance on test set: Batch Loss = 5.12996387482, Accuracy = 0.624999940395\n",
      "Training epochs #7982000:   Batch Loss = 4.947714, Accuracy = 0.679000020027\n",
      "Performance on test set: Batch Loss = 5.02091741562, Accuracy = 0.656999945641\n",
      "Training epochs #7984000:   Batch Loss = 5.050321, Accuracy = 0.634000003338\n",
      "Performance on test set: Batch Loss = 5.03769493103, Accuracy = 0.637000024319\n",
      "Training epochs #7986000:   Batch Loss = 5.025748, Accuracy = 0.647000014782\n",
      "Performance on test set: Batch Loss = 4.9824552536, Accuracy = 0.666000008583\n",
      "Training epochs #7988000:   Batch Loss = 4.989727, Accuracy = 0.660999953747\n",
      "Performance on test set: Batch Loss = 5.04902744293, Accuracy = 0.631999969482\n",
      "Training epochs #7990000:   Batch Loss = 5.093641, Accuracy = 0.629999995232\n",
      "Performance on test set: Batch Loss = 4.98609161377, Accuracy = 0.660000026226\n",
      "Training epochs #7992000:   Batch Loss = 4.994801, Accuracy = 0.633999943733\n",
      "Performance on test set: Batch Loss = 5.08310461044, Accuracy = 0.621000051498\n",
      "Training epochs #7994000:   Batch Loss = 4.975688, Accuracy = 0.668999969959\n",
      "Performance on test set: Batch Loss = 5.05527210236, Accuracy = 0.636000037193\n",
      "Training epochs #7996000:   Batch Loss = 5.047904, Accuracy = 0.619000017643\n",
      "Performance on test set: Batch Loss = 4.93965578079, Accuracy = 0.65600001812\n",
      "Training epochs #7998000:   Batch Loss = 4.961409, Accuracy = 0.663999974728\n",
      "Performance on test set: Batch Loss = 4.95428466797, Accuracy = 0.657000005245\n",
      "Training epochs #8000000:   Batch Loss = 4.984452, Accuracy = 0.644999980927\n",
      "Performance on test set: Batch Loss = 5.03770875931, Accuracy = 0.627000033855\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# tracking training performance\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "test_predictions = []\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "\n",
    "# Launch the graph\n",
    "# Note that log_device_placement can be turned ON but will cause console spam with RNNs.\n",
    "sess = tf.InteractiveSession(config=sess_config)\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Perform Training steps with \"batch_size\" amount of example data at each loop\n",
    "step = 1\n",
    "while step * config.batch_size <= config.training_epochs:\n",
    "    batch_xs = extract_batch_size(X_train, step, config.batch_size)\n",
    "    batch_ys = one_hot(extract_batch_size(y_train, step, config.batch_size))\n",
    "\n",
    "    # Fit training using batch data\n",
    "    _, loss, acc = sess.run([optimizer, cost, accuracy],\n",
    "        feed_dict={x: batch_xs, y: batch_ys})\n",
    "\n",
    "    train_losses.append(loss)\n",
    "    train_accuracies.append(acc)  \n",
    "    \n",
    "    batch_xt = extract_batch_size(X_test, step, config.batch_size)\n",
    "    batch_yt = one_hot(extract_batch_size(y_test, step, config.batch_size))\n",
    "    \n",
    "    # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
    "    test_predict, test_loss, accs = sess.run([pred_y, cost, accuracy], \n",
    "                                    feed_dict={x: batch_xt, y: batch_yt})\n",
    "\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(accs)\n",
    "    test_predictions.append(test_predict)\n",
    "\n",
    "    # Evaluate network only at some steps for faster training: \n",
    "    if (step*config.batch_size % config.display_iter == 0) or (step == 1) \\\n",
    "        or (step * config.batch_size > config.training_epochs):\n",
    "\n",
    "        # To not spam console, show training accuracy/loss in this \"if\"\n",
    "        print(\"Training epochs #\" + str(step*config.batch_size) + \\\n",
    "          \":   Batch Loss = \" + \"{:.6f}\".format(loss) + \\\n",
    "          \", Accuracy = {}\".format(acc))\n",
    "\n",
    "        print(\"Performance on test set: \" + \"Batch Loss = {}\".format(test_loss) + \\\n",
    "            \", Accuracy = {}\".format(accs))\n",
    "    step += 1\n",
    "\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# performance visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnOb8P3M1tw1ocZHQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8e6c63f410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8e58120610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#font = {'family': 'Bitstream Vera Sans', 'weight': 'bold', 'size': 12}\n",
    "#matplotlib.rc('font', **font)\n",
    "\n",
    "#width = 12\n",
    "#height = 12\n",
    "#plt.figure(figsize=(width, height))\n",
    "\n",
    "indep_train_axis = np.array(range(config.batch_size, \n",
    "    (len(train_losses)+1)*config.batch_size, config.batch_size))\n",
    "plt.plot(indep_train_axis, np.array(train_losses), \"b--\", label=\"Train losses\")\n",
    "plt.plot(indep_train_axis, np.array(train_accuracies), \"g--\", label=\"Train accuracies\")\n",
    "\n",
    "indep_test_axis = np.append(np.array(range(config.batch_size, \n",
    "    len(test_losses)*config.display_iter, config.display_iter)[:-1]), \n",
    "    [config.training_epochs])\n",
    "\n",
    "plt.plot(indep_test_axis, np.array(test_losses), \"b-\", label=\"Test losses\")\n",
    "plt.plot(indep_test_axis, np.array(test_accuracies), \"g-\", label=\"Test accuracies\")\n",
    "\n",
    "plt.title(\"Training session's progress over iterations\")\n",
    "plt.legend(loc='upper right', shadow=False)\n",
    "plt.ylabel('Training Progress (Loss or Accuracy values)')\n",
    "plt.xlabel('Training iteration')\n",
    "plt.savefig('result/lstm_loss_accuracy.png')\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi-class confusion matrix and metrics plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: Tensor(\"mul_7:0\", shape=(), dtype=float32)%\n",
      "\n",
      "Precision: 41.4356820458%\n",
      "Recall: 42.16%\n",
      "f1_score: 41.7788714039%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[   0    3    3    0    2    3   25    0    0    0    0    0    0]\n",
      " [   0  187   82    0  233   92 1119    5    0    0    0   97    0]\n",
      " [   0   49   18    0   59   28  337    1    0    0    0   14    0]\n",
      " [   0    0    0    0    0    0    1    0    0    0    0    0    0]\n",
      " [   0  290  102    0  268  126 1522    0    0    0    0  121    0]\n",
      " [   0  112   34    0  119   50  594    4    0    0    0   48    0]\n",
      " [   0 1339  565    0 1403  621 7873   11    0    0    0  644    0]\n",
      " [   0   53   31    0   54   21  404    0    0    0    0   32    0]\n",
      " [   0   33   18    0   58   23  262    0    0    0    0   28    0]\n",
      " [   0    1    1    0    5    1   20    0    0    0    0    1    0]\n",
      " [   0    3    1    0    3    0   16    0    0    0    0    2    0]\n",
      " [   0   63   32    0   83   45  461    1    0    0    0   36    0]\n",
      " [   0    1    0    0    0    0    3    0    0    0    0    0    0]]\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     security       0.00      0.00      0.00        36\n",
      "       others       0.09      0.10      0.09      1815\n",
      "    processor       0.02      0.04      0.03       506\n",
      "         disk       0.00      0.00      0.00         1\n",
      "      network       0.12      0.11      0.11      2429\n",
      "         file       0.05      0.05      0.05       961\n",
      "     database       0.62      0.63      0.63     12456\n",
      "      service       0.00      0.00      0.00       595\n",
      "       memory       0.00      0.00      0.00       422\n",
      "communication       0.00      0.00      0.00        29\n",
      "       system       0.00      0.00      0.00        25\n",
      "       driver       0.04      0.05      0.04       721\n",
      "           io       0.00      0.00      0.00         4\n",
      "\n",
      "  avg / total       0.41      0.42      0.42     20000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAEmCAYAAAAA6gkZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCdLMrMAJ0syswAnSzKzgG5gR+R48Ge9tAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8e756d3ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Results\n",
    "predictions = test_predictions[730]\n",
    "for i in range(731,750):\n",
    "    predictions = np.vstack((predictions, test_predictions[i]))\n",
    "predictions = predictions.argmax(1)\n",
    "\n",
    "print(\"Testing Accuracy: {}%\".format(100*accuracy))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Precision: {}%\".format(100*metrics.precision_score(\n",
    "    y_test, predictions, average=\"weighted\")))\n",
    "print(\"Recall: {}%\".format(100*metrics.recall_score(y_test, \n",
    "    predictions, average=\"weighted\")))\n",
    "print(\"f1_score: {}%\".format(100*metrics.f1_score(y_test, \n",
    "    predictions, average=\"weighted\")))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix:\")\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, predictions)\n",
    "print(confusion_matrix)\n",
    "\n",
    "# Plot: \n",
    "## cmap can be changed to many colors, (colormaps.Oranges,OrRd, etc)\n",
    "def plot_CM(cm, title=\"Normalized Confusion Matrix\", cmap=plt.cm.summer):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(fault_label))\n",
    "    plt.xticks(tick_marks, fault_label.values(), rotation=90)\n",
    "    plt.yticks(tick_marks, fault_label.values())\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.show()\n",
    "    \n",
    "print(metrics.classification_report(\n",
    "    y_test, predictions, target_names = list(fault_label.values())))\n",
    "\n",
    "cm = confusion_matrix\n",
    "\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:,np.newaxis]\n",
    "plt.figure()\n",
    "plot_CM(cm_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
