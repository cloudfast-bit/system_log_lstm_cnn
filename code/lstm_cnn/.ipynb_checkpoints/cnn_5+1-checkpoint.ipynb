{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cross_validation, metrics\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 0.4)\n",
    "sess_config = tf.ConfigProto(gpu_options=gpu_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = {'0':'file', '1':'network', '2':'service', '3':'database', '4':'communication', '5':'memory', '6':'driver', \n",
    "    '7':'system', '8':'application', '9':'io', '10':'others', '11':'security', '12':'disk', '13':'processor'}\n",
    "\n",
    "fault_label = {'0':'file', '1':'network', '2':'service', '3':'database', '4':'communication', '5':'memory', '6':'driver', \n",
    "    '7':'system', '8':'application', '9':'io', '10':'others', '11':'security', '12':'disk', '13':'processor'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separating data into 80% training set & 20% test set...\n",
      "Dataset separated.\n",
      "\n",
      "((80000, 10, 14, 1), (80000, 14), (20000, 14))\n"
     ]
    }
   ],
   "source": [
    "def one_hot(y):\n",
    "    y = y.reshape(len(y))\n",
    "    n_values = np.max(y) + 1\n",
    "    return np.eye(n_values)[np.array(y, dtype=np.int32)]  # Returns FLOATS\n",
    "\n",
    "\n",
    "def load_X(X_path):\n",
    "    X_list = []\n",
    "    file = open(X_path, 'r')\n",
    "    # Read dataset from disk, dealing with text files' syntax\n",
    "    X_signal = [np.array(item, dtype=np.float32) for item in [\n",
    "               line.strip().split('\\t') for line in file]]\n",
    "    X_list.append(X_signal)\n",
    "    file.close()\n",
    "    return np.transpose(np.array(X_list), (1, 2, 0))\n",
    "\n",
    "\n",
    "# Load \"y\" (the neural network's training and testing outputs)\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    # Read dataset from disk, dealing with text file's syntax\n",
    "    y_ = np.array([elem for elem in [line.strip().split('\\t') for line in file]], \n",
    "                  dtype=np.int32)\n",
    "    file.close()\n",
    "    # Substract 1 to each output class for friendly 0-based indexing\n",
    "    return one_hot(y_-1)\n",
    "\n",
    "\n",
    "dataset_path = \"data_msg_type/\"\n",
    "X_path = dataset_path + \"semantic_sim.txt\"\n",
    "y_path = dataset_path + \"semantic_label_index.txt\"\n",
    "\n",
    "X = load_X(X_path)\n",
    "y = load_y(y_path)\n",
    "x = X.reshape(len(X), 10, 14, 1)\n",
    "\n",
    "# Separate our training data into test and training.\n",
    "print(\"Separating data into 80% training set & 20% test set...\")\n",
    "train_x, test_x, train_y, test_y = cross_validation.train_test_split(\n",
    "    x, y, test_size=0.2, random_state=33)#add random state here...\n",
    "print(\"Dataset separated.\\n\")\n",
    "print(train_x.shape, train_y.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "training_iters = 8000000\n",
    "batch_size = 1000\n",
    "display_step = 20000\n",
    "\n",
    "# Network Parameters\n",
    "input_height = 10\n",
    "input_width = 14\n",
    "num_channels = 1\n",
    "n_classes = 14\n",
    "#dropout = 0.75 # Dropout, probability to keep units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# network config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases):\n",
    "    # Reshape input picture\n",
    "    x = tf.reshape(x, shape=[-1, 10, 14, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "    print(conv1.shape)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    print(conv2.shape)\n",
    "    \n",
    "    # Convolution Layer\n",
    "    conv3 = conv2d(conv2, weights['wc3'], biases['bc3'])\n",
    "    conv4 = conv2d(conv3, weights['wc4'], biases['bc4'])\n",
    "    conv5 = conv2d(conv4, weights['wc5'], biases['bc5'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv5 = maxpool2d(conv5, k=2)\n",
    "    print(conv3.shape)\n",
    "    print(conv4.shape)\n",
    "    print(conv5.shape)\n",
    "\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv5, [-1, weights['out'].get_shape().as_list()[0]])\n",
    "\n",
    "    # Output, class prediction\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    print(fc1.shape)\n",
    "    return fc1\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 3x4 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([3, 4, 1, 32])),\n",
    "    # 3x4 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([3, 4, 32, 64])),\n",
    "    # 3x4 conv, 64 inputs, 64 outputs\n",
    "    'wc3': tf.Variable(tf.random_normal([3, 4, 64, 64])),\n",
    "    # 3x4 conv, 64 inputs, 32 outputs\n",
    "    'wc4': tf.Variable(tf.random_normal([3, 4, 64, 32])),\n",
    "    # 3x4 conv, 32 inputs, 64 outputs\n",
    "    'wc5': tf.Variable(tf.random_normal([3, 4, 32, 64])),\n",
    "    # fully connected, 2*2*64 inputs, 14outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([2*2*64, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bc3': tf.Variable(tf.random_normal([64])),\n",
    "    'bc4': tf.Variable(tf.random_normal([32])),\n",
    "    'bc5': tf.Variable(tf.random_normal([64])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 5, 7, 32)\n",
      "(?, 3, 4, 64)\n",
      "(?, 3, 4, 64)\n",
      "(?, 3, 4, 32)\n",
      "(?, 2, 2, 64)\n",
      "(?, 14)\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, [None,input_height,input_width,num_channels])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "\n",
    "pred = conv_net(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #1000: Batch Loss = 567624.562500, Accuracy = 0.0010000000475\n",
      "Performance on test set: Training epochs #1000, Batch Loss = 536931.375, Accuracy = 0.00200000009499\n",
      "Training epochs #20000: Batch Loss = 318058.468750, Accuracy = 0.0130000002682\n",
      "Performance on test set: Training epochs #20000, Batch Loss = 323289.03125, Accuracy = 0.00700000021607\n",
      "Training epochs #40000: Batch Loss = 227637.906250, Accuracy = 0.00700000068173\n",
      "Performance on test set: Training epochs #40000, Batch Loss = 232701.3125, Accuracy = 0.00800000037998\n",
      "Training epochs #60000: Batch Loss = 164959.359375, Accuracy = 0.0180000010878\n",
      "Performance on test set: Training epochs #60000, Batch Loss = 160803.9375, Accuracy = 0.00800000037998\n",
      "Training epochs #80000: Batch Loss = 98290.843750, Accuracy = 0.0140000013635\n",
      "Performance on test set: Training epochs #80000, Batch Loss = 98599.8828125, Accuracy = 0.00800000037998\n",
      "Training epochs #100000: Batch Loss = 40789.031250, Accuracy = 0.0120000001043\n",
      "Performance on test set: Training epochs #100000, Batch Loss = 48534.2890625, Accuracy = 0.00900000054389\n",
      "Training epochs #120000: Batch Loss = 16653.615234, Accuracy = 0.108999997377\n",
      "Performance on test set: Training epochs #120000, Batch Loss = 25138.5878906, Accuracy = 0.111000008881\n",
      "Training epochs #140000: Batch Loss = 19662.779297, Accuracy = 0.145999997854\n",
      "Performance on test set: Training epochs #140000, Batch Loss = 18026.6796875, Accuracy = 0.13299998641\n",
      "Training epochs #160000: Batch Loss = 10586.125977, Accuracy = 0.121000006795\n",
      "Performance on test set: Training epochs #160000, Batch Loss = 12903.4042969, Accuracy = 0.131999999285\n",
      "Training epochs #180000: Batch Loss = 6724.235840, Accuracy = 0.112999998033\n",
      "Performance on test set: Training epochs #180000, Batch Loss = 10237.9072266, Accuracy = 0.106000006199\n",
      "Training epochs #200000: Batch Loss = 3597.985840, Accuracy = 0.118999995291\n",
      "Performance on test set: Training epochs #200000, Batch Loss = 7516.43359375, Accuracy = 0.130999997258\n",
      "Training epochs #220000: Batch Loss = 6459.834961, Accuracy = 0.149000003934\n",
      "Performance on test set: Training epochs #220000, Batch Loss = 5205.90869141, Accuracy = 0.137999996543\n",
      "Training epochs #240000: Batch Loss = 2587.818115, Accuracy = 0.122999995947\n",
      "Performance on test set: Training epochs #240000, Batch Loss = 3344.73242188, Accuracy = 0.138999998569\n",
      "Training epochs #260000: Batch Loss = 1871.285645, Accuracy = 0.135000005364\n",
      "Performance on test set: Training epochs #260000, Batch Loss = 2117.11401367, Accuracy = 0.131999984384\n",
      "Training epochs #280000: Batch Loss = 60.358204, Accuracy = 0.119000002742\n",
      "Performance on test set: Training epochs #280000, Batch Loss = 1694.17553711, Accuracy = 0.131999999285\n",
      "Training epochs #300000: Batch Loss = 2067.203613, Accuracy = 0.145000010729\n",
      "Performance on test set: Training epochs #300000, Batch Loss = 1313.47814941, Accuracy = 0.131999999285\n",
      "Training epochs #320000: Batch Loss = 877.189209, Accuracy = 0.12199999392\n",
      "Performance on test set: Training epochs #320000, Batch Loss = 965.056091309, Accuracy = 0.138999983668\n",
      "Training epochs #340000: Batch Loss = 948.044189, Accuracy = 0.138999998569\n",
      "Performance on test set: Training epochs #340000, Batch Loss = 782.626586914, Accuracy = 0.138999998569\n",
      "Training epochs #360000: Batch Loss = 2.739643, Accuracy = 0.122000008821\n",
      "Performance on test set: Training epochs #360000, Batch Loss = 545.917419434, Accuracy = 0.138999983668\n",
      "Training epochs #380000: Batch Loss = 611.589172, Accuracy = 0.148999989033\n",
      "Performance on test set: Training epochs #380000, Batch Loss = 313.782226562, Accuracy = 0.138999998569\n",
      "Training epochs #400000: Batch Loss = 373.201569, Accuracy = 0.12199999392\n",
      "Performance on test set: Training epochs #400000, Batch Loss = 70.7288131714, Accuracy = 0.138999998569\n",
      "Training epochs #420000: Batch Loss = 489.004730, Accuracy = 0.140000000596\n",
      "Performance on test set: Training epochs #420000, Batch Loss = 20.5921897888, Accuracy = 0.141000002623\n",
      "Training epochs #440000: Batch Loss = 2.319731, Accuracy = 0.122000008821\n",
      "Performance on test set: Training epochs #440000, Batch Loss = 20.2711410522, Accuracy = 0.141000002623\n",
      "Training epochs #460000: Batch Loss = 116.518654, Accuracy = 0.151999995112\n",
      "Performance on test set: Training epochs #460000, Batch Loss = 20.0498123169, Accuracy = 0.140999987721\n",
      "Training epochs #480000: Batch Loss = 289.792206, Accuracy = 0.122999988496\n",
      "Performance on test set: Training epochs #480000, Batch Loss = 19.7883052826, Accuracy = 0.141000002623\n",
      "Training epochs #500000: Batch Loss = 430.579620, Accuracy = 0.140000015497\n",
      "Performance on test set: Training epochs #500000, Batch Loss = 19.3079166412, Accuracy = 0.140999987721\n",
      "Training epochs #520000: Batch Loss = 2.319731, Accuracy = 0.122000001371\n",
      "Performance on test set: Training epochs #520000, Batch Loss = 14.3481626511, Accuracy = 0.140999987721\n",
      "Training epochs #540000: Batch Loss = 103.316742, Accuracy = 0.152000010014\n",
      "Performance on test set: Training epochs #540000, Batch Loss = 17.8315296173, Accuracy = 0.140999987721\n",
      "Training epochs #560000: Batch Loss = 266.585022, Accuracy = 0.123000003397\n",
      "Performance on test set: Training epochs #560000, Batch Loss = 13.2431402206, Accuracy = 0.141000002623\n",
      "Training epochs #580000: Batch Loss = 386.587189, Accuracy = 0.140000000596\n",
      "Performance on test set: Training epochs #580000, Batch Loss = 7.63710403442, Accuracy = 0.141000002623\n",
      "Training epochs #600000: Batch Loss = 2.319731, Accuracy = 0.122000008821\n",
      "Performance on test set: Training epochs #600000, Batch Loss = 7.42069244385, Accuracy = 0.140999987721\n",
      "Training epochs #620000: Batch Loss = 82.746422, Accuracy = 0.149000003934\n",
      "Performance on test set: Training epochs #620000, Batch Loss = 7.59541988373, Accuracy = 0.141000002623\n",
      "Training epochs #640000: Batch Loss = 235.583405, Accuracy = 0.122000001371\n",
      "Performance on test set: Training epochs #640000, Batch Loss = 7.15671157837, Accuracy = 0.141000002623\n",
      "Training epochs #660000: Batch Loss = 333.886078, Accuracy = 0.139999985695\n",
      "Performance on test set: Training epochs #660000, Batch Loss = 6.7096118927, Accuracy = 0.140999987721\n",
      "Training epochs #680000: Batch Loss = 2.319731, Accuracy = 0.122000008821\n",
      "Performance on test set: Training epochs #680000, Batch Loss = 2.27486753464, Accuracy = 0.141000002623\n",
      "Training epochs #700000: Batch Loss = 71.668381, Accuracy = 0.151999980211\n",
      "Performance on test set: Training epochs #700000, Batch Loss = 2.27486753464, Accuracy = 0.141000002623\n",
      "Training epochs #720000: Batch Loss = 212.077393, Accuracy = 0.123000003397\n",
      "Performance on test set: Training epochs #720000, Batch Loss = 2.27486753464, Accuracy = 0.141000002623\n",
      "Training epochs #740000: Batch Loss = 296.567596, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #740000, Batch Loss = 2.55460739136, Accuracy = 0.0349999964237\n",
      "Training epochs #760000: Batch Loss = 2.588915, Accuracy = 0.0200000014156\n",
      "Performance on test set: Training epochs #760000, Batch Loss = 2.55460739136, Accuracy = 0.0349999964237\n",
      "Training epochs #780000: Batch Loss = 58.884102, Accuracy = 0.0329999998212\n",
      "Performance on test set: Training epochs #780000, Batch Loss = 2.55460739136, Accuracy = 0.035000000149\n",
      "Training epochs #800000: Batch Loss = 184.212479, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #800000, Batch Loss = 2.55460739136, Accuracy = 0.0349999964237\n",
      "Training epochs #820000: Batch Loss = 253.097427, Accuracy = 0.0290000010282\n",
      "Performance on test set: Training epochs #820000, Batch Loss = 2.55460739136, Accuracy = 0.035000000149\n",
      "Training epochs #840000: Batch Loss = 2.591554, Accuracy = 0.0190000012517\n",
      "Performance on test set: Training epochs #840000, Batch Loss = 2.55460739136, Accuracy = 0.0349999964237\n",
      "Training epochs #860000: Batch Loss = 44.585880, Accuracy = 0.0329999998212\n",
      "Performance on test set: Training epochs #860000, Batch Loss = 2.55460739136, Accuracy = 0.035000000149\n",
      "Training epochs #880000: Batch Loss = 156.488281, Accuracy = 0.0159999988973\n",
      "Performance on test set: Training epochs #880000, Batch Loss = 2.55460739136, Accuracy = 0.0349999964237\n",
      "Training epochs #900000: Batch Loss = 214.726868, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #900000, Batch Loss = 2.57663846016, Accuracy = 0.035000000149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #920000: Batch Loss = 2.591554, Accuracy = 0.0190000012517\n",
      "Performance on test set: Training epochs #920000, Batch Loss = 2.95998692513, Accuracy = 0.035000000149\n",
      "Training epochs #940000: Batch Loss = 30.660215, Accuracy = 0.0329999998212\n",
      "Performance on test set: Training epochs #940000, Batch Loss = 3.06753253937, Accuracy = 0.035000000149\n",
      "Training epochs #960000: Batch Loss = 130.523651, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #960000, Batch Loss = 2.55460739136, Accuracy = 0.035000000149\n",
      "Training epochs #980000: Batch Loss = 180.676010, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #980000, Batch Loss = 2.55460739136, Accuracy = 0.035000000149\n",
      "Training epochs #1000000: Batch Loss = 2.591554, Accuracy = 0.0190000012517\n",
      "Performance on test set: Training epochs #1000000, Batch Loss = 2.55460739136, Accuracy = 0.035000000149\n",
      "Training epochs #1020000: Batch Loss = 21.173605, Accuracy = 0.0330000035465\n",
      "Performance on test set: Training epochs #1020000, Batch Loss = 2.55460739136, Accuracy = 0.0349999964237\n",
      "Training epochs #1040000: Batch Loss = 107.361092, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #1040000, Batch Loss = 2.55460739136, Accuracy = 0.035000000149\n",
      "Training epochs #1060000: Batch Loss = 149.685547, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #1060000, Batch Loss = 2.55460739136, Accuracy = 0.035000000149\n",
      "Training epochs #1080000: Batch Loss = 2.591554, Accuracy = 0.0190000012517\n",
      "Performance on test set: Training epochs #1080000, Batch Loss = 2.55460739136, Accuracy = 0.035000000149\n",
      "Training epochs #1100000: Batch Loss = 14.950145, Accuracy = 0.0329999998212\n",
      "Performance on test set: Training epochs #1100000, Batch Loss = 2.55460739136, Accuracy = 0.035000000149\n",
      "Training epochs #1120000: Batch Loss = 84.012863, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #1120000, Batch Loss = 2.55460739136, Accuracy = 0.0349999964237\n",
      "Training epochs #1140000: Batch Loss = 117.648331, Accuracy = 0.0290000010282\n",
      "Performance on test set: Training epochs #1140000, Batch Loss = 2.55460739136, Accuracy = 0.035000000149\n",
      "Training epochs #1160000: Batch Loss = 2.591554, Accuracy = 0.0190000012517\n",
      "Performance on test set: Training epochs #1160000, Batch Loss = 2.55460739136, Accuracy = 0.035000000149\n",
      "Training epochs #1180000: Batch Loss = 10.098811, Accuracy = 0.0329999998212\n",
      "Performance on test set: Training epochs #1180000, Batch Loss = 2.55460739136, Accuracy = 0.0349999964237\n",
      "Training epochs #1200000: Batch Loss = 62.113060, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #1200000, Batch Loss = 2.55460739136, Accuracy = 0.0349999964237\n",
      "Training epochs #1220000: Batch Loss = 88.175079, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #1220000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #1240000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #1240000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #1260000: Batch Loss = 5.111132, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #1260000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #1280000: Batch Loss = 40.625916, Accuracy = 0.0159999988973\n",
      "Performance on test set: Training epochs #1280000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #1300000: Batch Loss = 60.773834, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #1300000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #1320000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #1320000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #1340000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #1340000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #1360000: Batch Loss = 23.134886, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #1360000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #1380000: Batch Loss = 32.299637, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #1380000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #1400000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #1400000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #1420000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #1420000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #1440000: Batch Loss = 5.619039, Accuracy = 0.0159999988973\n",
      "Performance on test set: Training epochs #1440000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #1460000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #1460000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #1480000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #1480000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #1500000: Batch Loss = 2.570442, Accuracy = 0.0290000010282\n",
      "Performance on test set: Training epochs #1500000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #1520000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #1520000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #1540000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #1540000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #1560000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #1560000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #1580000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #1580000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #1600000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #1600000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #1620000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #1620000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #1640000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #1640000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #1660000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #1660000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #1680000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #1680000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #1700000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #1700000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #1720000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #1720000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #1740000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #1740000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #1760000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #1760000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #1780000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #1780000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #1800000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #1800000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #1820000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #1820000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #1840000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #1840000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #1860000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #1860000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #1880000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #1880000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #1900000: Batch Loss = 2.570442, Accuracy = 0.0290000010282\n",
      "Performance on test set: Training epochs #1900000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #1920000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #1920000, Batch Loss = 2.57308077812, Accuracy = 0.0279999990016\n",
      "Training epochs #1940000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #1940000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #1960000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #1960000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #1980000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #1980000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #2000000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #2000000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #2020000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #2020000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #2040000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #2040000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #2060000: Batch Loss = 2.570442, Accuracy = 0.0290000010282\n",
      "Performance on test set: Training epochs #2060000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #2080000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #2080000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #2100000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #2100000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #2120000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #2120000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #2140000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #2140000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #2160000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #2160000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #2180000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #2180000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #2200000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #2200000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #2220000: Batch Loss = 2.570442, Accuracy = 0.0290000010282\n",
      "Performance on test set: Training epochs #2220000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #2240000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #2240000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #2260000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #2260000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #2280000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #2280000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #2300000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #2300000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #2320000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #2320000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #2340000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #2340000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #2360000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #2360000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #2380000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #2380000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #2400000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #2400000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #2420000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #2420000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #2440000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #2440000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #2460000: Batch Loss = 2.570442, Accuracy = 0.0290000010282\n",
      "Performance on test set: Training epochs #2460000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #2480000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #2480000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #2500000: Batch Loss = 2.580998, Accuracy = 0.0250000003725\n",
      "Performance on test set: Training epochs #2500000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #2520000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #2520000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #2540000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #2540000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #2560000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #2560000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #2580000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #2580000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #2600000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #2600000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #2620000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #2620000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #2640000: Batch Loss = 2.602110, Accuracy = 0.0159999988973\n",
      "Performance on test set: Training epochs #2640000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #2660000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #2660000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #2680000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #2680000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #2700000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #2700000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #2720000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #2720000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #2740000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #2740000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #2760000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #2760000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #2780000: Batch Loss = 2.570442, Accuracy = 0.0290000010282\n",
      "Performance on test set: Training epochs #2780000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #2800000: Batch Loss = 2.602110, Accuracy = 0.0159999988973\n",
      "Performance on test set: Training epochs #2800000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #2820000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #2820000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #2840000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #2840000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #2860000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #2860000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #2880000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #2880000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #2900000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #2900000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #2920000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #2920000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #2940000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #2940000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #2960000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #2960000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #2980000: Batch Loss = 2.580998, Accuracy = 0.0250000003725\n",
      "Performance on test set: Training epochs #2980000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #3000000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #3000000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #3020000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #3020000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #3040000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #3040000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #3060000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #3060000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #3080000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #3080000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #3100000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #3100000, Batch Loss = 2.57308077812, Accuracy = 0.0279999990016\n",
      "Training epochs #3120000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #3120000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #3140000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #3140000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #3160000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #3160000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #3180000: Batch Loss = 2.570442, Accuracy = 0.0290000010282\n",
      "Performance on test set: Training epochs #3180000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #3200000: Batch Loss = 2.602110, Accuracy = 0.0159999988973\n",
      "Performance on test set: Training epochs #3200000, Batch Loss = 2.57308077812, Accuracy = 0.0279999990016\n",
      "Training epochs #3220000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #3220000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #3240000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #3240000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #3260000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #3260000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #3280000: Batch Loss = 2.602110, Accuracy = 0.0159999988973\n",
      "Performance on test set: Training epochs #3280000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #3300000: Batch Loss = 2.580998, Accuracy = 0.0250000003725\n",
      "Performance on test set: Training epochs #3300000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #3320000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #3320000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #3340000: Batch Loss = 2.570442, Accuracy = 0.0290000010282\n",
      "Performance on test set: Training epochs #3340000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #3360000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #3360000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #3380000: Batch Loss = 2.580998, Accuracy = 0.0250000003725\n",
      "Performance on test set: Training epochs #3380000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #3400000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #3400000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #3420000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #3420000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #3440000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #3440000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #3460000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #3460000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #3480000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #3480000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #3500000: Batch Loss = 2.570442, Accuracy = 0.0290000010282\n",
      "Performance on test set: Training epochs #3500000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #3520000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #3520000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #3540000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #3540000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #3560000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #3560000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #3580000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #3580000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #3600000: Batch Loss = 2.602110, Accuracy = 0.0159999988973\n",
      "Performance on test set: Training epochs #3600000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #3620000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #3620000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #3640000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #3640000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #3660000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #3660000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #3680000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #3680000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #3700000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #3700000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #3720000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #3720000, Batch Loss = 2.57308077812, Accuracy = 0.0279999990016\n",
      "Training epochs #3740000: Batch Loss = 2.570442, Accuracy = 0.0290000010282\n",
      "Performance on test set: Training epochs #3740000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #3760000: Batch Loss = 2.602110, Accuracy = 0.0159999988973\n",
      "Performance on test set: Training epochs #3760000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #3780000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #3780000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #3800000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #3800000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #3820000: Batch Loss = 2.570442, Accuracy = 0.0290000010282\n",
      "Performance on test set: Training epochs #3820000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #3840000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #3840000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #3860000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #3860000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #3880000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #3880000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #3900000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #3900000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #3920000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #3920000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #3940000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #3940000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #3960000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #3960000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #3980000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #3980000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #4000000: Batch Loss = 2.602110, Accuracy = 0.0159999988973\n",
      "Performance on test set: Training epochs #4000000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #4020000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #4020000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #4040000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #4040000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #4060000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #4060000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #4080000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #4080000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #4100000: Batch Loss = 2.580998, Accuracy = 0.0250000003725\n",
      "Performance on test set: Training epochs #4100000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #4120000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #4120000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #4140000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #4140000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #4160000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #4160000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #4180000: Batch Loss = 2.580998, Accuracy = 0.0250000003725\n",
      "Performance on test set: Training epochs #4180000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #4200000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #4200000, Batch Loss = 2.57308077812, Accuracy = 0.0279999990016\n",
      "Training epochs #4220000: Batch Loss = 2.570442, Accuracy = 0.0290000010282\n",
      "Performance on test set: Training epochs #4220000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #4240000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #4240000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #4260000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #4260000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #4280000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #4280000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #4300000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #4300000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #4320000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #4320000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #4340000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #4340000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #4360000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #4360000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #4380000: Batch Loss = 2.570442, Accuracy = 0.0290000010282\n",
      "Performance on test set: Training epochs #4380000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #4400000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #4400000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #4420000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #4420000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #4440000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #4440000, Batch Loss = 2.57308077812, Accuracy = 0.0279999990016\n",
      "Training epochs #4460000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #4460000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #4480000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #4480000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #4500000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #4500000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #4520000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #4520000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #4540000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #4540000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #4560000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #4560000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #4580000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #4580000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #4600000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #4600000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #4620000: Batch Loss = 2.570442, Accuracy = 0.0290000010282\n",
      "Performance on test set: Training epochs #4620000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #4640000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #4640000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #4660000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #4660000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #4680000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #4680000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #4700000: Batch Loss = 2.570442, Accuracy = 0.0290000010282\n",
      "Performance on test set: Training epochs #4700000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #4720000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #4720000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #4740000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #4740000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #4760000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #4760000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #4780000: Batch Loss = 2.570442, Accuracy = 0.0290000010282\n",
      "Performance on test set: Training epochs #4780000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #4800000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #4800000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #4820000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #4820000, Batch Loss = 2.57308077812, Accuracy = 0.0279999990016\n",
      "Training epochs #4840000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #4840000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #4860000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #4860000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #4880000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #4880000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #4900000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #4900000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #4920000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #4920000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #4940000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #4940000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #4960000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #4960000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #4980000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #4980000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #5000000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #5000000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #5020000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #5020000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #5040000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #5040000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #5060000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #5060000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #5080000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #5080000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #5100000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #5100000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #5120000: Batch Loss = 2.602110, Accuracy = 0.0159999988973\n",
      "Performance on test set: Training epochs #5120000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #5140000: Batch Loss = 2.580998, Accuracy = 0.0250000003725\n",
      "Performance on test set: Training epochs #5140000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #5160000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #5160000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #5180000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #5180000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #5200000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #5200000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #5220000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #5220000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #5240000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #5240000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #5260000: Batch Loss = 2.570442, Accuracy = 0.0290000010282\n",
      "Performance on test set: Training epochs #5260000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #5280000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #5280000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #5300000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #5300000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #5320000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #5320000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #5340000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #5340000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #5360000: Batch Loss = 2.602110, Accuracy = 0.0159999988973\n",
      "Performance on test set: Training epochs #5360000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #5380000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #5380000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #5400000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #5400000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #5420000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #5420000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #5440000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #5440000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #5460000: Batch Loss = 2.580998, Accuracy = 0.0250000003725\n",
      "Performance on test set: Training epochs #5460000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #5480000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #5480000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #5500000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #5500000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #5520000: Batch Loss = 2.602110, Accuracy = 0.0159999988973\n",
      "Performance on test set: Training epochs #5520000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #5540000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #5540000, Batch Loss = 2.57308077812, Accuracy = 0.0279999990016\n",
      "Training epochs #5560000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #5560000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #5580000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #5580000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #5600000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #5600000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #5620000: Batch Loss = 2.580998, Accuracy = 0.0250000003725\n",
      "Performance on test set: Training epochs #5620000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #5640000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #5640000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #5660000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #5660000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #5680000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #5680000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #5700000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #5700000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #5720000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #5720000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #5740000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #5740000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #5760000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #5760000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #5780000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #5780000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #5800000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #5800000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #5820000: Batch Loss = 2.570442, Accuracy = 0.0290000010282\n",
      "Performance on test set: Training epochs #5820000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #5840000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #5840000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #5860000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #5860000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #5880000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #5880000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #5900000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #5900000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #5920000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #5920000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #5940000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #5940000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #5960000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #5960000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #5980000: Batch Loss = 2.570442, Accuracy = 0.0290000010282\n",
      "Performance on test set: Training epochs #5980000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #6000000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #6000000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #6020000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #6020000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #6040000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #6040000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #6060000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #6060000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #6080000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #6080000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #6100000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #6100000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #6120000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #6120000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #6140000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #6140000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #6160000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #6160000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #6180000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #6180000, Batch Loss = 2.57308077812, Accuracy = 0.0279999990016\n",
      "Training epochs #6200000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #6200000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #6220000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #6220000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #6240000: Batch Loss = 2.602110, Accuracy = 0.0159999988973\n",
      "Performance on test set: Training epochs #6240000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #6260000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #6260000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #6280000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #6280000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #6300000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #6300000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #6320000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #6320000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #6340000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #6340000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #6360000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #6360000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #6380000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #6380000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #6400000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #6400000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #6420000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #6420000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #6440000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #6440000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #6460000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #6460000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #6480000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #6480000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #6500000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #6500000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #6520000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #6520000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #6540000: Batch Loss = 2.570442, Accuracy = 0.0290000010282\n",
      "Performance on test set: Training epochs #6540000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #6560000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #6560000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #6580000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #6580000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #6600000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #6600000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #6620000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #6620000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #6640000: Batch Loss = 2.602110, Accuracy = 0.0159999988973\n",
      "Performance on test set: Training epochs #6640000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #6660000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #6660000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #6680000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #6680000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #6700000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #6700000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #6720000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #6720000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #6740000: Batch Loss = 2.580998, Accuracy = 0.0250000003725\n",
      "Performance on test set: Training epochs #6740000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #6760000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #6760000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #6780000: Batch Loss = 2.570442, Accuracy = 0.0290000010282\n",
      "Performance on test set: Training epochs #6780000, Batch Loss = 2.57308077812, Accuracy = 0.0279999990016\n",
      "Training epochs #6800000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #6800000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #6820000: Batch Loss = 2.580998, Accuracy = 0.0250000003725\n",
      "Performance on test set: Training epochs #6820000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #6840000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #6840000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #6860000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #6860000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #6880000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #6880000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #6900000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #6900000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #6920000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #6920000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #6940000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #6940000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #6960000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #6960000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #6980000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #6980000, Batch Loss = 2.57308077812, Accuracy = 0.0279999990016\n",
      "Training epochs #7000000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #7000000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #7020000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #7020000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #7040000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #7040000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #7060000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #7060000, Batch Loss = 2.57308077812, Accuracy = 0.0279999990016\n",
      "Training epochs #7080000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #7080000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #7100000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #7100000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #7120000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #7120000, Batch Loss = 2.57308077812, Accuracy = 0.0279999990016\n",
      "Training epochs #7140000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #7140000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #7160000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #7160000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #7180000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #7180000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #7200000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #7200000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #7220000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #7220000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #7240000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #7240000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #7260000: Batch Loss = 2.570442, Accuracy = 0.0290000010282\n",
      "Performance on test set: Training epochs #7260000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #7280000: Batch Loss = 2.602110, Accuracy = 0.0159999988973\n",
      "Performance on test set: Training epochs #7280000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #7300000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #7300000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #7320000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #7320000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #7340000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #7340000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #7360000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #7360000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #7380000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #7380000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #7400000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #7400000, Batch Loss = 2.57308077812, Accuracy = 0.0279999990016\n",
      "Training epochs #7420000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #7420000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #7440000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #7440000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #7460000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #7460000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #7480000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #7480000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #7500000: Batch Loss = 2.570442, Accuracy = 0.0290000010282\n",
      "Performance on test set: Training epochs #7500000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #7520000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #7520000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #7540000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #7540000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #7560000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #7560000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #7580000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #7580000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #7600000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #7600000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #7620000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #7620000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #7640000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #7640000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #7660000: Batch Loss = 2.570442, Accuracy = 0.0290000010282\n",
      "Performance on test set: Training epochs #7660000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #7680000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #7680000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #7700000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #7700000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #7720000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #7720000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #7740000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #7740000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #7760000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #7760000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #7780000: Batch Loss = 2.580998, Accuracy = 0.0250000022352\n",
      "Performance on test set: Training epochs #7780000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #7800000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #7800000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #7820000: Batch Loss = 2.570442, Accuracy = 0.0290000010282\n",
      "Performance on test set: Training epochs #7820000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #7840000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #7840000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #7860000: Batch Loss = 2.580998, Accuracy = 0.0250000003725\n",
      "Performance on test set: Training epochs #7860000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #7880000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #7880000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #7900000: Batch Loss = 2.570442, Accuracy = 0.0290000010282\n",
      "Performance on test set: Training epochs #7900000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #7920000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #7920000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #7940000: Batch Loss = 2.580998, Accuracy = 0.0250000003725\n",
      "Performance on test set: Training epochs #7940000, Batch Loss = 2.57308077812, Accuracy = 0.0280000008643\n",
      "Training epochs #7960000: Batch Loss = 2.596832, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #7960000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #7980000: Batch Loss = 2.570442, Accuracy = 0.0290000028908\n",
      "Performance on test set: Training epochs #7980000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Training epochs #8000000: Batch Loss = 2.602110, Accuracy = 0.01600000076\n",
      "Performance on test set: Training epochs #8000000, Batch Loss = 2.57308077812, Accuracy = 0.0280000027269\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "def extract_batch_size(_train, step, batch_size):   \n",
    "    # Function to fetch a \"batch_size\" amount of data from \"(X|y)_train\" data.    \n",
    "    shape = list(_train.shape)\n",
    "    shape[0] = batch_size\n",
    "    batch_s = np.empty(shape)\n",
    "    for i in range(batch_size):\n",
    "        # Loop index\n",
    "        index = ((step-1)*batch_size + i) % len(_train)\n",
    "        batch_s[i] = _train[index] \n",
    "    return batch_s\n",
    "\n",
    "\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "test_predictions = []\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session(config=sess_config) as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size <= training_iters:\n",
    "        batch_xs = extract_batch_size(train_x, step, batch_size)\n",
    "        batch_ys = extract_batch_size(train_y, step, batch_size)\n",
    "\n",
    "        # Fit training using batch data\n",
    "        _, loss, acc = sess.run([optimizer, cost, accuracy],\n",
    "                       feed_dict={x: batch_xs, y: batch_ys})\n",
    "\n",
    "        train_losses.append(loss)\n",
    "        train_accuracies.append(acc)  \n",
    "        \n",
    "        \n",
    "        batch_xt = extract_batch_size(test_x, step, batch_size)\n",
    "        batch_yt = extract_batch_size(test_y, step, batch_size)\n",
    "    \n",
    "        # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
    "        test_predict, test_loss, test_acc = sess.run([pred, cost, accuracy], \n",
    "                                            feed_dict={x: batch_xt, y: batch_yt})\n",
    "        \n",
    "        test_predictions.append(np.argmax(test_predict, axis=1))\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_acc)\n",
    "\n",
    "        # Evaluate network only at some steps for faster training: \n",
    "        if (step*batch_size % display_step == 0) or (step == 1) \\\n",
    "            or (step * batch_size > training_iters):\n",
    "\n",
    "            print(\"Training epochs #\" + str(step*batch_size) + \\\n",
    "                  \": Batch Loss = \" + \"{:.6f}\".format(loss) + \\\n",
    "                  \", Accuracy = {}\".format(acc))\n",
    "        \n",
    "            print(\"Performance on test set: \" + \"Training epochs #\" + str(step*batch_size) +\\\n",
    "                  \", Batch Loss = {}\".format(test_loss) + \", Accuracy = {}\".format(test_acc))\n",
    "        step += 1\n",
    "\n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# performance visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsoAAAHwCAYAAAC/n0kWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAIQgUQYAAABCkCgDAAAAIf4fN0h78BUDmYEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f236c16afd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f236c175950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#font = {'family': 'Bitstream Vera Sans', 'weight': 'bold', 'size': 12}\n",
    "#matplotlib.rc('font', **font)\n",
    "width = 12\n",
    "height = 8\n",
    "plt.figure(figsize=(width, height))\n",
    "\n",
    "indep_train_axis = np.array(range(batch_size, \n",
    "                   (len(train_losses)+1)*batch_size, batch_size))\n",
    "plt.plot(indep_train_axis, np.array(train_losses), \"b--\", label=\"Train losses\")\n",
    "plt.plot(indep_train_axis, np.array(train_accuracies), \"g--\", label=\"Train accuracies\")\n",
    "\n",
    "indep_test_axis = np.append(np.array(range(batch_size, \n",
    "                  len(test_losses)*batch_size, batch_size)), training_iters)\n",
    "plt.plot(indep_test_axis, np.array(test_losses), \"b-\", label=\"Test losses\")\n",
    "plt.plot(indep_test_axis, np.array(test_accuracies), \"g-\", label=\"Test accuracies\")\n",
    "\n",
    "plt.title(\"Training session's progress over iterations\")\n",
    "plt.legend(loc='upper right', shadow=False)\n",
    "plt.ylabel('Training Progress (Loss or Accuracy values)')\n",
    "plt.xlabel('Training iteration')\n",
    "plt.savefig('result/lstm_loss_accuracy.png')\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 98.9000022411%\n",
      "\n",
      "Precision: 98.1583307452%\n",
      "Recall: 98.1%\n",
      "f1_score: 98.0815279125%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[   11     2     2     0     0     1     7     2     0     0     0    11\n",
      "      0]\n",
      " [    2  1711    12     0    31    19     3    13     0     1    15     8\n",
      "      0]\n",
      " [    0     0   490     0     0     0     0     0     7     0     9     0\n",
      "      0]\n",
      " [    0     1     0     0     0     0     0     0     0     0     0     0\n",
      "      0]\n",
      " [    0     5     2     0  2398     0     0    11     0     0     4     9\n",
      "      0]\n",
      " [    1    11     0     0    11   932     1     3     0     0     2     0\n",
      "      0]\n",
      " [    0     3     0     0     0     0 12445     1     2     0     3     2\n",
      "      0]\n",
      " [    0    36     2     0    29     9     0   509     0     0     6     4\n",
      "      0]\n",
      " [    0     0     0     0     0     0     3     1   416     0     2     0\n",
      "      0]\n",
      " [    0     0     3     0     0     0    10     0     0    16     0     0\n",
      "      0]\n",
      " [    0     4     0     0     0     1     0     0     0     0    20     0\n",
      "      0]\n",
      " [    0     9    14     0     4     5    15     2     0     0     0   672\n",
      "      0]\n",
      " [    0     0     0     0     0     4     0     0     0     0     0     0\n",
      "      0]]\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     security       0.79      0.31      0.44        36\n",
      "       others       0.96      0.94      0.95      1815\n",
      "    processor       0.93      0.97      0.95       506\n",
      "         disk       0.00      0.00      0.00         1\n",
      "      network       0.97      0.99      0.98      2429\n",
      "         file       0.96      0.97      0.96       961\n",
      "     database       1.00      1.00      1.00     12456\n",
      "      service       0.94      0.86      0.90       595\n",
      "       memory       0.98      0.99      0.98       422\n",
      "communication       0.94      0.55      0.70        29\n",
      "       system       0.33      0.80      0.47        25\n",
      "       driver       0.95      0.93      0.94       721\n",
      "           io       0.00      0.00      0.00         4\n",
      "\n",
      "  avg / total       0.98      0.98      0.98     20000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f24486de390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAo4AAAJGCAYAAADVtQQ5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f24445b0a10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Results\n",
    "predictions = []\n",
    "length = len(test_predictions)-20\n",
    "for i in range(20):\n",
    "    predictions.extend(test_predictions[length+i])\n",
    "\n",
    "true_y = np.argmax(test_y, axis=1)\n",
    "\n",
    "print(\"Testing Accuracy: {}%\".format(100*max(test_accuracies)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Precision: {}%\".format(100*metrics.precision_score(\n",
    "    true_y, predictions, average=\"weighted\")))\n",
    "print(\"Recall: {}%\".format(100*metrics.recall_score(true_y, \n",
    "    predictions, average=\"weighted\")))\n",
    "print(\"f1_score: {}%\".format(100*metrics.f1_score(true_y, \n",
    "    predictions, average=\"weighted\")))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix:\")\n",
    "confusion_matrix = metrics.confusion_matrix(true_y, predictions)\n",
    "print(confusion_matrix)\n",
    "\n",
    "# Plot: \n",
    "## cmap can be changed to many colors, (colormaps.Oranges,OrRd, etc)\n",
    "def plot_CM(cm, title=\"Normalized Confusion Matrix\", cmap=plt.cm.summer):\n",
    "    width = 12\n",
    "    height = 8\n",
    "    plt.figure(figsize=(width, height))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(fault_label))\n",
    "    plt.xticks(tick_marks, fault_label.values(), rotation=90)\n",
    "    plt.yticks(tick_marks, fault_label.values())\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.show()\n",
    "    \n",
    "print(metrics.classification_report(\n",
    "    true_y, predictions, target_names = list(fault_label.values())))\n",
    "\n",
    "cm = confusion_matrix\n",
    "\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:,np.newaxis]\n",
    "plt.figure()\n",
    "plot_CM(cm_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
