{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 0.4)\n",
    "sess_config = tf.ConfigProto(gpu_options=gpu_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separating data into 80% training set & 20% test set...\n",
      "Dataset separated.\n",
      "\n",
      "((160000, 1, 7, 2), (160000, 14), (40000, 14))\n"
     ]
    }
   ],
   "source": [
    "def one_hot(y):\n",
    "    y = y.reshape(len(y))\n",
    "    n_values = np.max(y) + 1\n",
    "    return np.eye(n_values)[np.array(y, dtype=np.int32)]  # Returns FLOATS\n",
    "\n",
    "\n",
    "def load_X(X_path):\n",
    "    X_list = []\n",
    "    file = open(X_path, 'r')\n",
    "    # Read dataset from disk, dealing with text files' syntax\n",
    "    X_signal = [np.array(item, dtype=np.float32) for item in [\n",
    "               line.strip().split('\\t') for line in file]]\n",
    "    X_list.append(X_signal)\n",
    "    file.close()\n",
    "    return np.transpose(np.array(X_list), (1, 2, 0))\n",
    "\n",
    "\n",
    "# Load \"y\" (the neural network's training and testing outputs)\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    # Read dataset from disk, dealing with text file's syntax\n",
    "    y_ = np.array([elem for elem in [line.strip().split('\\t') for line in file]], \n",
    "                  dtype=np.int32)\n",
    "    file.close()\n",
    "    # Substract 1 to each output class for friendly 0-based indexing\n",
    "    return one_hot(y_)\n",
    "\n",
    "labels = {'0':'file', '1':'network', '2':'service', '3':'database', '4':'communication', '5':'memory', '6':'driver', \n",
    "    '7':'system', '8':'application', '9':'io', '10':'others', '11':'security', '12':'disk', '13':'processor'}\n",
    "\n",
    "dataset_path = \"data_msg_type/\"\n",
    "X_path = dataset_path + \"x.txt\"\n",
    "y_path = dataset_path + \"y.txt\"\n",
    "\n",
    "X = load_X(X_path)\n",
    "y = load_y(y_path)\n",
    "x = X.reshape(len(X), 1, 7, 2)\n",
    "\n",
    "# Separate our training data into test and training.\n",
    "print(\"Separating data into 80% training set & 20% test set...\")\n",
    "train_x, test_x, train_y, test_y = cross_validation.train_test_split(\n",
    "    x, y, test_size=0.2, random_state=33)#add random state here...\n",
    "print(\"Dataset separated.\\n\")\n",
    "print(train_x.shape, train_y.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_height = 1\n",
    "input_width = 7\n",
    "num_channels = 2\n",
    "num_labels = 14\n",
    "\n",
    "batch_size = 100\n",
    "kernel_size = 4\n",
    "depth = 35\n",
    "num_hidden = 256\n",
    "\n",
    "learning_rate = 0.0001\n",
    "# iteration times\n",
    "training_epochs = 100\n",
    "lstm_size = 128\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "\n",
    "n_classes = 14\n",
    "n_hidden = 128\n",
    "n_inputs = 10\n",
    "\n",
    "rnnW = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_inputs, n_hidden])),\n",
    "    'output': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "rnnBiases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_hidden], mean=1.0)),\n",
    "    'output': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# netowrk config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# init nn parameters\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.0, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# conv\n",
    "def depthwise_conv2d(x, W):\n",
    "    return tf.nn.depthwise_conv2d(x,W, [1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "def apply_depthwise_conv(x,kernel_size,num_channels,depth):\n",
    "    weights = weight_variable([1, kernel_size, num_channels, depth])\n",
    "    biases = bias_variable([depth * num_channels])\n",
    "    return tf.nn.relu(tf.add(depthwise_conv2d(x, weights),biases))\n",
    "    \n",
    "def apply_max_pool(x,kernel_size,stride_size):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, kernel_size, 1], \n",
    "                          strides=[1, 1, stride_size, 1], padding='VALID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1, 4, 70)\n",
      "(?, 1, 1, 210)\n",
      "(?, 128)\n",
      "(?, 14)\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "print(c.shape)\n",
    "p = apply_max_pool(c,2,1)\n",
    "c2 = apply_depthwise_conv(p,3,depth*num_channels,depth//10)\n",
    "print(c2.shape)\n",
    "c2Reshape = tf.reshape(c2, [-1, 7, n_inputs])\n",
    "shuff = tf.transpose(c2Reshape, [1, 0, 2])\n",
    "shuff = tf.reshape(shuff, [-1, n_inputs])\n",
    "\n",
    "# Linear activation, reshaping inputs to the LSTM's number of hidden:\n",
    "hidden = tf.nn.relu(tf.matmul(shuff, rnnW['hidden']) + rnnBiases['hidden'])\n",
    "print(hidden.shape)\n",
    "\n",
    "# Split the series because the rnn cell needs time_steps features, each of shape:\n",
    "hidden = tf.split(axis=0, num_or_size_splits=21,value=hidden)\n",
    "\n",
    "lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(lstm_size, forget_bias=1.0, \n",
    "                                          state_is_tuple=True)\n",
    "lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(lstm_size, forget_bias=1.0, \n",
    "                                          state_is_tuple=True)\n",
    "\n",
    "# Stack two LSTM layers, both layers has the same shape\n",
    "lstm_layers = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], \n",
    "                                          state_is_tuple=True)\n",
    "\n",
    "lstmOutputs, _ = tf.contrib.rnn.static_rnn(lstm_layers, hidden, dtype=tf.float32)\n",
    "lstmLastOutput = lstmOutputs[-1]\n",
    "y_ = tf.matmul(lstmLastOutput, rnnW['output']) + rnnBiases['output']\n",
    "print(y_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net work done\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"cost\"):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=Y))\n",
    "    # optimizer = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(loss)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=\n",
    "                                                  learning_rate).minimize(loss)\n",
    "\n",
    "    # Add scalar summary for cost\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(y_, 1)) \n",
    "    # Count correct predictions\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) \n",
    "    # Cast boolean to float to average\n",
    "    # Add scalar summary for accuracy\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "\n",
    "print(\"net work done\")\n",
    "\n",
    "cost_history = np.empty(shape=[1],dtype=float)\n",
    "\n",
    "with tf.Session(config=sess_config) as session:\n",
    "    # create a log writer. run 'tensorboard --logdir=./logs/nn_logs'\n",
    "    writer = tf.summary.FileWriter(\"data/cnn_lstm_logs\", session.graph)  # for 1.0\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        for b in range(total_batches):    \n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "\n",
    "            test_indices = np.arange(len(test_x))  # Get A Test Batch\n",
    "            np.random.shuffle(test_indices)\n",
    "            test_indices = test_indices[0:]\n",
    "            summary, acc=session.run([merged, accuracy], \n",
    "                        feed_dict={X: test_x[test_indices], Y: test_y[test_indices]})\n",
    "        print \"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \", acc\n",
    "        writer.add_summary(summary, epoch)  # Write summary\n",
    "\n",
    "    print \"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y})\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='Normalized Confusion matrix', \n",
    "                          cmap=plt.cm.get_cmap(\"Blues\")): \n",
    "    cm = cm / cm.astype(np.float).sum(axis=1) \n",
    "    #print \"confusion_matrix: \\n{}\".format(cm) \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap) \n",
    "    plt.title(title) \n",
    "    plt.colorbar() \n",
    "    tick_marks =np.arange(len(fault_label))\n",
    "    plt.xticks(tick_marks, fault_label.values(), rotation=45) \n",
    "    plt.yticks(tick_marks, fault_label.values()) \n",
    "    plt.tight_layout() \n",
    "    plt.ylabel('True label') \n",
    "    plt.xlabel('Predicted label') \n",
    "    plt.show()\n",
    "\n",
    "def test_(): \n",
    "    y_true = np.load(\"data/y_true.npy\") \n",
    "    y_pred = np.load(\"data/y_pred.npy\") \n",
    "    print(classification_report(y_true, y_pred, \n",
    "          target_names = fault_label.values()))\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print()\n",
    "    plot_confusion_matrix(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
