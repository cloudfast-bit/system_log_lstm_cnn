{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 0.4)\n",
    "sess_config = tf.ConfigProto(gpu_options=gpu_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def segment_(data,window_size = 40):\n",
    "    segments = np.empty((0,1,window_size,3))\n",
    "    labels = np.empty((0))\n",
    "    for line in data:\n",
    "        line = np.array(line).reshape(1,1,window_size,3)\n",
    "        segments = np.vstack([segments,line])\n",
    "    return segments\n",
    "\n",
    "# init nn parameters\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.0, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# conv\n",
    "def depthwise_conv2d(x, W):\n",
    "    return tf.nn.depthwise_conv2d(x,W, [1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "def apply_depthwise_conv(x,kernel_size,num_channels,depth):\n",
    "    weights = weight_variable([1, kernel_size, num_channels, depth])\n",
    "    biases = bias_variable([depth * num_channels])\n",
    "    return tf.nn.relu(tf.add(depthwise_conv2d(x, weights),biases))\n",
    "    \n",
    "def apply_max_pool(x,kernel_size,stride_size):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, kernel_size, 1], \n",
    "                          strides=[1, 1, stride_size, 1], padding='VALID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8360, 1, 40, 3)\n",
      "(8360, 7)\n",
      "Separating data into 67% training set & 33% test set...\n",
      "Dataset separated.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fault_label = {'1': 'info',\n",
    "               '2': 'Critical',\n",
    "               '3': 'error',\n",
    "               '4': 'notice',\n",
    "               '5': 'warning',\n",
    "               '6': 'alert',\n",
    "               '7': 'emergency'}\n",
    "\n",
    "X, X_val, labels, labels_val = [], [], [], [] #validation set features\n",
    "\n",
    "with open(\"data/msg_token_train.txt\", 'rU') as f:\n",
    "        res = list(f)\n",
    "        for line in res:\n",
    "            line.strip(\"\\n\")\n",
    "            features = line.split(\"\\t\")\n",
    "            while features.__contains__(\"\"):\n",
    "                features.remove(\"\")\n",
    "            for i in range(len(features)):\n",
    "                features[i] = float(features[i])\n",
    "            X.append(features)\n",
    "            \n",
    "with open(\"data/msg_label_train.txt\", 'rU') as f:\n",
    "        res = list(f)\n",
    "        for line in res:\n",
    "            labels.append(int(line.strip(\"\\n\")[0]))\n",
    "            \n",
    "with open(\"data/msg_token_test.txt\", 'rU') as f:\n",
    "        res = list(f)\n",
    "        for line in res:\n",
    "            line.strip(\"\\n\")\n",
    "            features = line.split(\"\\t\")\n",
    "            while features.__contains__(\"\"):\n",
    "                features.remove(\"\")\n",
    "            for i in range(len(features)):\n",
    "                features[i] = float(features[i])\n",
    "            X_val.append(features)\n",
    "            \n",
    "with open(\"data/msg_label_test.txt\", 'rU') as f:\n",
    "        res = list(f)\n",
    "        for line in res:\n",
    "            labels_val.append(int(line.strip(\"\\n\")[0]))\n",
    "\n",
    "segments = segment_(X)\n",
    "segments_val = segment_(X_val)\n",
    "print(segments.shape)\n",
    "\n",
    "labels = np.asarray(pd.get_dummies(labels), dtype = np.int8)\n",
    "labels_val = np.asarray(pd.get_dummies(labels_val), dtype = np.int8)\n",
    "print(labels.shape)\n",
    "\n",
    "train_x = segments.reshape(len(segments), 1, 40, 3)\n",
    "train_y = labels\n",
    "test_x = segments_val.reshape(len(segments_val), 1, 40, 3)\n",
    "test_y = labels_val\n",
    "\n",
    "# Separate our training data into test and training.\n",
    "print(\"Separating data into 67% training set & 33% test set...\")\n",
    "train_x, test_x, train_y, test_y = cross_validation.train_test_split(\n",
    "    train_x, train_y, test_size=0.33, random_state=33)#add random state here...\n",
    "print(\"Dataset separated.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_height = 1\n",
    "input_width = 40\n",
    "num_channels = 3\n",
    "num_labels = 7\n",
    "\n",
    "batch_size = 128\n",
    "kernel_size = 5\n",
    "depth = 20\n",
    "num_hidden = 1000\n",
    "\n",
    "learning_rate = 0.0001\n",
    "# iteration times\n",
    "training_epochs = 100\n",
    "lstm_size = 128\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "\n",
    "n_classes = 7\n",
    "n_hidden = 128\n",
    "n_inputs = 120\n",
    "\n",
    "rnnW = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_inputs, n_hidden])),\n",
    "    'output': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "rnnBiases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_hidden], mean=1.0)),\n",
    "    'output': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,5,2)\n",
    "c2 = apply_depthwise_conv(p,3,depth*num_channels,depth//10)\n",
    "c2Reshape = tf.reshape(c2, [-1, 7, n_inputs])\n",
    "shuff = tf.transpose(c2Reshape, [1, 0, 2])\n",
    "shuff = tf.reshape(shuff, [-1, n_inputs])\n",
    "\n",
    "# Linear activation, reshaping inputs to the LSTM's number of hidden:\n",
    "hidden = tf.nn.relu(tf.matmul(shuff, rnnW['hidden']) + rnnBiases['hidden'])\n",
    "\n",
    "# Split the series because the rnn cell needs time_steps features, each of shape:\n",
    "hidden = tf.split(axis=0, num_or_size_splits=14, value=hidden)\n",
    "\n",
    "lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(lstm_size, forget_bias=1.0, \n",
    "                                          state_is_tuple=True)\n",
    "lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(lstm_size, forget_bias=1.0, \n",
    "                                          state_is_tuple=True)\n",
    "\n",
    "# Stack two LSTM layers, both layers has the same shape\n",
    "lstm_layers = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], \n",
    "                                          state_is_tuple=True)\n",
    "\n",
    "lstmOutputs, _ = tf.contrib.rnn.static_rnn(lstm_layers, hidden, dtype=tf.float32)\n",
    "lstmLastOutput = lstmOutputs[-1]\n",
    "y_ = tf.matmul(lstmLastOutput, rnnW['output']) + rnnBiases['output']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net work done\n",
      "Epoch:  0  Training Loss:  1.5546  Training Accuracy:  0.433128\n",
      "Epoch:  1  Training Loss:  1.42265  Training Accuracy:  0.437477\n",
      "Epoch:  2  Training Loss:  1.32914  Training Accuracy:  0.418992\n",
      "Epoch:  3  Training Loss:  1.2654  Training Accuracy:  0.42153\n",
      "Epoch:  4  Training Loss:  1.2338  Training Accuracy:  0.429503\n",
      "Epoch:  5  Training Loss:  1.2114  Training Accuracy:  0.427691\n",
      "Epoch:  6  Training Loss:  1.20749  Training Accuracy:  0.428054\n",
      "Epoch:  7  Training Loss:  1.17167  Training Accuracy:  0.437115\n",
      "Epoch:  8  Training Loss:  1.1529  Training Accuracy:  0.434578\n",
      "Epoch:  9  Training Loss:  1.16942  Training Accuracy:  0.43784\n",
      "Epoch:  10  Training Loss:  1.17405  Training Accuracy:  0.425879\n",
      "Epoch:  11  Training Loss:  1.16498  Training Accuracy:  0.433853\n",
      "Epoch:  12  Training Loss:  1.19389  Training Accuracy:  0.430228\n",
      "Epoch:  13  Training Loss:  1.17959  Training Accuracy:  0.440015\n",
      "Epoch:  14  Training Loss:  1.19844  Training Accuracy:  0.436028\n",
      "Epoch:  15  Training Loss:  1.1648  Training Accuracy:  0.426604\n",
      "Epoch:  16  Training Loss:  1.1844  Training Accuracy:  0.427329\n",
      "Epoch:  17  Training Loss:  1.14797  Training Accuracy:  0.442552\n",
      "Epoch:  18  Training Loss:  1.15836  Training Accuracy:  0.436752\n",
      "Epoch:  19  Training Loss:  1.15891  Training Accuracy:  0.412831\n",
      "Epoch:  20  Training Loss:  1.14683  Training Accuracy:  0.429141\n",
      "Epoch:  21  Training Loss:  1.16709  Training Accuracy:  0.428054\n",
      "Epoch:  22  Training Loss:  1.15218  Training Accuracy:  0.432765\n",
      "Epoch:  23  Training Loss:  1.19017  Training Accuracy:  0.415005\n",
      "Epoch:  24  Training Loss:  1.14532  Training Accuracy:  0.43784\n",
      "Epoch:  25  Training Loss:  1.14101  Training Accuracy:  0.422617\n",
      "Epoch:  26  Training Loss:  1.15266  Training Accuracy:  0.422617\n",
      "Epoch:  27  Training Loss:  1.14658  Training Accuracy:  0.419355\n",
      "Epoch:  28  Training Loss:  1.14465  Training Accuracy:  0.426241\n",
      "Epoch:  29  Training Loss:  1.14942  Training Accuracy:  0.43639\n",
      "Epoch:  30  Training Loss:  1.15723  Training Accuracy:  0.430591\n",
      "Epoch:  31  Training Loss:  1.16141  Training Accuracy:  0.42153\n",
      "Epoch:  32  Training Loss:  1.15845  Training Accuracy:  0.447263\n",
      "Epoch:  33  Training Loss:  1.16408  Training Accuracy:  0.435665\n",
      "Epoch:  34  Training Loss:  1.1586  Training Accuracy:  0.432403\n",
      "Epoch:  35  Training Loss:  1.15544  Training Accuracy:  0.432765\n",
      "Epoch:  36  Training Loss:  1.16478  Training Accuracy:  0.438565\n",
      "Epoch:  37  Training Loss:  1.16012  Training Accuracy:  0.432765\n",
      "Epoch:  38  Training Loss:  1.1671  Training Accuracy:  0.42008\n",
      "Epoch:  39  Training Loss:  1.15982  Training Accuracy:  0.436028\n",
      "Epoch:  40  Training Loss:  1.14729  Training Accuracy:  0.425154\n",
      "Epoch:  41  Training Loss:  1.14501  Training Accuracy:  0.428054\n",
      "Epoch:  42  Training Loss:  1.14445  Training Accuracy:  0.416818\n",
      "Epoch:  43  Training Loss:  1.14823  Training Accuracy:  0.421167\n",
      "Epoch:  44  Training Loss:  1.14571  Training Accuracy:  0.448351\n",
      "Epoch:  45  Training Loss:  1.14058  Training Accuracy:  0.416093\n",
      "Epoch:  46  Training Loss:  1.14945  Training Accuracy:  0.437477\n",
      "Epoch:  47  Training Loss:  1.14428  Training Accuracy:  0.424067\n",
      "Epoch:  48  Training Loss:  1.1388  Training Accuracy:  0.427329\n",
      "Epoch:  49  Training Loss:  1.14749  Training Accuracy:  0.429866\n",
      "Epoch:  50  Training Loss:  1.13422  Training Accuracy:  0.424792\n",
      "Epoch:  51  Training Loss:  1.14091  Training Accuracy:  0.432403\n",
      "Epoch:  52  Training Loss:  1.13358  Training Accuracy:  0.441827\n",
      "Epoch:  53  Training Loss:  1.14085  Training Accuracy:  0.41718\n",
      "Epoch:  54  Training Loss:  1.14279  Training Accuracy:  0.422979\n",
      "Epoch:  55  Training Loss:  1.14776  Training Accuracy:  0.437477\n",
      "Epoch:  56  Training Loss:  1.16016  Training Accuracy:  0.43494\n",
      "Epoch:  57  Training Loss:  1.15109  Training Accuracy:  0.444726\n",
      "Epoch:  58  Training Loss:  1.14451  Training Accuracy:  0.416455\n",
      "Epoch:  59  Training Loss:  1.15045  Training Accuracy:  0.433128\n",
      "Epoch:  60  Training Loss:  1.15219  Training Accuracy:  0.437115\n",
      "Epoch:  61  Training Loss:  1.15032  Training Accuracy:  0.410294\n",
      "Epoch:  62  Training Loss:  1.15734  Training Accuracy:  0.425879\n",
      "Epoch:  63  Training Loss:  1.148  Training Accuracy:  0.430591\n",
      "Epoch:  64  Training Loss:  1.144  Training Accuracy:  0.443639\n",
      "Epoch:  65  Training Loss:  1.15162  Training Accuracy:  0.41718\n",
      "Epoch:  66  Training Loss:  1.14914  Training Accuracy:  0.413193\n",
      "Epoch:  67  Training Loss:  1.14171  Training Accuracy:  0.430228\n",
      "Epoch:  68  Training Loss:  1.14083  Training Accuracy:  0.427691\n",
      "Epoch:  69  Training Loss:  1.13774  Training Accuracy:  0.441827\n",
      "Epoch:  70  Training Loss:  1.13014  Training Accuracy:  0.43929\n",
      "Epoch:  71  Training Loss:  1.13796  Training Accuracy:  0.41863\n",
      "Epoch:  72  Training Loss:  1.14912  Training Accuracy:  0.425154\n",
      "Epoch:  73  Training Loss:  1.15522  Training Accuracy:  0.441102\n",
      "Epoch:  74  Training Loss:  1.15288  Training Accuracy:  0.429141\n",
      "Epoch:  75  Training Loss:  1.15289  Training Accuracy:  0.427691\n",
      "Epoch:  76  Training Loss:  1.15563  Training Accuracy:  0.444001\n",
      "Epoch:  77  Training Loss:  1.15913  Training Accuracy:  0.419717\n",
      "Epoch:  78  Training Loss:  1.16274  Training Accuracy:  0.424792\n",
      "Epoch:  79  Training Loss:  1.14578  Training Accuracy:  0.434578\n",
      "Epoch:  80  Training Loss:  1.15005  Training Accuracy:  0.43349\n",
      "Epoch:  81  Training Loss:  1.14968  Training Accuracy:  0.433853\n",
      "Epoch:  82  Training Loss:  1.1461  Training Accuracy:  0.432765\n",
      "Epoch:  83  Training Loss:  1.1558  Training Accuracy:  0.413918\n",
      "Epoch:  84  Training Loss:  1.15442  Training Accuracy:  0.434578\n",
      "Epoch:  85  Training Loss:  1.15157  Training Accuracy:  0.422979\n",
      "Epoch:  86  Training Loss:  1.151  Training Accuracy:  0.436028\n",
      "Epoch:  87  Training Loss:  1.15719  Training Accuracy:  0.423704\n",
      "Epoch:  88  Training Loss:  1.15552  Training Accuracy:  0.43349\n",
      "Epoch:  89  Training Loss:  1.15743  Training Accuracy:  0.434215\n",
      "Epoch:  90  Training Loss:  1.14388  Training Accuracy:  0.429141\n",
      "Epoch:  91  Training Loss:  1.14994  Training Accuracy:  0.426241\n",
      "Epoch:  92  Training Loss:  1.14718  Training Accuracy:  0.443277\n",
      "Epoch:  93  Training Loss:  1.15187  Training Accuracy:  0.421167\n",
      "Epoch:  94  Training Loss:  1.14529  Training Accuracy:  0.43494\n",
      "Epoch:  95  Training Loss:  1.14885  Training Accuracy:  0.426604\n",
      "Epoch:  96  Training Loss:  1.14147  Training Accuracy:  0.416818\n",
      "Epoch:  97  Training Loss:  1.13527  Training Accuracy:  0.41718\n",
      "Epoch:  98  Training Loss:  1.14068  Training Accuracy:  0.425516\n",
      "Epoch:  99  Training Loss:  1.13992  Training Accuracy:  0.431316\n",
      "Testing Accuracy: 0.426241\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"cost\"):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=Y))\n",
    "    # optimizer = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(loss)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=\n",
    "                                                  learning_rate).minimize(loss)\n",
    "\n",
    "    # Add scalar summary for cost\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(y_, 1)) \n",
    "    # Count correct predictions\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) \n",
    "    # Cast boolean to float to average\n",
    "    # Add scalar summary for accuracy\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "\n",
    "print(\"net work done\")\n",
    "\n",
    "cost_history = np.empty(shape=[1],dtype=float)\n",
    "\n",
    "with tf.Session(config=sess_config) as session:\n",
    "    # create a log writer. run 'tensorboard --logdir=./logs/nn_logs'\n",
    "    writer = tf.summary.FileWriter(\"data/cnn_lstm_logs\", session.graph)  # for 1.0\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        for b in range(total_batches):    \n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "\n",
    "            test_indices = np.arange(len(test_x))  # Get A Test Batch\n",
    "            np.random.shuffle(test_indices)\n",
    "            test_indices = test_indices[0:]\n",
    "            summary, acc=session.run([merged, accuracy], \n",
    "                        feed_dict={X: test_x[test_indices], Y: test_y[test_indices]})\n",
    "        print \"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \", acc\n",
    "        writer.add_summary(summary, epoch)  # Write summary\n",
    "\n",
    "    print \"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y})\n",
    "    writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
