{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdf5 is not supported on this machine (please install/reinstall h5py for optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "from tflearn.layers.estimator import regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 0.4)\n",
    "sess_config = tf.ConfigProto(gpu_options=gpu_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = {'0':'file', '1':'network', '2':'service', '3':'database', '4':'communication', '5':'memory', '6':'driver', \n",
    "    '7':'system', '8':'application', '9':'io', '10':'others', '11':'security', '12':'disk', '13':'processor'}\n",
    "\n",
    "fault_label = {'0':'file', '1':'network', '2':'service', '3':'database', '4':'communication', '5':'memory', '6':'driver', \n",
    "    '7':'system', '9':'io', '10':'others', '11':'security', '12':'disk', '13':'processor'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separating data into 80% training set & 20% test set...\n",
      "Dataset separated.\n",
      "\n",
      "((80000, 10, 14, 1), (80000, 14), (20000, 14))\n"
     ]
    }
   ],
   "source": [
    "def one_hot(y):\n",
    "    y = y.reshape(len(y))\n",
    "    n_values = np.max(y) + 1\n",
    "    return np.eye(n_values)[np.array(y, dtype=np.int32)]  # Returns FLOATS\n",
    "\n",
    "\n",
    "def load_X(X_path):\n",
    "    X_list = []\n",
    "    file = open(X_path, 'r')\n",
    "    # Read dataset from disk, dealing with text files' syntax\n",
    "    X_signal = [np.array(item, dtype=np.float32) for item in [\n",
    "               line.strip().split('\\t') for line in file]]\n",
    "    X_list.append(X_signal)\n",
    "    file.close()\n",
    "    return np.transpose(np.array(X_list), (1, 2, 0))\n",
    "\n",
    "\n",
    "# Load \"y\" (the neural network's training and testing outputs)\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    # Read dataset from disk, dealing with text file's syntax\n",
    "    y_ = np.array([elem for elem in [line.strip().split('\\t') for line in file]], \n",
    "                  dtype=np.int32)\n",
    "    file.close()\n",
    "    # Substract 1 to each output class for friendly 0-based indexing\n",
    "    return one_hot(y_-1)\n",
    "\n",
    "\n",
    "dataset_path = \"data_msg_type/\"\n",
    "X_path = dataset_path + \"semantic_sim.txt\"\n",
    "y_path = dataset_path + \"semantic_label_index.txt\"\n",
    "\n",
    "X = load_X(X_path)\n",
    "y = load_y(y_path)\n",
    "x = X.reshape(len(X), 10, 14, 1)\n",
    "\n",
    "# Separate our training data into test and training.\n",
    "print(\"Separating data into 80% training set & 20% test set...\")\n",
    "train_x, test_x, train_y, test_y = cross_validation.train_test_split(\n",
    "    x, y, test_size=0.2, random_state=33)#add random state here...\n",
    "print(\"Dataset separated.\\n\")\n",
    "print(train_x.shape, train_y.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 124999  | total loss: \u001b[1m\u001b[32m0.30370\u001b[0m\u001b[0m | time: 7.657s\n",
      "| AdaGrad | epoch: 200 | loss: 0.30370 - acc: 0.9086 -- iter: 79872/80000\n",
      "Training Step: 125000  | total loss: \u001b[1m\u001b[32m0.28314\u001b[0m\u001b[0m | time: 8.852s\n",
      "| AdaGrad | epoch: 200 | loss: 0.28314 - acc: 0.9162 | val_loss: 0.24249 - val_acc: 0.9418 -- iter: 80000/80000\n",
      "--\n",
      "('accuracy: ', 0.94179999999999997)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=sess_config) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    network = input_data(shape=[None, 10, 14, 1])\n",
    "    network = conv_2d(network, 4, 5, strides=1, activation='tanh')\n",
    "    network = max_pool_2d(network, 2, strides=1)\n",
    "    network = local_response_normalization(network)    \n",
    "    network = conv_2d(network, 6, 5, strides=4, activation='tanh')\n",
    "    network = max_pool_2d(network, 2, strides=1)\n",
    "    network = local_response_normalization(network)        \n",
    "    network = fully_connected(network, 64, activation='tanh')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 14, activation='softmax')\n",
    "    network = regression(network, optimizer='Adagrad',\n",
    "          loss='categorical_crossentropy', learning_rate=0.01)\n",
    "    \n",
    "    # train test\n",
    "    if not os.path.isdir('checkpoints'):\n",
    "        os.makedirs('checkpoints')\n",
    "    if not os.path.isdir('model'):\n",
    "        os.makedirs('model')\n",
    "    model = tflearn.DNN(network, checkpoint_path='checkpoints/lenet',\n",
    "                    max_checkpoints=1, tensorboard_verbose=0)\n",
    "    model.fit(train_x, train_y, n_epoch=200, validation_set=(test_x, test_y), shuffle=True,\n",
    "                  show_metric=True, batch_size=128, snapshot_step=200,\n",
    "                  snapshot_epoch=True, run_id='lenet')\n",
    "    z = np.argmax(model.predict(test_x), axis=1)\n",
    "    acc = np.mean(np.argmax(test_y,axis=1) == z)\n",
    "    model.save('model/model_retrained_by_lenet')\n",
    "    print(\"accuracy: \", acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
