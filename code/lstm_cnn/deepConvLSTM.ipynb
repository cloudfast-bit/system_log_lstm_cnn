{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import cPickle as cp #serializing and de-serializing a Python object structure\n",
    "from sklearn import cross_validation\n",
    "from sliding_window import sliding_window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 0.4)\n",
    "config = tf.ConfigProto(gpu_options=gpu_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# params setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NB_SENSOR_CHANNELS = 120\n",
    "\n",
    "NUM_CLASSES = 7\n",
    "\n",
    "# Hardcoded length of the sliding window mechanism employed to segment the data\n",
    "SLIDING_WINDOW_LENGTH = 24\n",
    "\n",
    "# Length of the input sequence after convolutional operations\n",
    "FINAL_SEQUENCE_LENGTH = 8\n",
    "\n",
    "# Hardcoded step of the sliding window mechanism employed to segment the data\n",
    "SLIDING_WINDOW_STEP = 12\n",
    "\n",
    "# Batch Size\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# Number filters convolutional layers\n",
    "NUM_FILTERS = 64\n",
    "\n",
    "# Size filters convolutional layers\n",
    "FILTER_SIZE = 5\n",
    "\n",
    "# Number of unit in the long short-term recurrent layers\n",
    "NUM_UNITS_LSTM = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset is now located at: data/\n",
      "Separating data into 67% training set & 33% test set...\n",
      "Dataset separated.\n",
      "\n",
      "((8360, 120), (8360, 1), (1671, 1))\n"
     ]
    }
   ],
   "source": [
    "def load_X(X_path):\n",
    "    file = open(X_path, 'r')\n",
    "    # Read dataset from disk, dealing with text files' syntax\n",
    "    X_signal = [np.array(item, dtype=np.float32) for item in [\n",
    "               line.strip().split('\\t') for line in file]]\n",
    "    file.close()\n",
    "    return np.array(X_signal)\n",
    "\n",
    "\n",
    "# Load \"y\" (the neural network's training and testing outputs)\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    # Read dataset from disk, dealing with text file's syntax\n",
    "    y_ = np.array([elem for elem in [line.strip().split('\\t') for line in file]], \n",
    "                  dtype=np.int32)\n",
    "    file.close()\n",
    "    # Substract 1 to each output class for friendly 0-based indexing\n",
    "    return y_ \n",
    "\n",
    "labels = ['info', 'crit', 'err', 'notice', 'warning', 'alert', 'emerg']\n",
    "\n",
    "dataset_path = \"data/\"\n",
    "print(\"\\n\" + \"Dataset is now located at: \" + dataset_path)\n",
    "\n",
    "X_train_signals_path = dataset_path + \"msg_token_train.txt\"\n",
    "X_test_signals_path = dataset_path + \"msg_token_test.txt\"\n",
    "X_train = load_X(X_train_signals_path)\n",
    "X_test = load_X(X_test_signals_path)\n",
    "\n",
    "y_train_path = dataset_path + \"msg_label_train.txt\"\n",
    "y_test_path = dataset_path + \"msg_label_test.txt\"\n",
    "\n",
    "y_train = load_y(y_train_path)\n",
    "y_test = load_y(y_test_path)\n",
    "\n",
    "# Separate our training data into test and training.\n",
    "print(\"Separating data into 67% training set & 33% test set...\")\n",
    "#X_train, X_test, y_train, y_test = cross_validation.train_test_split(\n",
    "    #X_train, y_train, test_size=0.33, random_state=33) #add random state here...\n",
    "print(\"Dataset separated.\\n\")\n",
    "\n",
    "print(X_train.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after sliding window (testing): inputs (138, 24, 120), targets (138,)\n",
      "after sliding window (testing): inputs (138, 24, 120, 1), targets (138,)\n"
     ]
    }
   ],
   "source": [
    "assert NB_SENSOR_CHANNELS == X_train.shape[1]\n",
    "def opp_sliding_window(data_x, data_y, ws, ss):\n",
    "    data_x = sliding_window(data_x,(ws,data_x.shape[1]),(ss,1))\n",
    "    data_y = np.asarray([[i[-1]] for i in sliding_window(data_y,(ws,1),(ss,1))])\n",
    "    return data_x.astype(np.float32), data_y.reshape(len(data_y)).astype(np.int32)\n",
    "\n",
    "# Sensor data is segmented using a sliding window mechanism\n",
    "y_test\n",
    "X_test, y_test = opp_sliding_window(X_test, y_test, SLIDING_WINDOW_LENGTH, \n",
    "                                    SLIDING_WINDOW_STEP)\n",
    "print(\"after sliding window (testing): inputs {0}, targets {1}\".format(X_test.shape, \n",
    "                                                                          y_test.shape))\n",
    "# Data is reshaped since the input of the network is a 4 dimension tensor\n",
    "X_test = X_test.reshape((-1, SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS, 1))\n",
    "\n",
    "# Sensor data is segmented using a sliding window mechanism\n",
    "X_train, y_train = opp_sliding_window(X_train, y_train, SLIDING_WINDOW_LENGTH, \n",
    "                                      SLIDING_WINDOW_STEP)\n",
    "print(\"after sliding window (testing): inputs {0}, targets {1}\".format(\n",
    "                                      X_test.shape, y_test.shape))\n",
    "# Data is reshaped since the input of the network is a 4 dimension tensor\n",
    "X_train = X_train.reshape((-1, SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data is ready\n"
     ]
    }
   ],
   "source": [
    "def one_hot(label):\n",
    "    \"\"\"convert label from dense to one hot\n",
    "      argument:\n",
    "        label: ndarray dense label ,shape: [sample_num,1]\n",
    "      return:\n",
    "        one_hot_label: ndarray  one hot, shape: [sample_num,n_class]\n",
    "    \"\"\"\n",
    "    label_num = len(label)\n",
    "    new_label = label.reshape(label_num)  # shape : [sample_num]\n",
    "    # because max is 6, and we will create 7 columns\n",
    "    n_values = np.max(new_label) + 1\n",
    "    return np.eye(n_values)[np.array(new_label, dtype=np.int32)]\n",
    "\n",
    "y_test=one_hot(y_test)\n",
    "y_train=one_hot(y_train)\n",
    "\n",
    "print(\"data is ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n",
    "\n",
    "def model(X, w, w2, w3, w4, rnnW, rnnB, lstm_size):\n",
    "    # l1a shape=(?, 28, 28, 32)\n",
    "    l1a = tf.nn.relu(tf.nn.conv2d(X, w, strides=[1, 1, 1, 1], padding='VALID'))\n",
    "    # l1 shape=(?, 14, 14, 32)\n",
    "    #|l1 = tf.nn.max_pool(l1a, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    #l1 = tf.nn.dropout(l1, p_keep_conv)\n",
    "                     \n",
    "    # l2a shape=(?, 14, 14, 64)\n",
    "    l2a = tf.nn.relu(tf.nn.conv2d(l1a, w2, strides=[1, 1, 1, 1], padding='VALID'))\n",
    "    # l2 shape=(?, 7, 7, 64)\n",
    "    #l2 = tf.nn.max_pool(l2a, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    #l2 = tf.nn.dropout(l2, p_keep_conv)\n",
    "\n",
    "    # l3a shape=(?, 7, 7, 128)\n",
    "    l3a = tf.nn.relu(tf.nn.conv2d(l2a, w3, strides=[1, 1, 1, 1], padding='VALID'))        \n",
    "    # l3 shape=(?, 4, 4, 128)                           \n",
    "    #l3 = tf.nn.max_pool(l3a, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    # reshape to (?, 2048)\n",
    "    #l3 = tf.reshape(l3, [-1, w4.get_shape().as_list()[0]])  \n",
    "    #l3 = tf.nn.dropout(l3, p_keep_conv)\n",
    "                 \n",
    "    # l3a shape=(?, 7, 7, 128)\n",
    "    l4a = tf.nn.relu(tf.nn.conv2d(l3a, w4, strides=[1, 1, 1, 1], padding='VALID'))\n",
    "                     \n",
    "    shuff = tf.transpose(l4a, [1, 0, 2, 3])\n",
    "    shp1 = tf.reshape(shuff, [-1, lstm_size])\n",
    "    # split them to time_step_size (28 arrays)\n",
    "    X_split = tf.split(shp1, 480, 0) \n",
    "    \n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(lstm_size, \n",
    "                                        forget_bias=1.0, state_is_tuple=True)  \n",
    "    lstm_cell2 = tf.contrib.rnn.BasicLSTMCell(lstm_size, \n",
    "                                        forget_bias=1.0, state_is_tuple=True)\n",
    "    # Stack two LSTM layers, both layers has the same shape\n",
    "    lstm_layers = tf.contrib.rnn.MultiRNNCell([lstm_cell, lstm_cell2], \n",
    "                                              state_is_tuple=True)\n",
    "                     \n",
    "    outputs, _states = tf.contrib.rnn.static_rnn(lstm_layers, X_split, dtype=tf.float32)\n",
    "    \n",
    "    print(\"tf net end\")\n",
    "\n",
    "    return tf.matmul(outputs[-1], rnnW) + rnnB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf net end\n",
      "get cnn output\n",
      "net work done\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(\"float\", [None, 24, 120, 1])\n",
    "Y = tf.placeholder(\"float\", [None, 7])\n",
    "\n",
    "lstm_size = 128\n",
    "w = init_weights([5, 1, 1, 64])       # 3x3x1 conv, 32 outputs\n",
    "w2 = init_weights([5, 1, 64, 64])     # 3x3x32 conv, 64 outputs\n",
    "w3 = init_weights([5, 1, 64, 64])    # 3x3x32 conv, 128 outputs\n",
    "w4 = init_weights([5, 1, 64, 64]) # FC 128 * 4 * 4 inputs, 625 outputs\n",
    "\n",
    "rnnW = init_weights([lstm_size, 7])\n",
    "rnnB = init_weights([7])\n",
    "pre_Y = model(X, w, w2, w3, w4, rnnW, rnnB,lstm_size)\n",
    "print (\"get cnn output\");\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pre_Y, labels=Y))\n",
    "train_op = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cost)\n",
    "predict_op = tf.argmax(pre_Y, 1)\n",
    "\n",
    "print(\"net work done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.072463768115942032)\n",
      "(1, 0.41304347826086957)\n",
      "(2, 0.38405797101449274)\n",
      "(3, 0.48550724637681159)\n",
      "(4, 0.40579710144927539)\n",
      "(5, 0.40579710144927539)\n",
      "(6, 0.40579710144927539)\n",
      "(7, 0.43478260869565216)\n",
      "(8, 0.40579710144927539)\n",
      "(9, 0.40579710144927539)\n",
      "(10, 0.40579710144927539)\n",
      "(11, 0.40579710144927539)\n",
      "(12, 0.40579710144927539)\n",
      "(13, 0.40579710144927539)\n",
      "(14, 0.40579710144927539)\n",
      "(15, 0.40579710144927539)\n",
      "(16, 0.40579710144927539)\n",
      "(17, 0.40579710144927539)\n",
      "(18, 0.40579710144927539)\n",
      "(19, 0.40579710144927539)\n",
      "(20, 0.40579710144927539)\n",
      "(21, 0.40579710144927539)\n",
      "(22, 0.40579710144927539)\n",
      "(23, 0.40579710144927539)\n",
      "(24, 0.40579710144927539)\n",
      "(25, 0.40579710144927539)\n",
      "(26, 0.40579710144927539)\n",
      "(27, 0.40579710144927539)\n",
      "(28, 0.40579710144927539)\n",
      "(29, 0.40579710144927539)\n",
      "(30, 0.40579710144927539)\n",
      "(31, 0.40579710144927539)\n",
      "(32, 0.40579710144927539)\n",
      "(33, 0.40579710144927539)\n",
      "(34, 0.40579710144927539)\n",
      "(35, 0.40579710144927539)\n",
      "(36, 0.40579710144927539)\n",
      "(37, 0.40579710144927539)\n",
      "(38, 0.40579710144927539)\n",
      "(39, 0.40579710144927539)\n",
      "(40, 0.40579710144927539)\n",
      "(41, 0.40579710144927539)\n",
      "(42, 0.40579710144927539)\n",
      "(43, 0.40579710144927539)\n",
      "(44, 0.40579710144927539)\n",
      "(45, 0.40579710144927539)\n",
      "(46, 0.40579710144927539)\n",
      "(47, 0.40579710144927539)\n",
      "(48, 0.40579710144927539)\n",
      "(49, 0.40579710144927539)\n",
      "(50, 0.40579710144927539)\n",
      "(51, 0.40579710144927539)\n",
      "(52, 0.40579710144927539)\n",
      "(53, 0.40579710144927539)\n",
      "(54, 0.40579710144927539)\n",
      "(55, 0.40579710144927539)\n",
      "(56, 0.40579710144927539)\n",
      "(57, 0.40579710144927539)\n",
      "(58, 0.40579710144927539)\n",
      "(59, 0.40579710144927539)\n",
      "(60, 0.40579710144927539)\n",
      "(61, 0.40579710144927539)\n",
      "(62, 0.40579710144927539)\n",
      "(63, 0.40579710144927539)\n",
      "(64, 0.40579710144927539)\n",
      "(65, 0.40579710144927539)\n",
      "(66, 0.40579710144927539)\n",
      "(67, 0.40579710144927539)\n",
      "(68, 0.40579710144927539)\n",
      "(69, 0.40579710144927539)\n",
      "(70, 0.40579710144927539)\n",
      "(71, 0.40579710144927539)\n",
      "(72, 0.37681159420289856)\n",
      "(73, 0.45652173913043476)\n",
      "(74, 0.48550724637681159)\n",
      "(75, 0.4420289855072464)\n",
      "(76, 0.39855072463768115)\n",
      "(77, 0.40579710144927539)\n",
      "(78, 0.43478260869565216)\n",
      "(79, 0.4420289855072464)\n",
      "(80, 0.40579710144927539)\n",
      "(81, 0.40579710144927539)\n",
      "(82, 0.40579710144927539)\n",
      "(83, 0.40579710144927539)\n",
      "(84, 0.40579710144927539)\n",
      "(85, 0.40579710144927539)\n",
      "(86, 0.39855072463768115)\n",
      "(87, 0.39130434782608697)\n",
      "(88, 0.40579710144927539)\n",
      "(89, 0.39130434782608697)\n",
      "(90, 0.31159420289855072)\n",
      "(91, 0.36956521739130432)\n",
      "(92, 0.36956521739130432)\n",
      "(93, 0.40579710144927539)\n",
      "(94, 0.39130434782608697)\n",
      "(95, 0.3188405797101449)\n",
      "(96, 0.39130434782608697)\n",
      "(97, 0.37681159420289856)\n",
      "(98, 0.38405797101449274)\n",
      "(99, 0.36231884057971014)\n"
     ]
    }
   ],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "\n",
    "# Launch the graph in a session\n",
    "with tf.Session(config=config) as sess:\n",
    "    # you need to initialize all variables\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for i in range(100):\n",
    "        for start, end in zip(range(0, len(X_train), BATCH_SIZE), \n",
    "                              range(BATCH_SIZE, len(X_train)+1, BATCH_SIZE)):\n",
    "            sess.run(train_op, feed_dict={X: X_train[start:end], Y: y_train[start:end]})\n",
    "\n",
    "        test_indices = np.arange(len(X_test))  # Get A Test Batch\n",
    "        np.random.shuffle(test_indices)\n",
    "        test_indices = test_indices[0:]\n",
    "\n",
    "        print(i, np.mean(np.argmax(y_test[test_indices], axis=1) ==\n",
    "                         sess.run(predict_op, feed_dict={X: X_test[test_indices]})))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
